{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":11513013,"sourceType":"datasetVersion","datasetId":7219604},{"sourceId":11513195,"sourceType":"datasetVersion","datasetId":7219748},{"sourceId":11519501,"sourceType":"datasetVersion","datasetId":7224624}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install ipykernal","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================================\n# === TPU SCRIPT: Train Advanced H-RPE Transformer + SSMD Eval (Reduced Logging) ===\n# ====================================================================================\n# Target: Chord Progression (Model 1) using existing dataset files.\n# H-RPE: Advanced version using learnable embeddings based on numerical features.\n# Evaluation: Loss, Accuracy, Perplexity, SSMD(k=1) on Function Codes.\n# Assumes running in a Google Cloud TPU environment with PyTorch XLA.\n# --- MODIFIED FOR REDUCED LOGGING & PER-EPOCH METRICS ---\n# --- Calculates Val Acc & SSMD per epoch (controlled by HPARAMS) ---\n# --- Updates epoch progress bar postfix ---\n# ====================================================================================\n\n# !!! IMPORTANT !!!\n# For the dynamic progress bar (`tqdm.notebook`) to work on Kaggle/Colab,\n# you might need to explicitly install/update ipywidgets first.\n# Run this in a separate cell BEFORE running this script, then RESTART the kernel:\n# !pip install -U ipywidgets --quiet\n# !jupyter nbextension enable --py widgetsnbextension --sys-prefix --quiet # Optional, sometimes needed\n\n# --- Necessary Imports ---\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, Subset, random_split\nfrom torch.nn.utils.rnn import pad_sequence\n# ============================================= #\n# >>> MODIFICATION: Using tqdm.notebook <<<     #\nfrom tqdm.notebook import tqdm                  #\n# ============================================= #\nimport logging\nimport os\nimport math\nimport sys\nimport time\nimport traceback\nimport random\nimport numpy as np\nimport gc\nfrom typing import Optional, List, Dict, Tuple, Any\nfrom pathlib import Path\n# Import for SSMD\nfrom scipy.spatial.distance import pdist, squareform\n\n# --- XLA Imports ---\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.utils.serialization as xser # For saving/loading\n\n# === CONFIGURATION - ASSUMING Sequence Length 32 and token_features.json ===\n# >>>>>>>>>> VERIFY YOUR INPUT FILES MATCH THIS CONFIG <<<<<<<<<<<<<<\nINPUT_SEQUENCES_PATH = Path(\"/kaggle/input/advance-h-rpe/chord_sequences_for_training.jsonl\") # Example path, replace with yours\nVOCAB_PATH = Path(\"/kaggle/input/advance-h-rpe/chord_progression_vocab.json\") # Example path\nTOKEN_FEATURES_PATH = Path(\"/kaggle/input/advance-h-rpe/token_features.json\") # Example path\nRESULTS_DIR = Path(\"./harmony_results_AdvH_RPE_SeqLen32_TPU_EpochMetrics\") # Updated name\nCHECKPOINT_TO_LOAD = None # Set path like \"/path/to/prev-ckpt/best_model_... .pth\" to resume\nCHECKPOINT_FILENAME_BEST_PREFIX = \"best_model_ep\"\n# >>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n\n# Data Split Ratios / HPARAMS / Other Params\nHPARAMS = {\n    # Model Architecture\n    'd_model': 512, 'nhead': 8, 'num_layers': 6, 'dim_feedforward': 2048, 'dropout': 0.1,\n\n    # --- ADVANCED H-RPE Specific ---\n    'num_root_intervals': 12,\n    'num_qualities': 10,\n    'num_functions': 11,\n    'relation_embedding_dim': None,\n    'feature_keys': ['root_pc', 'quality_code', 'function_code'],\n\n    # Training Hyperparameters\n    # >>>>> ADJUSTED FOR TPU (Per Core Batch Size) <<<<<\n    # Total batch size = batch_size * num_cores (e.g., 8 cores * 8 = 64)\n    'batch_size': 512,      # BATCH SIZE PER TPU CORE\n    'eval_batch_size': 16,      # EVAL BATCH SIZE PER TPU CORE\n    'lr': 5e-5,                # Base LR - may need scaling\n    'weight_decay': 0.01,\n    'warm_up_steps': 100,       # Placeholder: Adjust\n    'max_grad_norm': 1.0,\n    'num_epochs': 5,\n    'patience': 5,\n    'lr_scale_factor': None,     # Set to xm.xrt_world_size() later if scaling needed\n\n    # Data & Environment\n    'sequence_length': 32,\n    'seed': 42,\n    'num_workers': 4,           # Recommended for TPUs (adjust based on environment)\n\n    # Generation & Evaluation\n    'max_gen_len': 16,\n    'ssmd_context_len': 16,\n    'ssmd_eval_every_n_epochs': 1, # <<< How often to run SSMD eval (1 = every epoch)\n    'ssmd_eval_max_batches': 32,   # <<< Max validation batches for SSMD per epoch (None = all)\n\n    # Checkpointing\n    'save_every_n_epochs': 5\n}\n\nTRAIN_RATIO = 0.8\nVAL_RATIO = 0.1\nTEST_RATIO = 0.1\n\nPAD_TOKEN = \"<PAD>\"\n# AMP is generally handled differently in XLA, often bf16 is preferred and automatic.\n# We will remove explicit AMP scaler logic.\n\n# === Setup Logging (Simplified for Multiprocessing) ===\n# Set to INFO to see epoch summaries including new metrics\nlog_level = logging.INFO\nlogging.basicConfig(\n    level=log_level, format='%(asctime)s %(levelname)-8s [Process %(process)d - %(name)s]: %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S', stream=sys.stdout)\nlogger = logging.getLogger(__name__) # Root logger\n\n# === Global PADDING_VALUE & Feature Map (Initialized in _mp_fn) ===\nPADDING_VALUE: Optional[int] = None\ntoken_to_features_map: Optional[Dict[int, Dict[str, int]]] = None\nid_to_vocab: Optional[Dict[int, str]] = None\nvocab: Optional[Dict[str, int]] = None\nvocab_size: int = 0\n\n\n# === Dataset for Chord Progression (Modified for Features) ===\nclass ChordProgressionDataset(Dataset):\n    def __init__(self, data_file_path: Path, seq_len: int):\n        self.samples = []\n        self.seq_len = seq_len\n        self.logger = logging.getLogger(__name__ + \".ChordProgressionDataset\")\n        self.feature_keys = HPARAMS.get('feature_keys', ['root_pc', 'quality_code', 'function_code'])\n\n        if not data_file_path.is_file():\n            self.logger.error(f\"Data file not found: {data_file_path}\")\n            return\n\n        self.logger.info(f\"Loading data from: {data_file_path} (Expecting seq_len={self.seq_len})\")\n        skipped_count = 0\n        try:\n            with open(data_file_path, 'r') as f:\n                for line_num, line in enumerate(f):\n                    try:\n                        sample = json.loads(line.strip())\n                        if not all(k in sample for k in ['input_ids', 'target_id'] + self.feature_keys):\n                            self.logger.debug(f\"Skipping line {line_num+1}: Missing required keys. Found: {list(sample.keys())}\")\n                            skipped_count += 1; continue\n                        input_ids_val = sample.get('input_ids', [])\n                        if not isinstance(input_ids_val, list) or len(input_ids_val) != self.seq_len:\n                            self.logger.debug(f\"Skipping line {line_num+1}: Invalid input_ids length ({len(input_ids_val)}) != {self.seq_len}.\")\n                            skipped_count += 1; continue\n                        valid_features = True\n                        for key in self.feature_keys:\n                            feature_val = sample.get(key, [])\n                            if not isinstance(feature_val, list) or len(feature_val) != self.seq_len:\n                                self.logger.debug(f\"Skipping line {line_num+1}: Invalid feature '{key}' length ({len(feature_val)}) != {self.seq_len}.\")\n                                valid_features = False; break\n                        if not valid_features:\n                            skipped_count += 1; continue\n                        self.samples.append(sample)\n                    except json.JSONDecodeError:\n                        self.logger.warning(f\"Skipping line {line_num+1}: Invalid JSON.\")\n                        skipped_count += 1\n                    except Exception as e:\n                        self.logger.warning(f\"Skipping line {line_num+1}: Error processing sample - {e}\")\n                        skipped_count += 1\n\n            if skipped_count > 0:\n                self.logger.warning(f\"Skipped {skipped_count} samples during loading due to format/length issues.\")\n            if not self.samples:\n                self.logger.error(f\"No valid samples loaded from {data_file_path}. Check file content and seq_len ({self.seq_len}) match HPARAMS.\")\n            else:\n                self.logger.info(f\"Successfully loaded {len(self.samples)} samples.\")\n\n        except Exception as e:\n            self.logger.error(f\"Failed to load or process data file {data_file_path}: {e}\", exc_info=True)\n\n    def __len__(self): return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        input_ids = torch.tensor(sample['input_ids'], dtype=torch.long)\n        target_id = torch.tensor(sample['target_id'], dtype=torch.long)\n        features = {key: torch.tensor(sample[key], dtype=torch.long) for key in self.feature_keys}\n        return input_ids, target_id, features, sample['input_ids'] # Return original ids for decode\n\n\n# === Collate Function (Modified for Features) ===\ndef collate_fn_progression(batch: List[Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor], List[int]]]) -> Optional[Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor], List[List[int]]]]:\n    global PADDING_VALUE\n    if PADDING_VALUE is None:\n        logging.error(\"Global PADDING_VALUE not set in collate_fn_progression.\")\n        return None\n\n    if not batch: return None\n    valid_batch = [item for item in batch if item is not None and isinstance(item[0], torch.Tensor) and isinstance(item[1], torch.Tensor) and isinstance(item[2], dict)]\n    if not valid_batch: return None\n\n    inputs, targets, features_list, orig_inputs_list = zip(*valid_batch)\n\n    try:\n        inputs_stacked = torch.stack(inputs, dim=0)\n        targets_stacked = torch.stack(targets, dim=0)\n    except RuntimeError as e:\n        logging.error(f\"Error stacking batch tensors: {e}. Sequence lengths might be inconsistent.\", exc_info=True)\n        return None\n\n    collated_features = {}\n    if features_list:\n        feature_keys = HPARAMS.get('feature_keys', list(features_list[0].keys()))\n        for key in feature_keys:\n            if key not in features_list[0]: continue\n            if not all(key in f for f in features_list): continue\n            feature_tensors = [f[key] for f in features_list]\n            try:\n                collated_features[key] = torch.stack(feature_tensors, dim=0)\n            except RuntimeError as e:\n                logging.error(f\"Error stacking feature '{key}': {e}. Feature lengths might be inconsistent.\", exc_info=True)\n                return None\n\n    return inputs_stacked, targets_stacked, collated_features, list(orig_inputs_list)\n\n\n# =====================================================\n# === ADVANCED H-RPE MODEL DEFINITION ===\n# =====================================================\nclass AdvancedHarmonicRelativeAttention(nn.Module):\n    def __init__(self, d_model, nhead, dropout, padding_idx: Optional[int], num_root_intervals=12, num_qualities=8, num_functions=10, relation_embedding_dim: Optional[int] = None):\n        super().__init__()\n        if d_model % nhead != 0: raise ValueError(\"d_model must be divisible by nhead\")\n        self.d_model = d_model; self.nhead = nhead; self.head_dim = d_model // nhead\n        self.dropout = nn.Dropout(dropout)\n        self.q_proj = nn.Linear(d_model, d_model); self.k_proj = nn.Linear(d_model, d_model)\n        self.v_proj = nn.Linear(d_model, d_model); self.out_proj = nn.Linear(d_model, d_model)\n        self.padding_idx = int(padding_idx) if padding_idx is not None else None\n        self.local_logger = logging.getLogger(__name__ + \".AdvancedHarmonicRelativeAttention\")\n\n        bias_head_dim = relation_embedding_dim if relation_embedding_dim is not None else self.head_dim // 4\n        if bias_head_dim <= 0: bias_head_dim = 1\n\n        if num_root_intervals <= 0: raise ValueError(\"num_root_intervals must be positive\")\n        if num_qualities <= 0: raise ValueError(\"num_qualities must be positive\")\n        if num_functions <= 0: raise ValueError(\"num_functions must be positive\")\n\n        self.root_interval_embed = nn.Embedding(num_root_intervals, self.nhead * bias_head_dim)\n        num_quality_rels = num_qualities * num_qualities\n        self.quality_rel_embed = nn.Embedding(num_quality_rels, self.nhead * bias_head_dim)\n        num_function_rels = num_functions * num_functions\n        self.function_rel_embed = nn.Embedding(num_function_rels, self.nhead * bias_head_dim)\n\n        self.num_feature_embeddings = 3\n\n        self.bias_combiner = nn.Sequential(\n            nn.Linear(self.num_feature_embeddings * bias_head_dim, bias_head_dim),\n            nn.ReLU(),\n            nn.Linear(bias_head_dim, 1)\n        )\n\n        self.local_logger.info(f\"Initialized Advanced Theory H-RPE (Embeddings + Combiner)\")\n        self.local_logger.info(f\" HRPE Params: #Qual={num_qualities}, #Func={num_functions}, RootInt={num_root_intervals}, BiasHeadDim={bias_head_dim}\")\n        self.local_logger.info(f\" QualityRelEmbed size: {num_quality_rels}, FunctionRelEmbed size: {num_function_rels}\")\n\n    def _compute_advanced_hrpe_bias(self, query_root_pc, key_root_pc, query_quality, key_quality, query_function, key_function, query_token_ids_for_padding, key_token_ids_for_padding):\n        batch_size, seq_len_q = query_root_pc.shape\n        _, seq_len_k = key_root_pc.shape\n        device = query_root_pc.device\n\n        rel_root_interval = (query_root_pc.unsqueeze(2) - key_root_pc.unsqueeze(1) + 12) % 12\n        num_qualities = int(math.sqrt(self.quality_rel_embed.num_embeddings))\n        rel_quality_code = query_quality.unsqueeze(2) * num_qualities + key_quality.unsqueeze(1)\n        num_functions = int(math.sqrt(self.function_rel_embed.num_embeddings))\n        rel_function_code = query_function.unsqueeze(2) * num_functions + key_function.unsqueeze(1)\n\n        root_bias_vectors = self.root_interval_embed(rel_root_interval.long().clamp_(0, self.root_interval_embed.num_embeddings - 1))\n        quality_bias_vectors = self.quality_rel_embed(rel_quality_code.long().clamp_(0, self.quality_rel_embed.num_embeddings - 1))\n        function_bias_vectors = self.function_rel_embed(rel_function_code.long().clamp_(0, self.function_rel_embed.num_embeddings - 1))\n\n        all_bias_vectors = [root_bias_vectors, quality_bias_vectors, function_bias_vectors]\n        bias_head_dim = root_bias_vectors.shape[-1] // self.nhead\n        reshaped_bias_vectors = [bv.view(batch_size, seq_len_q, seq_len_k, self.nhead, bias_head_dim) for bv in all_bias_vectors]\n\n        if self.bias_combiner:\n            combined_features = torch.cat(reshaped_bias_vectors, dim=-1)\n            combined_features_flat = combined_features.view(-1, self.num_feature_embeddings * bias_head_dim)\n            scalar_bias_flat = self.bias_combiner(combined_features_flat)\n            total_bias = scalar_bias_flat.view(batch_size, seq_len_q, seq_len_k, self.nhead)\n        else:\n            self.local_logger.warning(\"Bias combiner not found, using simple sum.\")\n            total_bias = torch.stack(reshaped_bias_vectors, dim=0).sum(dim=0).sum(dim=-1)\n\n        total_bias = total_bias.permute(0, 3, 1, 2)\n\n        if self.padding_idx is not None:\n            query_padding_mask = (query_token_ids_for_padding == self.padding_idx)\n            key_padding_mask = (key_token_ids_for_padding == self.padding_idx)\n            combined_pad_mask = (query_padding_mask.unsqueeze(2) | key_padding_mask.unsqueeze(1))\n            combined_pad_mask_expanded = combined_pad_mask.unsqueeze(1).expand_as(total_bias)\n            total_bias = total_bias.masked_fill(combined_pad_mask_expanded, 0.0)\n\n        return total_bias\n\n    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None, root_pc=None, quality_code=None, function_code=None, query_tokens=None, key_tokens=None):\n        feature_sets = [(root_pc, \"root_pc\"), (quality_code, \"quality_code\"), (function_code, \"function_code\")]\n        if query_tokens is None or key_tokens is None: raise ValueError(\"Missing query_tokens/key_tokens required for H-RPE padding mask calculation.\")\n        for feat, name in feature_sets:\n            if feat is None: raise ValueError(f\"Missing required auxiliary feature for Advanced H-RPE: {name}\")\n\n        batch_size, seq_len_q, _ = query.shape\n        seq_len_k = key.shape[1]\n\n        q = self.q_proj(query)\n        k = self.k_proj(key)\n        v = self.v_proj(value)\n\n        q = q.view(batch_size, seq_len_q, self.nhead, self.head_dim).transpose(1, 2)\n        k = k.view(batch_size, seq_len_k, self.nhead, self.head_dim).transpose(1, 2)\n        v = v.view(batch_size, seq_len_k, self.nhead, self.head_dim).transpose(1, 2)\n\n        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n\n        hrpe_bias = self._compute_advanced_hrpe_bias(\n            query_root_pc=root_pc, key_root_pc=root_pc,\n            query_quality=quality_code, key_quality=quality_code,\n            query_function=function_code, key_function=function_code,\n            query_token_ids_for_padding=query_tokens,\n            key_token_ids_for_padding=key_tokens\n        )\n\n        if hrpe_bias.shape != attn_scores.shape:\n             self.local_logger.error(f\"Shape mismatch! Attn: {attn_scores.shape}, Bias: {hrpe_bias.shape}. Check feature tensor shapes.\")\n             raise ValueError(\"Attention scores and H-RPE bias shape mismatch.\")\n        attn_scores = attn_scores + hrpe_bias.type_as(attn_scores)\n\n        mask_value = -torch.inf if attn_scores.dtype == torch.float32 else torch.finfo(attn_scores.dtype).min\n        device = query.device\n\n        if attn_mask is not None:\n            attn_mask_bool = attn_mask.to(device=device, dtype=torch.bool)\n            if attn_mask_bool.dim() == 2:\n                attn_mask_expanded = attn_mask_bool.unsqueeze(0).unsqueeze(0).expand_as(attn_scores)\n            elif attn_mask_bool.dim() == 3 and attn_mask_bool.shape[0] == batch_size:\n                attn_mask_expanded = attn_mask_bool.unsqueeze(1).expand_as(attn_scores)\n            elif attn_mask_bool.shape == attn_scores.shape:\n                attn_mask_expanded = attn_mask_bool\n            else:\n                raise ValueError(f\"Unsupported attn_mask shape {attn_mask.shape}, expected 2D, 3D(B,Sq,Sk), or 4D(B,H,Sq,Sk)\")\n            attn_scores = attn_scores.masked_fill(attn_mask_expanded, mask_value)\n\n        if key_padding_mask is not None:\n            key_padding_mask_bool = key_padding_mask.to(device=device, dtype=torch.bool)\n            key_padding_mask_expanded = key_padding_mask_bool.unsqueeze(1).unsqueeze(2).expand_as(attn_scores)\n            attn_scores = attn_scores.masked_fill(key_padding_mask_expanded, mask_value)\n\n        softmax_dtype = torch.float32 # Keep float32 for softmax robustness\n        attn_weights = F.softmax(attn_scores.to(softmax_dtype), dim=-1).to(attn_scores.dtype)\n        attn_weights = self.dropout(attn_weights)\n\n        output = torch.matmul(attn_weights, v)\n        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n        output = self.out_proj(output)\n        return output\n\n\nclass CustomTransformerEncoderLayerAdv(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward, dropout, padding_idx, **hrpe_kwargs):\n        super().__init__()\n        self.self_attn = AdvancedHarmonicRelativeAttention(d_model, nhead, dropout, padding_idx, **hrpe_kwargs)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.activation = nn.GELU()\n\n    def forward(self, src, src_mask=None, src_key_padding_mask=None, src_tokens=None, src_root_pc=None, src_quality=None, src_function=None):\n        if src_tokens is None:\n            raise ValueError(\"src_tokens (original token IDs) are required for H-RPE padding calculation in CustomTransformerEncoderLayerAdv.\")\n        if src_root_pc is None or src_quality is None or src_function is None:\n            raise ValueError(\"Missing auxiliary features (root_pc, quality, function) required by AdvancedHarmonicRelativeAttention.\")\n\n        attn_output = self.self_attn(\n            query=src, key=src, value=src,\n            key_padding_mask=src_key_padding_mask,\n            attn_mask=src_mask,\n            root_pc=src_root_pc,\n            quality_code=src_quality,\n            function_code=src_function,\n            query_tokens=src_tokens,\n            key_tokens=src_tokens\n        )\n\n        src = self.norm1(src + self.dropout1(attn_output))\n        ff_output = self.linear2(self.dropout(self.activation(self.linear1(src))))\n        src = self.norm2(src + self.dropout2(ff_output))\n        return src\n\n\nclass HarmonyTransformerWithAdvH_RPE(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, dropout, dim_feedforward, padding_idx: Optional[int], **hrpe_kwargs):\n        super().__init__()\n        global PADDING_VALUE\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.padding_idx = padding_idx\n\n        self.token_embedding = nn.Embedding(vocab_size, d_model, padding_idx=self.padding_idx)\n        self.dropout_embed = nn.Dropout(dropout)\n\n        valid_hrpe_keys = ['num_root_intervals', 'num_qualities', 'num_functions', 'relation_embedding_dim']\n        filtered_hrpe_kwargs = {k: v for k, v in hrpe_kwargs.items() if k in valid_hrpe_keys and v is not None}\n        encoder_layers = [\n            CustomTransformerEncoderLayerAdv(\n                d_model, nhead, dim_feedforward, dropout, padding_idx=self.padding_idx, **filtered_hrpe_kwargs\n            ) for _ in range(num_layers)\n        ]\n        self.transformer_encoder_layers = nn.ModuleList(encoder_layers)\n        self.encoder_norm = nn.LayerNorm(d_model)\n        self.fc_out = nn.Linear(d_model, vocab_size)\n\n        self._init_parameters()\n        self.local_logger = logging.getLogger(__name__ + \".HarmonyTransformerAdv\")\n        self.local_logger.info(f\"HarmonyTransformerWithAdvH_RPE initialized. Vocab: {vocab_size}, Padding ID: {self.padding_idx}\")\n        self.local_logger.info(f\" H-RPE Kwargs passed to layers: {filtered_hrpe_kwargs}\")\n\n\n    def _init_parameters(self):\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                nn.init.zeros_(module.bias)\n            elif isinstance(module, nn.Embedding) and name == 'token_embedding':\n                nn.init.normal_(module.weight, mean=0, std=0.02)\n                if module.padding_idx is not None:\n                    if 0 <= module.padding_idx < module.num_embeddings:\n                        with torch.no_grad():\n                            module.weight[module.padding_idx].fill_(0)\n                    else:\n                        self.local_logger.warning(f\"Invalid padding_idx ({module.padding_idx}) for embedding size ({module.num_embeddings}). Cannot zero padding.\")\n            elif isinstance(module, nn.LayerNorm):\n                nn.init.ones_(module.weight)\n                nn.init.zeros_(module.bias)\n            elif isinstance(module, AdvancedHarmonicRelativeAttention):\n                pass\n\n    def _generate_square_subsequent_mask(self, sz: int, device: torch.device) -> torch.Tensor:\n        mask = torch.triu(torch.ones((sz, sz), device=device), diagonal=1).bool()\n        return mask\n\n    def forward(self, src_tokens, root_pc=None, quality_code=None, function_code=None, src_mask=None, src_key_padding_mask=None):\n        global vocab_size\n\n        if self.padding_idx is not None:\n              valid_non_pad = (src_tokens != self.padding_idx)\n              if valid_non_pad.any() and \\\n                   (src_tokens[valid_non_pad].max() >= self.vocab_size or src_tokens[valid_non_pad].min() < 0):\n                  self.local_logger.debug(f\"Token IDs outside valid range detected (excluding padding). Range: [{src_tokens[valid_non_pad].min().item()}, {src_tokens[valid_non_pad].max().item()}], Vocab Size: {self.vocab_size}.\")\n        elif src_tokens.max() >= self.vocab_size or src_tokens.min() < 0:\n            self.local_logger.debug(f\"Token IDs outside valid range detected. Range: [{src_tokens.min().item()}, {src_tokens.max().item()}], Vocab Size: {self.vocab_size}.\")\n\n        if root_pc is None: raise ValueError(\"Missing required feature: root_pc\")\n        if quality_code is None: raise ValueError(\"Missing required feature: quality_code\")\n        if function_code is None: raise ValueError(\"Missing required feature: function_code\")\n        expected_shape = src_tokens.shape\n        if root_pc.shape != expected_shape: raise ValueError(f\"Shape mismatch: root_pc {root_pc.shape} vs src_tokens {expected_shape}\")\n        if quality_code.shape != expected_shape: raise ValueError(f\"Shape mismatch: quality_code {quality_code.shape} vs src_tokens {expected_shape}\")\n        if function_code.shape != expected_shape: raise ValueError(f\"Shape mismatch: function_code {function_code.shape} vs src_tokens {expected_shape}\")\n\n        batch_size, seq_len = src_tokens.shape\n        device = src_tokens.device\n\n        if src_mask is None:\n            src_mask = self._generate_square_subsequent_mask(seq_len, device)\n        elif src_mask.device != device:\n            src_mask = src_mask.to(device)\n\n        if src_key_padding_mask is None and self.padding_idx is not None:\n            src_key_padding_mask = (src_tokens == self.padding_idx)\n        elif src_key_padding_mask is not None and src_key_padding_mask.device != device:\n            src_key_padding_mask = src_key_padding_mask.to(device)\n        if src_key_padding_mask is not None:\n            src_key_padding_mask = src_key_padding_mask.bool()\n\n        embedded = self.token_embedding(src_tokens) * math.sqrt(self.d_model)\n        embedded = self.dropout_embed(embedded)\n\n        output = embedded\n        for layer in self.transformer_encoder_layers:\n            output = layer(\n                src=output,\n                src_mask=src_mask,\n                src_key_padding_mask=src_key_padding_mask,\n                src_tokens=src_tokens,\n                src_root_pc=root_pc,\n                src_quality=quality_code,\n                src_function=function_code\n            )\n\n        if self.encoder_norm is not None:\n            output = self.encoder_norm(output)\n\n        logits = self.fc_out(output)\n        return logits\n\n\n    # --- Generation Method (Needs careful device handling for XLA) ---\n    @torch.no_grad()\n    def generate(self, start_token_ids, start_features: Dict[str, torch.Tensor], max_length=32, temperature=1.0, top_k=0, top_p=0.9, get_features_for_id_fn=None ):\n        self.eval()\n        # Assume this is called on the correct device already (e.g., master's XLA device)\n        device = start_token_ids.device\n\n        if get_features_for_id_fn is None:\n              raise ValueError(\"`get_features_for_id_fn` is required for autoregressive generation with features.\")\n\n        current_token_ids = start_token_ids # Assume already on device\n\n        expected_feature_keys = HPARAMS.get('feature_keys', [])\n        if not expected_feature_keys:\n              logger.warning(\"`HPARAMS['feature_keys']` not found, cannot prepare features for generation.\")\n              return torch.empty((start_token_ids.shape[0], 0), dtype=torch.long, device=device)\n\n        current_features = {}\n        for k in expected_feature_keys:\n            if k not in start_features:\n                logger.error(f\"Generation start features missing required key: '{k}'\")\n                return torch.empty((start_token_ids.shape[0], 0), dtype=torch.long, device=device)\n            current_features[k] = start_features[k] # Assume already on device\n\n        generated_ids_list = []\n\n        for step in range(max_length):\n            input_ids_step = current_token_ids\n            input_features_step = current_features\n            current_seq_len = input_ids_step.shape[1]\n\n            causal_mask = self._generate_square_subsequent_mask(current_seq_len, device)\n            current_padding_idx = self.padding_idx if hasattr(self, 'padding_idx') else PADDING_VALUE\n            padding_mask = (input_ids_step == current_padding_idx) if current_padding_idx is not None else None\n\n            try:\n                logits = self(\n                    src_tokens=input_ids_step,\n                    src_mask=causal_mask,\n                    src_key_padding_mask=padding_mask,\n                    root_pc=input_features_step.get('root_pc'),\n                    quality_code=input_features_step.get('quality_code'),\n                    function_code=input_features_step.get('function_code')\n                )\n            except Exception as e:\n                self.local_logger.error(f\"Error during generation forward pass (step {step+1}): {e}\", exc_info=True)\n                break\n\n            next_token_logits = logits[:, -1, :]\n\n            if temperature > 1e-8:\n                 next_token_logits = next_token_logits / temperature\n            else:\n                 temperature = 0 # Argmax case\n\n            if top_k > 0 and temperature > 0:\n                k = min(top_k, next_token_logits.size(-1))\n                top_k_values, _ = torch.topk(next_token_logits, k, dim=-1)\n                kth_value = top_k_values[:, [-1]]\n                indices_to_remove = next_token_logits < kth_value\n                next_token_logits[indices_to_remove] = -float('Inf')\n\n            if 0 < top_p < 1.0 and temperature > 0:\n                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n                cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n                sorted_indices_to_remove = cum_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                sorted_indices_to_remove[..., 0] = 0\n                indices_to_remove = torch.zeros_like(next_token_logits, dtype=torch.bool).scatter_(\n                    dim=-1, index=sorted_indices, src=sorted_indices_to_remove\n                )\n                next_token_logits[indices_to_remove] = -float('Inf')\n\n            if temperature <= 1e-8:\n                next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n            else:\n                probs = F.softmax(next_token_logits.float(), dim=-1)\n                probs = torch.nan_to_num(probs, nan=0.0)\n\n                if torch.sum(probs, dim=-1).min() < 1e-6:\n                    self.local_logger.warning(f\"Generation probabilities collapsed at step {step+1}. Using argmax.\")\n                    next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n                else:\n                    try:\n                        next_token_id = torch.multinomial(probs, num_samples=1)\n                    except RuntimeError as e:\n                        self.local_logger.warning(f\"Multinomial sampling failed at step {step+1}: {e}. Using argmax.\")\n                        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n\n            generated_ids_list.append(next_token_id)\n            current_token_ids = torch.cat([current_token_ids, next_token_id], dim=1)\n\n            try:\n                # get_features_for_id expects CPU tensor, returns dict of CPU tensors\n                new_features_cpu_dict = get_features_for_id_fn(next_token_id.squeeze(-1).cpu())\n            except Exception as e:\n                self.local_logger.error(f\"Error calling get_features_for_id_fn (step {step+1}): {e}\", exc_info=True)\n                generated_ids_list = [] ; break\n\n            loop_broken = False\n            for key in current_features.keys():\n                if key not in new_features_cpu_dict:\n                    self.local_logger.error(f\"Feature key '{key}' missing from get_features_for_id_fn output (step {step+1}). Stopping generation.\")\n                    generated_ids_list = [] ; loop_broken = True; break\n\n                try:\n                    # Move new feature tensor back to the correct device\n                    new_feature_tensor = new_features_cpu_dict[key].to(device) # Shape should be (B, 1)\n                except Exception as e:\n                     self.local_logger.error(f\"Error processing feature '{key}' from get_features_for_id_fn (step {step+1}): {e}\", exc_info=True)\n                     generated_ids_list = [] ; loop_broken = True; break\n\n                if new_feature_tensor.dim() != 2 or new_feature_tensor.shape[0] != current_features[key].shape[0] or new_feature_tensor.shape[1] != 1:\n                    self.local_logger.error(f\"New feature '{key}' has incorrect shape {new_feature_tensor.shape}. Expected ({current_features[key].shape[0]}, 1) at step {step+1}. Stopping generation.\")\n                    generated_ids_list = [] ; loop_broken = True; break\n\n                current_features[key] = torch.cat([current_features[key], new_feature_tensor], dim=1)\n            if loop_broken: break\n\n        if generated_ids_list:\n             return torch.cat(generated_ids_list, dim=1)\n        else:\n             return torch.empty((start_token_ids.shape[0], 0), dtype=torch.long, device=device)\n\n\n# ==============================================================\n# === HELPER & TRAINING/EVAL LOOPS (Modified for XLA) ===\n# ==============================================================\n\nclass WarmUpLR(torch.optim.lr_scheduler.LambdaLR):\n    def __init__(self, optimizer, warm_up_steps, last_epoch=-1):\n        if warm_up_steps <= 0:\n            self.warm_up_steps = 0\n        else:\n            self.warm_up_steps = warm_up_steps\n        super().__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n\n    def lr_lambda(self, step):\n        current_step = max(0, step)\n        if self.warm_up_steps == 0:\n            return 1.0\n        if current_step < self.warm_up_steps:\n            return float(current_step + 1) / float(self.warm_up_steps)\n        return 1.0\n\ndef save_checkpoint(state: Dict[str, Any], is_best: bool, best_model_path: Optional[str], checkpoint_dir: Path, latest_filename: str, best_prefix: str):\n    # This function should only be called by the master process\n    local_logger = logging.getLogger(__name__ + \".save_checkpoint\")\n    try:\n        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n    except OSError as e:\n        local_logger.error(f\"Could not create checkpoint directory {checkpoint_dir}: {e}\")\n        return best_model_path\n\n    latest_filepath = checkpoint_dir / latest_filename\n    epoch_num = state.get('epoch', 0)\n    metric_name = state.get('metric_name', 'loss')\n    metric_value = state.get('best_metric_value', float('inf'))\n\n    if isinstance(metric_value, float) and math.isfinite(metric_value):\n        metric_value_str = f\"{metric_value:.4f}\".replace('.', '_')\n    else:\n        metric_value_str = \"inf\" if metric_value == float('inf') else \"nan\"\n\n    try:\n        xm.save(state, latest_filepath, master_only=True) # Ensure master_only safety\n    except Exception as e:\n        local_logger.error(f\"Error saving latest checkpoint to {latest_filepath}: {e}\", exc_info=True)\n\n    current_best_filepath = None\n    if is_best:\n        best_filename = f\"{best_prefix}{epoch_num+1}_{metric_name.lower()}_{metric_value_str}.pth\"\n        current_best_filepath = checkpoint_dir / best_filename\n        try:\n            xm.save(state, current_best_filepath, master_only=True) # Ensure master_only safety\n            local_logger.info(f\"Saved **best** checkpoint: {current_best_filepath} ({metric_name}={metric_value:.4f})\")\n\n            if best_model_path and Path(best_model_path).resolve() != current_best_filepath.resolve():\n                previous_best = Path(best_model_path)\n                if previous_best.exists() and previous_best.is_file():\n                    try:\n                        previous_best.unlink()\n                        local_logger.info(f\"Removed previous best checkpoint: {best_model_path}\")\n                    except OSError as e:\n                        local_logger.warning(f\"Could not remove previous best checkpoint {best_model_path}: {e}\")\n            return str(current_best_filepath)\n        except Exception as e:\n            local_logger.error(f\"Error saving best checkpoint to {current_best_filepath}: {e}\", exc_info=True)\n            return best_model_path\n    else:\n        save_freq = HPARAMS.get('save_every_n_epochs', 0)\n        if save_freq > 0 and (epoch_num + 1) % save_freq == 0:\n            periodic_filename = f\"checkpoint_ep{epoch_num+1}.pth\"\n            periodic_filepath = checkpoint_dir / periodic_filename\n            try:\n                if not (is_best and current_best_filepath and periodic_filepath.resolve() == current_best_filepath.resolve()):\n                     xm.save(state, periodic_filepath, master_only=True) # Ensure master_only safety\n                     local_logger.info(f\"Saved periodic checkpoint: {periodic_filepath}\")\n            except Exception as e:\n                local_logger.error(f\"Error saving periodic checkpoint to {periodic_filepath}: {e}\", exc_info=True)\n        return best_model_path\n\n\ndef load_checkpoint(checkpoint_path: Optional[str], model: nn.Module, optimizer: Optional[torch.optim.Optimizer]=None, scheduler: Optional[Any]=None) -> Tuple[int, float, Optional[str]]:\n    local_logger = logging.getLogger(__name__ + \".load_checkpoint\")\n    start_epoch = 0\n    best_metric_value = float('inf')\n    best_model_path = None\n    device = xm.xla_device() # Get the target device\n\n    checkpoint_path_obj = Path(checkpoint_path) if checkpoint_path else None\n\n    if checkpoint_path_obj and checkpoint_path_obj.is_file():\n        local_logger.info(f\"Attempting to load checkpoint: '{checkpoint_path_obj}'\")\n        try:\n            checkpoint = torch.load(checkpoint_path_obj, map_location='cpu')\n\n            if 'model_state_dict' not in checkpoint:\n                raise KeyError(\"Checkpoint missing 'model_state_dict'\")\n\n            state_dict = checkpoint['model_state_dict']\n            if all(key.startswith('module.') for key in state_dict.keys()):\n                local_logger.info(\"Removing 'module.' prefix from state_dict keys.\")\n                state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}\n\n            model.load_state_dict(state_dict)\n            model.to(device) # Move model to XLA device AFTER loading state\n            local_logger.info(\"Model state loaded successfully and moved to XLA device.\")\n\n            if optimizer and 'optimizer_state_dict' in checkpoint:\n                 local_logger.warning(\"Loading optimizer state from checkpoint is not fully implemented for XLA in this script. Optimizer will be re-initialized.\")\n                 pass\n            if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:\n                 local_logger.warning(\"Loading scheduler state from checkpoint is not fully implemented for XLA in this script. Scheduler will be re-initialized.\")\n                 pass\n\n            start_epoch = checkpoint.get('epoch', -1) + 1\n            best_metric_value = checkpoint.get('best_metric_value', float('inf'))\n            best_model_path = checkpoint.get('best_model_path', None) # Path stored in the checkpoint\n            metric_name = checkpoint.get('metric_name', 'loss')\n\n            local_logger.info(f\"Checkpoint loaded. Resuming from epoch {start_epoch}.\")\n            local_logger.info(f\" Previous best '{metric_name}': {best_metric_value:.4f}\")\n\n            if best_model_path and not Path(best_model_path).exists():\n                 logger.warning(f\"Stored best model path '{best_model_path}' in checkpoint is invalid or file missing.\")\n\n        except FileNotFoundError:\n            local_logger.error(f\"Checkpoint file not found at '{checkpoint_path_obj}'. Starting from scratch.\")\n        except Exception as e:\n            local_logger.error(f\"Failed to load checkpoint from '{checkpoint_path_obj}': {e}\", exc_info=True)\n            start_epoch = 0; best_metric_value = float('inf'); best_model_path = None\n    else:\n        if checkpoint_path:\n             local_logger.warning(f\"Checkpoint path specified ('{checkpoint_path}') but not found or invalid. Starting from scratch.\")\n        else:\n             local_logger.info(\"No checkpoint specified. Starting from scratch.\")\n\n    return start_epoch, best_metric_value, best_model_path\n\n\ndef train_epoch(rank, model, dataloader, optimizer, criterion, scheduler, device, epoch_num, num_epochs, padding_idx):\n    local_logger = logging.getLogger(__name__ + \".train_epoch\")\n    model.train()\n    total_loss = torch.tensor(0.0, device=device)\n    total_correct = torch.tensor(0.0, device=device)\n    total_items = torch.tensor(0.0, device=device)\n\n    global vocab_size\n    current_vocab_size = model.vocab_size if hasattr(model, 'vocab_size') else vocab_size\n\n    para_loader = pl.ParallelLoader(dataloader, [device])\n    data_iterator = para_loader.per_device_loader(device)\n\n    # Progress bar only shown on master core\n    progress_bar = tqdm(data_iterator,\n                        desc=f\"Train Epoch {epoch_num}/{num_epochs} [Core {rank}]\",\n                        unit=\"batch\",\n                        leave=False, # Keep leave=False so it disappears after the loop\n                        dynamic_ncols=True,\n                        disable=not xm.is_master_ordinal(),\n                        mininterval=5.0 # Update at most every 5 seconds\n                        )\n\n    batch_count = 0\n    for batch_data in progress_bar:\n        batch_count += 1\n        if batch_data is None:\n            local_logger.warning(f\"Skipping empty batch {batch_count} from ParallelLoader.\")\n            continue\n\n        try:\n            input_ids, target_ids_single, features_dict, _ = batch_data\n            if not isinstance(features_dict, dict) or not features_dict:\n                 local_logger.error(f\"Features dictionary is empty or not a dict in train batch {batch_count}. Skipping.\")\n                 continue\n        except (ValueError, TypeError) as e:\n            local_logger.error(f\"Skipping malformed train batch {batch_count}. Error: {e}\")\n            continue\n\n        if input_ids.numel() == 0 or target_ids_single.numel() == 0:\n            local_logger.warning(f\"Skipping batch {batch_count} with empty input or target tensors.\")\n            continue\n\n        features_dict_device = features_dict # Assume keys are already tensors on device\n\n        # Forward Pass\n        optimizer.zero_grad()\n        src_key_padding_mask = (input_ids == padding_idx) if padding_idx is not None else None\n\n        try:\n            output_logits = model(\n                src_tokens=input_ids,\n                src_key_padding_mask=src_key_padding_mask,\n                root_pc=features_dict_device.get('root_pc'),\n                quality_code=features_dict_device.get('quality_code'),\n                function_code=features_dict_device.get('function_code')\n            )\n            logits_for_next_token = output_logits[:, -1, :]\n\n            if torch.any((target_ids_single < 0) | (target_ids_single >= current_vocab_size)):\n                invalid_targets = target_ids_single[(target_ids_single < 0) | (target_ids_single >= current_vocab_size)]\n                local_logger.error(f\"Invalid target IDs found in Train Batch {batch_count} on Core {rank}! Values: {invalid_targets.unique().tolist()}. Vocab size: {current_vocab_size}. Clamping targets.\")\n                target_ids_safe = torch.clamp(target_ids_single, 0, current_vocab_size - 1).long()\n            else:\n                target_ids_safe = target_ids_single.long()\n\n            loss = criterion(logits_for_next_token.float(), target_ids_safe)\n\n        except Exception as e:\n             local_logger.error(f\"Error during forward/loss calculation in train batch {batch_count} on Core {rank}: {e}\", exc_info=True)\n             local_logger.error(f\" Shapes: Input {input_ids.shape}, Target {target_ids_single.shape}. Feature keys: {list(features_dict_device.keys())}\")\n             optimizer.zero_grad() # Ensure grads are zeroed\n             continue\n\n        # Backward Pass & Optimization (XLA specific)\n        if not torch.isfinite(loss):\n            local_logger.error(f\"NaN or Inf loss detected in Train Batch {batch_count} on Core {rank}! Loss: {loss.item()}. LR: {optimizer.param_groups[0]['lr']:.2e}. Skipping backward pass.\")\n            optimizer.zero_grad()\n            continue\n\n        try:\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), HPARAMS['max_grad_norm'])\n            xm.optimizer_step(optimizer)\n            if scheduler:\n                scheduler.step()\n        except Exception as e:\n            local_logger.error(f\"Error during backward pass or optimizer step in train batch {batch_count} on Core {rank}: {e}\", exc_info=True)\n            optimizer.zero_grad()\n            continue\n\n        # Calculate Metrics (Local)\n        with torch.no_grad():\n            pred_next_token = logits_for_next_token.argmax(dim=-1)\n            batch_correct = (pred_next_token == target_ids_safe).sum() # Keep as tensor\n            batch_items = torch.tensor(target_ids_safe.numel(), device=device) # Keep as tensor\n\n            if batch_items > 0:\n                total_loss += loss.detach() * batch_items # Accumulate weighted loss tensor\n                total_correct += batch_correct\n                total_items += batch_items\n                # Keep per-batch postfix update commented out for less noise\n                # if xm.is_master_ordinal(): ... progress_bar.set_postfix(...) ...\n            else:\n                local_logger.warning(f\"Train batch {batch_count} on Core {rank} had zero valid items.\")\n\n    progress_bar.close()\n\n    # Aggregate metrics across all cores\n    total_loss_reduced = xm.mesh_reduce('train_total_loss', total_loss, lambda x: torch.sum(torch.stack(x)))\n    total_correct_reduced = xm.mesh_reduce('train_total_correct', total_correct, lambda x: torch.sum(torch.stack(x)))\n    total_items_reduced = xm.mesh_reduce('train_total_items', total_items, lambda x: torch.sum(torch.stack(x)))\n\n    # Calculate final epoch metrics\n    final_loss = total_loss_reduced.item() / total_items_reduced.item() if total_items_reduced.item() > 0 else float('inf')\n    final_acc = total_correct_reduced.item() / total_items_reduced.item() if total_items_reduced.item() > 0 else 0.0\n\n    if not math.isfinite(final_loss):\n        local_logger.warning(f\"Non-finite average training loss calculated across cores for epoch {epoch_num}: {final_loss}\")\n\n    return final_loss, final_acc\n\n\n@torch.no_grad()\ndef evaluate_epoch(rank, model, dataloader, criterion, device, padding_idx):\n    local_logger = logging.getLogger(__name__ + \".evaluate_epoch\")\n    model.eval()\n    total_loss = torch.tensor(0.0, device=device)\n    total_correct = torch.tensor(0.0, device=device)\n    total_items = torch.tensor(0.0, device=device)\n\n    global vocab_size\n    current_vocab_size = model.vocab_size if hasattr(model, 'vocab_size') else vocab_size\n\n    para_loader = pl.ParallelLoader(dataloader, [device])\n    data_iterator = para_loader.per_device_loader(device)\n\n    progress_bar = tqdm(data_iterator,\n                        desc=f\"Evaluate [Core {rank}]\",\n                        unit=\"batch\",\n                        leave=False, # Keep leave=False\n                        dynamic_ncols=True,\n                        disable=not xm.is_master_ordinal(),\n                        mininterval=5.0 # Update at most every 5 seconds\n                        )\n\n    batch_count = 0\n    for batch_data in progress_bar:\n        batch_count += 1\n        if batch_data is None: continue\n        try:\n            input_ids, target_ids_single, features_dict, _ = batch_data\n            if not isinstance(features_dict, dict) or not features_dict:\n                local_logger.error(f\"Features dictionary is empty or not a dict in eval batch {batch_count} on Core {rank}. Skipping.\")\n                continue\n        except (ValueError, TypeError) as e:\n            local_logger.error(f\"Skipping malformed eval batch {batch_count} on Core {rank}. Error: {e}\")\n            continue\n\n        if input_ids.numel() == 0 or target_ids_single.numel() == 0: continue\n\n        features_dict_device = features_dict\n\n        # Forward Pass\n        src_key_padding_mask = (input_ids == padding_idx) if padding_idx is not None else None\n\n        try:\n            output_logits = model(\n                src_tokens=input_ids,\n                src_key_padding_mask=src_key_padding_mask,\n                root_pc=features_dict_device.get('root_pc'),\n                quality_code=features_dict_device.get('quality_code'),\n                function_code=features_dict_device.get('function_code')\n            )\n            logits_for_next_token = output_logits[:, -1, :]\n\n            if torch.any((target_ids_single < 0) | (target_ids_single >= current_vocab_size)):\n                invalid_targets = target_ids_single[(target_ids_single < 0) | (target_ids_single >= current_vocab_size)]\n                local_logger.error(f\"Invalid target IDs found in Eval Batch {batch_count} on Core {rank}! Values: {invalid_targets.unique().tolist()}. Vocab size: {current_vocab_size}. Clamping targets.\")\n                target_ids_safe = torch.clamp(target_ids_single, 0, current_vocab_size - 1).long()\n            else:\n                target_ids_safe = target_ids_single.long()\n\n            loss = criterion(logits_for_next_token.float(), target_ids_safe)\n\n        except Exception as e:\n             local_logger.error(f\"Error during forward/loss calculation in eval batch {batch_count} on Core {rank}: {e}\", exc_info=True)\n             continue\n\n        # Calculate Metrics\n        if torch.isfinite(loss):\n            pred_next_token = logits_for_next_token.argmax(dim=-1)\n            batch_correct = (pred_next_token == target_ids_safe).sum()\n            batch_items = torch.tensor(target_ids_safe.numel(), device=device)\n\n            if batch_items > 0:\n                total_loss += loss.detach() * batch_items\n                total_correct += batch_correct\n                total_items += batch_items\n                # Keep per-batch postfix update commented out for less noise\n                # if xm.is_master_ordinal(): ... progress_bar.set_postfix(...) ...\n            else:\n                local_logger.warning(f\"Eval batch {batch_count} on Core {rank} had zero valid items.\")\n        else:\n            local_logger.warning(f\"NaN or Inf loss detected in Eval Batch {batch_count} on Core {rank}! Loss: {loss.item()}. Skipping metrics.\")\n\n    progress_bar.close()\n\n    # Aggregate metrics across all cores\n    total_loss_reduced = xm.mesh_reduce('eval_total_loss', total_loss, lambda x: torch.sum(torch.stack(x)))\n    total_correct_reduced = xm.mesh_reduce('eval_total_correct', total_correct, lambda x: torch.sum(torch.stack(x)))\n    total_items_reduced = xm.mesh_reduce('eval_total_items', total_items, lambda x: torch.sum(torch.stack(x)))\n\n\n    final_loss = total_loss_reduced.item() / total_items_reduced.item() if total_items_reduced.item() > 0 else float('inf')\n    final_acc = total_correct_reduced.item() / total_items_reduced.item() if total_items_reduced.item() > 0 else 0.0\n    perplexity = float('inf')\n\n    if math.isfinite(final_loss) and final_loss >= 0:\n        try:\n            perplexity = math.exp(final_loss)\n        except OverflowError:\n            local_logger.warning(f\"Cannot calculate perplexity due to overflow with loss: {final_loss}\")\n            perplexity = float('inf')\n    elif total_items_reduced.item() > 0:\n        local_logger.warning(f\"Non-finite average evaluation loss across cores: {final_loss}. Perplexity will be infinite.\")\n\n    return final_loss, final_acc, perplexity\n\n\n# --- SSMD Calculation Helpers (Mostly unchanged, run on CPU) ---\ndef decode_sequence_strings(token_ids: List[int], id_to_vocab_map: Dict[int, str], padding_id: Optional[int]) -> List[str]:\n    decoded = []\n    for t_id in token_ids:\n        if padding_id is not None and t_id == padding_id: continue\n        label = id_to_vocab_map.get(t_id)\n        if label is not None: decoded.append(label)\n    return decoded\n\ndef parse_function_code(label: Optional[str]) -> Optional[int]:\n    # !!! Needs customization based on your specific vocab/labels !!!\n    # This is just an EXAMPLE based on common Roman Numeral Analysis patterns\n    if label:\n        # Simple checks (adjust based on your actual function codes in token_features.json)\n        if label == 'I': return 1\n        if label == 'V': return 5\n        if label == 'IV': return 4\n        if label == 'ii': return 2\n        if label == 'vi': return 6\n        if label == 'iii': return 3\n        if label == 'vii': return 7 # Often diminished\n        if label == 'I7': return 1 # Assuming dominant function overrides quality here\n        if label == 'V7': return 5\n        if label == 'IV7': return 4\n        # Add more specific checks for qualities (major/minor/dim/aug/etc.) if needed\n        # e.g., if 'i' in label and 'vi' not in label: return 1 # Minor tonic\n        # Map padding token if needed\n        global PADDING_VALUE, PAD_TOKEN, token_to_features_map\n        if PADDING_VALUE is not None and label == PAD_TOKEN:\n             # Return the function code associated with PAD_TOKEN in the map, default to 0\n             return token_to_features_map.get(PADDING_VALUE, {}).get('function_code', 0)\n    logging.debug(f\"Could not parse function code for label: {label}\")\n    return None # Important: return None if unparseable\n\ndef extract_function_codes(decoded_labels: List[str]) -> List[int]:\n    codes = []\n    for label in decoded_labels:\n        code = parse_function_code(label)\n        if code is not None: codes.append(code)\n    return codes\n\ndef calculate_ssm(feature_sequence: List[int], metric='cosine') -> Optional[np.ndarray]:\n    if len(feature_sequence) < 2: return None\n    features = np.array(feature_sequence).reshape(-1, 1)\n    if np.all(features == features[0]):\n        logging.debug(\"SSM calculation skipped: feature sequence is constant.\")\n        return None\n    try:\n        if metric == 'cosine':\n            if np.std(features) < 1e-9:\n                logging.debug(\"SSM calculation skipped: near-zero variance for cosine.\")\n                return None\n        # pdist requires at least 2 points and >0 dimensions\n        if features.shape[0] < 2 or features.shape[1] == 0:\n            logging.debug(f\"SSM calculation skipped: Insufficient data points or dimensions. Shape: {features.shape}\")\n            return None\n        dist_condensed = pdist(features, metric=metric)\n        ssm = squareform(dist_condensed)\n        if metric == 'cosine': ssm = 1.0 - ssm\n        ssm = np.nan_to_num(ssm, nan=0.0)\n        return ssm\n    except ValueError as ve:\n        # Handle cases where pdist fails, e.g., input contains NaN/inf\n        logging.warning(f\"ValueError calculating SSM with metric '{metric}': {ve}. Input shape: {features.shape}\", exc_info=False)\n        return None\n    except Exception as e:\n        logging.error(f\"Error calculating SSM with metric '{metric}': {e}\", exc_info=False)\n        return None\n\ndef calculate_mean_ssmd(ssm: Optional[np.ndarray], k: int = 1) -> Optional[float]:\n    if ssm is None or not isinstance(ssm, np.ndarray) or ssm.ndim != 2 or ssm.shape[0] != ssm.shape[1]: return None\n    n = ssm.shape[0]\n    if not (0 <= k < n): return None\n    if n <= k: return None # Need more elements than k for the k-th diagonal\n    diag_k = np.diag(ssm, k=k)\n    if diag_k.size == 0: return None\n    # Use nanmean to ignore potential NaNs if any slip through ssm calculation\n    mean_val = np.nanmean(diag_k)\n    return float(mean_val) if np.isfinite(mean_val) else None\n\n# --- End SSMD Helpers ---\n\n\n# === NEW: Per-Epoch SSMD Evaluation Function ===\n@torch.no_grad()\ndef calculate_ssmd_on_validation_set(model: nn.Module,\n                                     val_dataset: Dataset, # Pass the actual validation dataset subset\n                                     device: torch.device,\n                                     hparams: Dict[str, Any],\n                                     padding_idx: int,\n                                     id_to_vocab_map: Dict[int, str],\n                                     get_features_fn: callable,\n                                     epoch_num: int) -> Tuple[Optional[float], Optional[float]]:\n    \"\"\"Calculates Generation and Reference SSMD(k=1) on (a subset of) the validation set. Runs on master.\"\"\"\n    local_logger = logging.getLogger(__name__ + \".calculate_ssmd_on_validation_set\")\n    model.eval() # Ensure model is in eval mode\n\n    ssmd_metric = 'cosine'\n    ssmd_k = 1\n    ssmd_context_len = hparams.get('ssmd_context_len', 16)\n    ssmd_gen_len = hparams.get('max_gen_len', 16)\n    current_seq_len = hparams.get('sequence_length', 32)\n    eval_batch_size = hparams.get('eval_batch_size', 16)\n    num_workers = hparams.get('num_workers', 2) # Can use fewer workers for this potentially smaller eval\n    max_batches = hparams.get('ssmd_eval_max_batches', None)\n\n    if ssmd_context_len >= current_seq_len: ssmd_context_len = max(1, current_seq_len // 2)\n    if ssmd_gen_len <= 0: ssmd_gen_len = 4\n    if ssmd_context_len <= 0: ssmd_context_len = 1\n\n    local_logger.info(f\"Epoch {epoch_num} SSMD: Running calculation (Context={ssmd_context_len}, Gen={ssmd_gen_len}, MaxBatches={max_batches})\")\n\n    # Create a DataLoader specifically for the master process on the validation subset\n    # Use SequentialSampler for deterministic order\n    val_sampler_master = torch.utils.data.SequentialSampler(val_dataset)\n    # Use the global collate function\n    collate_wrapper = lambda batch: collate_fn_progression(batch)\n    val_loader_master = DataLoader(val_dataset,\n                                   batch_size=eval_batch_size,\n                                   sampler=val_sampler_master,\n                                   num_workers=num_workers,\n                                   collate_fn=collate_wrapper,\n                                   drop_last=False)\n\n    all_gen_func_codes, all_ref_func_codes = [], []\n    batches_processed = 0\n\n    # Use tqdm.notebook for the batch progress within SSMD calculation\n    ssmd_pbar = tqdm(val_loader_master,\n                     desc=f\"Epoch {epoch_num} SSMD Gen\",\n                     unit=\"batch\",\n                     leave=False, # Don't leave the bar after completion\n                     dynamic_ncols=True)\n\n    for batch_data in ssmd_pbar:\n        if batch_data is None: continue\n        batches_processed += 1\n        if max_batches is not None and batches_processed > max_batches:\n            local_logger.info(f\"Epoch {epoch_num} SSMD: Reached max batches ({max_batches}).\")\n            break\n\n        try:\n            input_ids, _, features_dict, _ = batch_data\n            if input_ids.numel() == 0: continue\n\n            # Move data to the master's device\n            input_ids = input_ids.to(device)\n            features_dict_device = {k: v.to(device) for k, v in features_dict.items()}\n\n            actual_context_len = min(ssmd_context_len, input_ids.shape[1])\n            if actual_context_len == 0: continue\n\n            input_context_ids = input_ids[:, :actual_context_len]\n            input_context_features = {}\n            valid_context = True\n            expected_feature_keys_gen = hparams.get('feature_keys', [])\n            for k in expected_feature_keys_gen:\n                if k in features_dict_device:\n                    input_context_features[k] = features_dict_device[k][:, :actual_context_len]\n                else:\n                    local_logger.warning(f\"Epoch {epoch_num} SSMD: Missing feature '{k}' in batch {batches_processed}. Skipping batch.\")\n                    valid_context = False; break\n            if not valid_context: continue\n\n            # Generate sequences\n            generated_ids_tensor = model.generate(\n                start_token_ids=input_context_ids, start_features=input_context_features,\n                max_length=ssmd_gen_len, temperature=0.7, top_k=50, top_p=0.95, # Example sampling params\n                get_features_for_id_fn=get_features_fn\n            )\n\n            # Get reference sequences\n            ref_start_idx = actual_context_len\n            ref_end_idx = min(ref_start_idx + generated_ids_tensor.shape[1], input_ids.shape[1])\n            ref_ids_tensor = input_ids[:, ref_start_idx : ref_end_idx]\n\n            # Decode and extract function codes (runs on CPU)\n            if id_to_vocab_map is None:\n                local_logger.error(f\"Epoch {epoch_num} SSMD: id_to_vocab map is missing.\")\n                return None, None\n            batch_gen_labels = [decode_sequence_strings(seq, id_to_vocab_map, padding_idx) for seq in generated_ids_tensor.cpu().tolist()]\n            batch_ref_labels = [decode_sequence_strings(seq, id_to_vocab_map, padding_idx) for seq in ref_ids_tensor.cpu().tolist()]\n\n            all_gen_func_codes.extend([extract_function_codes(labels) for labels in batch_gen_labels])\n            all_ref_func_codes.extend([extract_function_codes(labels) for labels in batch_ref_labels])\n\n        except Exception as e:\n            local_logger.error(f\"Epoch {epoch_num} SSMD: Error during generation/processing batch {batches_processed}: {e}\", exc_info=True)\n            # Continue to next batch if possible\n            continue\n\n    ssmd_pbar.close()\n\n    # Calculate SSMD Scores (on CPU)\n    gen_ssmd_scores, ref_ssmd_scores = [], []\n    num_skipped_gen, num_skipped_ref = 0, 0\n\n    if not all_gen_func_codes or not all_ref_func_codes:\n         local_logger.warning(f\"Epoch {epoch_num} SSMD: No function codes extracted for SSMD calculation.\")\n         return None, None\n\n    # Simple loop, could add tqdm here if many sequences are processed\n    for gen_funcs, ref_funcs in zip(all_gen_func_codes, all_ref_func_codes):\n        ssm_gen = calculate_ssm(gen_funcs, metric=ssmd_metric)\n        ssmd_gen = calculate_mean_ssmd(ssm_gen, k=ssmd_k)\n        if ssmd_gen is not None and math.isfinite(ssmd_gen): gen_ssmd_scores.append(ssmd_gen)\n        else: num_skipped_gen += 1\n\n        ssm_ref = calculate_ssm(ref_funcs, metric=ssmd_metric)\n        ssmd_ref = calculate_mean_ssmd(ssm_ref, k=ssmd_k)\n        if ssmd_ref is not None and math.isfinite(ssmd_ref): ref_ssmd_scores.append(ssmd_ref)\n        else: num_skipped_ref += 1\n\n    if num_skipped_gen > 0 : local_logger.warning(f\"Epoch {epoch_num} SSMD: Skipped {num_skipped_gen} generated sequences during SSMD math.\")\n    if num_skipped_ref > 0 : local_logger.warning(f\"Epoch {epoch_num} SSMD: Skipped {num_skipped_ref} reference sequences during SSMD math.\")\n\n    avg_gen_ssmd = np.mean(gen_ssmd_scores) if gen_ssmd_scores else None\n    avg_ref_ssmd = np.mean(ref_ssmd_scores) if ref_ssmd_scores else None\n\n    # Convert numpy float types to standard Python floats for JSON/logging if necessary\n    final_avg_gen_ssmd = float(avg_gen_ssmd) if avg_gen_ssmd is not None and np.isfinite(avg_gen_ssmd) else None\n    final_avg_ref_ssmd = float(avg_ref_ssmd) if avg_ref_ssmd is not None and np.isfinite(avg_ref_ssmd) else None\n\n    local_logger.info(f\"Epoch {epoch_num} SSMD: Avg Gen={final_avg_gen_ssmd:.4f} ({len(gen_ssmd_scores)} valid), Avg Ref={final_avg_ref_ssmd:.4f} ({len(ref_ssmd_scores)} valid)\")\n\n    return final_avg_gen_ssmd, final_avg_ref_ssmd\n# === End Per-Epoch SSMD Function ===\n\n\n# === XLA Multiprocessing Function ===\ndef _mp_fn(rank, flags):\n    global HPARAMS, PADDING_VALUE, token_to_features_map, id_to_vocab, vocab, vocab_size, RESULTS_DIR # Declare globals modified here\n\n    # --- Per-Process Setup ---\n    torch.set_default_tensor_type('torch.FloatTensor') # Recommended for XLA\n    seed = HPARAMS['seed'] + rank\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    is_master = xm.is_master_ordinal()\n    xm.master_print(f\"Starting _mp_fn on master process {rank}/{xm.xrt_world_size()}\")\n    if is_master:\n         logger.info(f\"Master process seed set to {seed}\")\n    xm.rendezvous(f'Process {rank} setup complete')\n\n    # Get XLA device for this process\n    device = xm.xla_device()\n\n    # --- Load Vocab & Features (All processes need this info) ---\n    if is_master: logger.info(\"--- Loading Vocabulary & Features ---\")\n    try:\n        if not VOCAB_PATH.is_file(): raise FileNotFoundError(f\"Vocab file not found: {VOCAB_PATH}\")\n        with open(VOCAB_PATH, 'r') as f: vocab = json.load(f)\n        if not vocab or not isinstance(vocab, dict): raise ValueError(\"Vocab file is empty or invalid format.\")\n\n        try: id_to_vocab = {int(v): k for k, v in vocab.items()}\n        except ValueError as e: raise ValueError(f\"Invalid token ID in vocab: {e}\")\n        if not id_to_vocab: raise ValueError(\"ID->Vocab map creation failed.\")\n\n        if id_to_vocab: vocab_size = max(id_to_vocab.keys()) + 1\n        else: raise ValueError(\"Could not determine vocab size.\")\n\n        if PAD_TOKEN not in vocab: raise ValueError(f\"Padding token '{PAD_TOKEN}' not found in vocabulary!\")\n        PADDING_VALUE = vocab[PAD_TOKEN]\n        if not isinstance(PADDING_VALUE, int) or PADDING_VALUE < 0: raise ValueError(f\"Invalid Padding ID '{PADDING_VALUE}'\")\n\n        if is_master:\n            logger.info(f\"Vocabulary loaded. Size: {vocab_size}. Padding ID: {PADDING_VALUE}\")\n            logger.info(f\"Loading token features from: {TOKEN_FEATURES_PATH}\")\n\n        # Load Token Features\n        token_to_features_map = {} ; loaded_from_file = False\n        if TOKEN_FEATURES_PATH.is_file():\n            try:\n                with open(TOKEN_FEATURES_PATH, 'r') as f: raw_map = json.load(f)\n                token_to_features_map = {int(k): v for k, v in raw_map.items()}\n                loaded_from_file = True\n                if is_master:\n                    vocab_ids = set(vocab.values())\n                    map_ids = set(token_to_features_map.keys())\n                    if vocab_ids != map_ids: logger.warning(f\"Mismatch between vocab IDs and feature map IDs!\")\n                    expected_feature_keys = set(HPARAMS.get('feature_keys', []))\n                    if token_to_features_map:\n                         first_item_features = next(iter(token_to_features_map.values()))\n                         if isinstance(first_item_features, dict):\n                             found_keys = set(first_item_features.keys())\n                             if found_keys != expected_feature_keys: logger.warning(f\"Feature keys mismatch! Expected: {expected_feature_keys}, Found: {found_keys}\")\n                         else: raise ValueError(\"Invalid feature map format.\")\n                    if PADDING_VALUE is not None and PADDING_VALUE not in token_to_features_map:\n                         logger.warning(f\"Padding ID {PADDING_VALUE} missing from feature map. Adding default zeros.\")\n                         token_to_features_map[PADDING_VALUE] = {key: 0 for key in expected_feature_keys}\n                    elif PADDING_VALUE is not None:\n                         pad_features = token_to_features_map[PADDING_VALUE]\n                         if set(pad_features.keys()) != expected_feature_keys:\n                             logger.warning(f\"Padding ID {PADDING_VALUE} features missing keys. Adding defaults.\")\n                             for key in expected_feature_keys:\n                                 if key not in pad_features: pad_features[key] = 0\n            except Exception as e:\n                if is_master: logger.error(f\"Failed to load or process features from {TOKEN_FEATURES_PATH}: {e}\", exc_info=True)\n                raise\n        if not loaded_from_file: raise FileNotFoundError(f\"Required token feature map missing at {TOKEN_FEATURES_PATH}\")\n        if not token_to_features_map: raise ValueError(\"Loaded token_to_features_map is empty.\")\n        if is_master: logger.info(f\"Token feature map ready for {len(token_to_features_map)} tokens.\")\n\n    except Exception as e:\n        xm.master_print(f\"FATAL [Rank {rank}]: Error during Vocabulary or Feature Map loading: {e}\", file=sys.stderr)\n        raise SystemExit(f\"Setup failed on rank {rank}\") from e\n\n\n    # --- Define get_features_for_id_fn (uses global token_to_features_map) ---\n    def get_features_for_id(token_ids_cpu: torch.Tensor) -> Dict[str, torch.Tensor]:\n        global token_to_features_map, PADDING_VALUE, HPARAMS\n        if token_to_features_map is None: raise RuntimeError(\"token_to_features_map not initialized.\")\n\n        feature_keys = HPARAMS.get('feature_keys', [])\n        if not feature_keys: raise RuntimeError(\"Could not determine feature keys.\")\n\n        batch_features = {key: [] for key in feature_keys}\n        default_features = token_to_features_map.get(PADDING_VALUE, {key: 0 for key in feature_keys})\n\n        for token_id_int in token_ids_cpu.tolist():\n            features = token_to_features_map.get(token_id_int)\n            if features is None:\n                # logger.warning(f\"Token ID {token_id_int} not found in feature map, using defaults.\")\n                features = default_features\n\n            for key in feature_keys:\n                batch_features[key].append(features.get(key, default_features.get(key, 0)))\n\n        output_features = {}\n        for key, val_list in batch_features.items():\n            try:\n                output_features[key] = torch.tensor(val_list, dtype=torch.long).unsqueeze(1)\n            except Exception as e:\n                logger.error(f\"Error converting features for key '{key}' to tensor: {e}\")\n                raise RuntimeError(f\"Failed to create feature tensor for key '{key}'\") from e\n        return output_features\n    # --- End define get_features_for_id_fn ---\n\n\n    # --- Pre-run Checks (Master Only) ---\n    if is_master:\n        logger.info(\"--- Performing Pre-run Checks ---\")\n        errors = []\n        if not INPUT_SEQUENCES_PATH.is_file(): errors.append(f\"Input data missing: '{INPUT_SEQUENCES_PATH}'\")\n        if not VOCAB_PATH.is_file(): errors.append(f\"Vocab missing: '{VOCAB_PATH}'\")\n        if not TOKEN_FEATURES_PATH.is_file(): errors.append(f\"Token features missing: '{TOKEN_FEATURES_PATH}'\")\n        if token_to_features_map:\n            max_q_code = -1; max_f_code = -1\n            for feat_dict in token_to_features_map.values():\n                if isinstance(feat_dict, dict):\n                    if 'quality_code' in feat_dict: max_q_code = max(max_q_code, feat_dict['quality_code'])\n                    if 'function_code' in feat_dict: max_f_code = max(max_f_code, feat_dict['function_code'])\n            if HPARAMS.get('num_qualities', 0) <= max_q_code: errors.append(f\"HPARAM 'num_qualities' too small ({HPARAMS.get('num_qualities')} <= {max_q_code})\")\n            if HPARAMS.get('num_functions', 0) <= max_f_code: errors.append(f\"HPARAM 'num_functions' too small ({HPARAMS.get('num_functions')} <= {max_f_code})\")\n        if errors:\n            logger.critical(\"Pre-run checks failed:\\n\" + \"\\n\".join(f\"  - {e}\" for e in errors))\n            raise SystemExit(\"Pre-run checks failed on master\")\n        logger.info(\"Pre-run checks passed.\")\n        try:\n            RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n            logger.info(f\"Results directory: {RESULTS_DIR}\")\n        except OSError as e:\n            logger.critical(f\"FATAL: Cannot create results dir '{RESULTS_DIR}': {e}\", exc_info=True); sys.exit(1)\n\n    xm.rendezvous(f'Rank {rank} passed pre-run checks')\n\n\n    # --- Create Datasets & DataLoaders ---\n    if is_master: logger.info(\"--- Creating Datasets & DataLoaders ---\")\n    train_dataset_full, val_dataset_full, test_dataset_full = None, None, None # Define scope\n    try:\n        full_dataset = ChordProgressionDataset(INPUT_SEQUENCES_PATH, HPARAMS['sequence_length'])\n        if len(full_dataset) == 0: raise ValueError(\"Dataset is empty.\")\n\n        num_samples = len(full_dataset)\n        val_size = int(VAL_RATIO * num_samples)\n        test_size = int(TEST_RATIO * num_samples)\n        if VAL_RATIO > 0 and val_size == 0 and num_samples > 0: val_size = 1\n        if TEST_RATIO > 0 and test_size == 0 and num_samples > val_size : test_size = 1\n        test_size = min(test_size, num_samples - val_size)\n        if test_size < 0: test_size = 0\n        train_size = num_samples - val_size - test_size\n        if train_size + val_size + test_size != num_samples:\n             train_size = num_samples - val_size - test_size # Adjust train\n             if is_master: logger.warning(f\"Adjusting split sizes: T={train_size}, V={val_size}, Te={test_size}\")\n        if train_size < 0: raise ValueError(\"Negative train size.\")\n\n        if is_master: logger.info(f\"Splitting {num_samples} samples: Train={train_size}, Val={val_size}, Test={test_size}\")\n\n        generator = torch.Generator().manual_seed(HPARAMS['seed'])\n        # Assign to variables accessible later in the master process scope\n        train_dataset_full, val_dataset_full, test_dataset_full = random_split(full_dataset, [train_size, val_size, test_size], generator=generator)\n\n        # Distributed Samplers\n        train_sampler = None\n        if train_dataset_full and len(train_dataset_full) > 0:\n            train_sampler = torch.utils.data.distributed.DistributedSampler(\n                train_dataset_full, num_replicas=xm.xrt_world_size(), rank=rank, shuffle=True, seed=HPARAMS['seed']\n            )\n        val_sampler = None\n        if val_dataset_full and len(val_dataset_full) > 0:\n             val_sampler = torch.utils.data.distributed.DistributedSampler(\n                 val_dataset_full, num_replicas=xm.xrt_world_size(), rank=rank, shuffle=False\n             )\n        test_sampler = None\n        if test_dataset_full and len(test_dataset_full) > 0:\n             test_sampler = torch.utils.data.distributed.DistributedSampler(\n                 test_dataset_full, num_replicas=xm.xrt_world_size(), rank=rank, shuffle=False\n             )\n\n        # DataLoaders\n        train_loader, val_loader, test_loader = None, None, None\n        collate_wrapper = lambda batch: collate_fn_progression(batch)\n\n        if train_sampler and HPARAMS['batch_size'] > 0:\n            train_loader = DataLoader(train_dataset_full, batch_size=HPARAMS['batch_size'], sampler=train_sampler,\n                                      num_workers=HPARAMS['num_workers'], collate_fn=collate_wrapper, drop_last=True)\n            if is_master: logger.info(f\"Train DataLoader created. Batches per core per epoch: ~{len(train_loader)}\")\n        elif is_master: logger.warning(\"Train loader not created.\")\n\n        if val_sampler and HPARAMS['eval_batch_size'] > 0:\n             val_loader = DataLoader(val_dataset_full, batch_size=HPARAMS['eval_batch_size'], sampler=val_sampler,\n                                      num_workers=HPARAMS['num_workers'], collate_fn=collate_wrapper, drop_last=False)\n             if is_master: logger.info(f\"Validation DataLoader created. Batches per core: {len(val_loader)}\")\n        elif is_master: logger.warning(\"Validation loader not created.\")\n\n        if test_sampler and HPARAMS['eval_batch_size'] > 0:\n             test_loader = DataLoader(test_dataset_full, batch_size=HPARAMS['eval_batch_size'], sampler=test_sampler,\n                                      num_workers=HPARAMS['num_workers'], collate_fn=collate_wrapper, drop_last=False)\n             if is_master: logger.info(f\"Test DataLoader created. Batches per core: {len(test_loader)}\")\n        elif is_master: logger.warning(\"Test loader not created.\")\n\n    except Exception as e:\n        xm.master_print(f\"FATAL [Rank {rank}]: Error during Dataset/DataLoader creation: {e}\", file=sys.stderr)\n        raise SystemExit(f\"Data setup failed on rank {rank}\") from e\n\n\n    # --- Initialize Model ---\n    if vocab_size <= 0 or PADDING_VALUE is None:\n        xm.master_print(f\"FATAL [Rank {rank}]: Vocab size ({vocab_size}) or Padding Value ({PADDING_VALUE}) invalid.\", file=sys.stderr)\n        raise SystemExit(f\"Model init prereqs failed on rank {rank}\")\n\n    if is_master: logger.info(\"--- Initializing Model ---\")\n    try:\n        model_hparams = {\n            'vocab_size': vocab_size, 'd_model': HPARAMS['d_model'], 'nhead': HPARAMS['nhead'],\n            'num_layers': HPARAMS['num_layers'], 'dim_feedforward': HPARAMS['dim_feedforward'],\n            'dropout': HPARAMS['dropout'], 'padding_idx': PADDING_VALUE,\n            'num_root_intervals': HPARAMS['num_root_intervals'], 'num_qualities': HPARAMS['num_qualities'],\n            'num_functions': HPARAMS['num_functions'], 'relation_embedding_dim': HPARAMS['relation_embedding_dim']\n        }\n        model = HarmonyTransformerWithAdvH_RPE(**model_hparams).to(device)\n\n        if is_master:\n            num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n            logger.info(f\"Model initialized on device {device}. Trainable parameters: {num_params:,}\")\n\n    except Exception as e:\n        xm.master_print(f\"FATAL [Rank {rank}]: Model initialization failed: {e}\", file=sys.stderr)\n        raise SystemExit(f\"Model init failed on rank {rank}\") from e\n\n\n    # --- Optimizer, Scheduler, Criterion ---\n    if is_master: logger.info(\"--- Initializing Optimizer, Scheduler, Criterion ---\")\n    optimizer, scheduler, criterion = None, None, None\n    try:\n        lr = HPARAMS['lr']\n        if HPARAMS.get('lr_scale_factor', None) is None:\n            HPARAMS['lr_scale_factor'] = xm.xrt_world_size()\n        if HPARAMS['lr_scale_factor'] > 1:\n            lr *= HPARAMS['lr_scale_factor']\n            if is_master: logger.info(f\"Scaled learning rate by {HPARAMS['lr_scale_factor']} to {lr:.2e}\")\n\n        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n                                       lr=lr, weight_decay=HPARAMS['weight_decay'])\n        if is_master: logger.info(f\"Optimizer: AdamW (lr={lr:.2e}, weight_decay={HPARAMS['weight_decay']})\")\n\n        actual_warm_up_steps = HPARAMS['warm_up_steps']\n        if actual_warm_up_steps > 0:\n            scheduler = WarmUpLR(optimizer, actual_warm_up_steps)\n            if is_master: logger.info(f\"Scheduler: WarmUpLR (steps={actual_warm_up_steps})\")\n        else:\n            scheduler = None\n            if is_master: logger.info(\"Scheduler: None\")\n\n        criterion = nn.CrossEntropyLoss(ignore_index=PADDING_VALUE)\n        if is_master: logger.info(f\"Criterion: CrossEntropyLoss (ignore_index={PADDING_VALUE})\")\n\n    except Exception as e:\n        xm.master_print(f\"FATAL [Rank {rank}]: Optimizer/Scheduler/Criterion init failed: {e}\", file=sys.stderr)\n        raise SystemExit(f\"Setup failed on rank {rank}\") from e\n\n\n    # --- Load Checkpoint ---\n    start_epoch = 0\n    best_val_loss = float('inf')\n    best_model_path = None\n    if CHECKPOINT_TO_LOAD:\n        checkpoint_path_str = str(CHECKPOINT_TO_LOAD)\n        start_epoch, best_val_loss, best_model_path = load_checkpoint(\n            checkpoint_path=checkpoint_path_str, model=model, optimizer=optimizer, scheduler=scheduler\n        )\n        xm.rendezvous(f'Rank {rank} finished checkpoint loading')\n\n\n    # --- Training Loop ---\n    target_epochs = HPARAMS['num_epochs']\n    actual_start_epoch = start_epoch\n\n    if is_master:\n        logger.info(f\"=== STARTING TRAINING (Epochs {actual_start_epoch + 1} to {target_epochs}) ===\")\n    patience_counter = 0\n    start_train_time = time.time()\n    # <<< MODIFIED training_history initialization >>>\n    training_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_perplexity': [],\n                        'val_gen_ssmd': [], 'val_ref_ssmd': []}\n\n    if not train_loader:\n         if is_master: logger.error(\"Cannot start training: Training loader is not available.\")\n    elif actual_start_epoch >= target_epochs:\n         if is_master: logger.warning(f\"Start epoch ({actual_start_epoch}) >= target ({target_epochs}). No training needed.\")\n    else:\n        epoch_pbar = tqdm(range(actual_start_epoch, target_epochs), desc=\"Epochs\", unit=\"epoch\",\n                          initial=actual_start_epoch, total=target_epochs, dynamic_ncols=True,\n                          disable=not is_master) # Disable on non-master\n\n        for epoch in epoch_pbar:\n            epoch_num_display = epoch + 1\n            epoch_start_time = time.time()\n\n            if train_sampler: train_sampler.set_epoch(epoch)\n\n            # Training Phase\n            train_loss, train_acc = train_epoch(\n                rank, model, train_loader, optimizer, criterion, scheduler, device,\n                epoch_num_display, target_epochs, PADDING_VALUE\n            )\n            if is_master: # Log aggregated results on master\n                training_history['train_loss'].append(train_loss)\n                training_history['train_acc'].append(train_acc)\n\n            gc.collect() # Optional cleanup\n\n            # --- >>> MODIFIED Validation Phase <<< ---\n            val_loss, val_acc, val_perplexity = float('inf'), 0.0, float('inf')\n            avg_gen_ssmd, avg_ref_ssmd = None, None # Initialize SSMD metrics for the epoch\n\n            if val_loader:\n                if val_sampler: val_sampler.set_epoch(epoch) # Set epoch for sampler\n                val_loss, val_acc, val_perplexity = evaluate_epoch(\n                    rank, model, val_loader, criterion, device, PADDING_VALUE\n                )\n                if is_master: # Store standard val metrics\n                    training_history['val_loss'].append(val_loss)\n                    training_history['val_acc'].append(val_acc)\n                    training_history['val_perplexity'].append(val_perplexity)\n\n                # --- Calculate SSMD on Validation Set (Master Only, periodically) ---\n                if is_master and (epoch_num_display % HPARAMS['ssmd_eval_every_n_epochs'] == 0):\n                    try:\n                        # Make sure the validation dataset split is available\n                        if val_dataset_full: # Check if it was created successfully\n                            avg_gen_ssmd, avg_ref_ssmd = calculate_ssmd_on_validation_set(\n                                model=model, # Model is already on the master device\n                                val_dataset=val_dataset_full, # Use the validation split\n                                device=device,\n                                hparams=HPARAMS,\n                                padding_idx=PADDING_VALUE,\n                                id_to_vocab_map=id_to_vocab,\n                                get_features_fn=get_features_for_id,\n                                epoch_num=epoch_num_display\n                            )\n                        else:\n                             logger.warning(f\"Epoch {epoch_num_display}: Cannot calculate SSMD, validation dataset split not found.\")\n\n                    except Exception as ssmd_e:\n                        logger.error(f\"Epoch {epoch_num_display}: Error calculating SSMD: {ssmd_e}\", exc_info=True)\n                # --- End SSMD Calculation ---\n\n                if is_master: # Store SSMD results (will be None if not calculated this epoch)\n                    training_history['val_gen_ssmd'].append(avg_gen_ssmd)\n                    training_history['val_ref_ssmd'].append(avg_ref_ssmd)\n\n                # Construct status message for logging (INFO level)\n                if is_master:\n                    status_msg_parts = [\n                        f\"Ep {epoch_num_display}\",\n                        f\"Train L={train_loss:.4f} A={train_acc:.4f}\",\n                        f\"| Val L={val_loss:.4f} A={val_acc:.4f} PPL={val_perplexity:.2f}\"\n                    ]\n                    # Add SSMD scores if they were calculated this epoch\n                    if avg_gen_ssmd is not None:\n                        status_msg_parts.append(f\"GS={avg_gen_ssmd:.4f}\")\n                    #if avg_ref_ssmd is not None: # Optionally add Ref SSMD to log too\n                    #    status_msg_parts.append(f\"RefS={avg_ref_ssmd:.4f}\")\n                    status_msg = \" \".join(status_msg_parts)\n\n            else: # If no validation loader\n                 if is_master:\n                    # Append None for all validation metrics\n                    training_history['val_loss'].append(None)\n                    training_history['val_acc'].append(None)\n                    training_history['val_perplexity'].append(None)\n                    training_history['val_gen_ssmd'].append(None)\n                    training_history['val_ref_ssmd'].append(None)\n                    val_loss = train_loss # Use train loss for best model check if no val\n                    status_msg = f\"Ep {epoch_num_display}: Train L={train_loss:.4f} A={train_acc:.4f} | Val Skipped\"\n\n            # Log epoch summary (master only)\n            if is_master: logger.info(status_msg)\n\n\n            # Checkpointing & Early Stopping (Master Only)\n            if is_master:\n                current_metric = val_loss\n                metric_name = \"Val_Loss\" if val_loader else \"Train_Loss\"\n                is_best = False\n\n                if isinstance(current_metric, float) and math.isfinite(current_metric):\n                    if current_metric < best_val_loss:\n                        best_val_loss = current_metric\n                        patience_counter = 0\n                        is_best = True\n                        logger.info(f\"*** New best {metric_name}: {best_val_loss:.4f} at epoch {epoch_num_display} ***\")\n                    elif val_loader: # Only increment patience if validation happened\n                        patience_counter += 1\n                # Only increment patience if validation happened and metric was non-finite\n                elif val_loader:\n                     patience_counter += 1\n                     logger.warning(f\"Epoch {epoch_num_display}: Non-finite validation metric ({current_metric}). Patience: {patience_counter}/{HPARAMS['patience']}\")\n\n                checkpoint_state = {\n                    'epoch': epoch, 'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n                    'best_metric_value': best_val_loss, 'metric_name': metric_name,\n                    'best_model_path': best_model_path, 'hparams': HPARAMS,\n                    'vocab_size': vocab_size, 'padding_idx': PADDING_VALUE\n                }\n\n                saved_best_path_updated = save_checkpoint(\n                    state=checkpoint_state, is_best=is_best, best_model_path=best_model_path,\n                    checkpoint_dir=RESULTS_DIR, latest_filename=\"latest_checkpoint.pth\",\n                    best_prefix=CHECKPOINT_FILENAME_BEST_PREFIX\n                )\n                if is_best: best_model_path = saved_best_path_updated\n\n                # Early stopping check\n                if val_loader and HPARAMS['patience'] > 0 and patience_counter >= HPARAMS['patience']:\n                    logger.warning(f\"EARLY STOPPING triggered at epoch {epoch_num_display} after {patience_counter} epochs without improvement.\")\n                    epoch_pbar.close()\n                    break # Break loop on master\n\n                # --- UPDATE Epoch Progress Bar Postfix ---\n                epoch_duration = time.time() - epoch_start_time\n                # Use metrics calculated in *this* epoch for the postfix\n                epoch_pbar_postfix = {\n                    \"TrL\": f\"{train_loss:.3f}\",\n                    \"ValL\": f\"{val_loss:.3f}\" if val_loss is not None and math.isfinite(val_loss) else \"N/A\",\n                    \"ValAcc\": f\"{val_acc:.3f}\" if val_acc is not None else \"N/A\",\n                    # Display Gen SSMD score if calculated this epoch\n                    \"GS\": f\"{avg_gen_ssmd:.3f}\" if avg_gen_ssmd is not None else \"...\",\n                    \"Time\": f\"{epoch_duration:.1f}s\"\n                }\n                if patience_counter > 0 and val_loader and HPARAMS['patience'] > 0:\n                    epoch_pbar_postfix[\"Patience\"] = f\"{patience_counter}/{HPARAMS['patience']}\"\n                # Refresh=True ensures the bar updates immediately\n                epoch_pbar.set_postfix(epoch_pbar_postfix, refresh=True)\n                # --- End Update ---\n\n            # Barrier to ensure all processes finish the epoch and master handles checkpointing/stopping\n            xm.rendezvous(f'Rank {rank} finished epoch {epoch_num_display}')\n            # Check if master decided to stop early\n            if is_master and 'epoch_pbar' in locals() and epoch_pbar.n < epoch_pbar.total -1 and patience_counter >= HPARAMS['patience'] and val_loader and HPARAMS['patience'] > 0:\n                 # If master broke early, other ranks might need to exit loop too\n                 break\n        # --- End Epoch Loop ---\n\n        if is_master and 'epoch_pbar' in locals(): epoch_pbar.close()\n\n    if is_master:\n        logger.info(\"=== FINISHED TRAINING ===\")\n        total_train_time = time.time() - start_train_time\n        logger.info(f\"Total Training Duration: {total_train_time // 3600:.0f}h {(total_train_time % 3600) // 60:.0f}m {total_train_time % 60:.2f}s\")\n\n\n    # === Final Evaluation on Test Set (Master Only) ===\n    if is_master:\n        logger.info(f\"=== STARTING FINAL EVALUATION (on Master Core {rank}) ===\")\n        final_results = {}\n\n        if best_model_path and Path(best_model_path).exists():\n            logger.info(f\"Loading best model for final evaluation from: {best_model_path}\")\n            try:\n                checkpoint = torch.load(best_model_path, map_location='cpu')\n                loaded_hparams = checkpoint.get('hparams', HPARAMS)\n                loaded_vocab_size = checkpoint.get('vocab_size', vocab_size)\n                loaded_padding_idx = checkpoint.get('padding_idx', PADDING_VALUE)\n\n                eval_model_hparams = {\n                     'vocab_size': loaded_vocab_size, 'd_model': loaded_hparams['d_model'], 'nhead': loaded_hparams['nhead'],\n                     'num_layers': loaded_hparams['num_layers'], 'dim_feedforward': loaded_hparams['dim_feedforward'],\n                     'dropout': 0.0, 'padding_idx': loaded_padding_idx, # Eval dropout=0\n                     'num_root_intervals': loaded_hparams['num_root_intervals'], 'num_qualities': loaded_hparams['num_qualities'],\n                     'num_functions': loaded_hparams['num_functions'], 'relation_embedding_dim': loaded_hparams['relation_embedding_dim']\n                 }\n                eval_model = HarmonyTransformerWithAdvH_RPE(**eval_model_hparams)\n\n                state_dict = checkpoint['model_state_dict']\n                if all(key.startswith('module.') for key in state_dict.keys()):\n                     state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}\n                eval_model.load_state_dict(state_dict)\n                eval_model.to(device)\n                eval_model.eval()\n                logger.info(\"Best model loaded successfully onto master XLA device for evaluation.\")\n\n                # Evaluate on Test Set (Loss, Acc, PPL) using master's dataloader slice\n                if test_dataset_full and len(test_dataset_full) > 0: # Check if test set exists\n                    test_sampler_master = torch.utils.data.SequentialSampler(test_dataset_full)\n                    test_loader_master = DataLoader(test_dataset_full,\n                                                    batch_size=HPARAMS['eval_batch_size'],\n                                                    sampler=test_sampler_master,\n                                                    num_workers=HPARAMS['num_workers'],\n                                                    collate_fn=collate_wrapper,\n                                                    drop_last=False)\n\n                    if test_loader_master:\n                        logger.info(\"--- Evaluating on Test Set (Loss, Acc, PPL) on Master ---\")\n                        # Use rank 0 and master loader\n                        test_loss, test_acc, test_perplexity = evaluate_epoch(\n                            rank=0, model=eval_model, dataloader=test_loader_master,\n                            criterion=criterion, device=device, padding_idx=loaded_padding_idx\n                        )\n                        final_results['Test Loss'] = test_loss if math.isfinite(test_loss) else None\n                        final_results['Test Accuracy'] = test_acc\n                        final_results['Test Perplexity'] = test_perplexity if math.isfinite(test_perplexity) else None\n                        logger.info(\"--- Final Test Set Results (Standard Metrics) ---\")\n                        logger.info(f\"  Test Loss:         {test_loss:.4f}\")\n                        logger.info(f\"  Test Accuracy:     {test_acc:.4f}\") # <-- Final Test Accuracy\n                        logger.info(f\"  Test Perplexity:   {test_perplexity:.4f}\")\n\n                        # Calculate Final SSMD Metrics on Test Set\n                        logger.info(\"--- Calculating Final Advanced Metrics (SSMD on Function Codes) on Master Test Set ---\")\n                        # Reuse the calculate_ssmd_on_validation_set function, but pass the test dataset\n                        # Set max_batches to None to evaluate on the full test set\n                        final_gen_ssmd, final_ref_ssmd = calculate_ssmd_on_validation_set(\n                            model=eval_model,\n                            val_dataset=test_dataset_full, # Use test set data\n                            device=device,\n                            hparams={**loaded_hparams, 'ssmd_eval_max_batches': None}, # Override max batches\n                            padding_idx=loaded_padding_idx,\n                            id_to_vocab_map=id_to_vocab,\n                            get_features_fn=get_features_for_id,\n                            epoch_num=0 # Indicate it's final eval\n                        )\n\n                        ssmd_k_final = loaded_hparams.get('ssmd_k', 1) # Assuming k=1 was used\n                        ssmd_metric_final = 'cosine' # Assuming cosine was used\n                        final_results[f'Test Avg Gen SSMD(k={ssmd_k_final}, FuncCode, {ssmd_metric_final})'] = final_gen_ssmd\n                        final_results[f'Test Avg Ref SSMD(k={ssmd_k_final}, FuncCode, {ssmd_metric_final})'] = final_ref_ssmd\n                        # Num valid sequences could be added here if needed\n\n                        logger.info(\"--- Final Test Set Results (SSMD) ---\")\n                        logger.info(f\"  Test Avg Gen SSMD(k={ssmd_k_final}, FuncCode): {final_gen_ssmd:.4f}\")\n                        logger.info(f\"  Test Avg Ref SSMD(k={ssmd_k_final}, FuncCode): {final_ref_ssmd:.4f}\")\n                    else:\n                        logger.warning(\"Skipping final test set evaluation: Test loader (master) could not be created.\")\n                else:\n                     logger.warning(\"Skipping final test set evaluation: Test dataset is empty or was not created.\")\n\n            except Exception as e:\n                logger.error(f\"Final evaluation failed: {e}\", exc_info=True)\n                final_results['Error'] = f\"Evaluation failed: {e}\"\n\n        elif not best_model_path:\n            logger.error(\"Skipping final evaluation: No best model was saved or loaded.\")\n        else:\n            logger.error(f\"Skipping final evaluation: Best model checkpoint file not found at {best_model_path}\")\n\n        # Save Final Results Summary (Master Only)\n        results_summary_path = RESULTS_DIR / \"final_evaluation_summary.json\"\n        logger.info(f\"--- Saving Final Results Summary ---\")\n        try:\n            # Ensure history lists have the same length (pad with None if needed)\n            max_hist_len = 0\n            if training_history:\n                for key in training_history.keys():\n                    if training_history[key]: max_hist_len = max(max_hist_len, len(training_history[key]))\n            if max_hist_len > 0:\n                for key in training_history.keys():\n                    current_len = len(training_history[key])\n                    if current_len < max_hist_len: training_history[key].extend([None] * (max_hist_len - current_len))\n                    elif current_len > max_hist_len: training_history[key] = training_history[key][:max_hist_len]\n\n            serializable_hparams = {}\n            for k, v in HPARAMS.items():\n                if isinstance(v, (int, float, str, bool, list, dict, type(None))): serializable_hparams[k] = v\n                elif isinstance(v, Path): serializable_hparams[k] = str(v)\n                else: logger.warning(f\"HPARAM '{k}' type {type(v)} not JSON serializable. Skipping.\")\n\n            metric_name_saved = 'N/A'\n            if 'checkpoint_state' in locals() and isinstance(checkpoint_state, dict):\n                 metric_name_saved = checkpoint_state.get('metric_name', 'N/A')\n\n            summary_data = {\n                'Timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n                'Best_Model_Path': str(best_model_path) if best_model_path else None,\n                'Best_Val_Metric': {'Name': metric_name_saved, 'Value': best_val_loss if math.isfinite(best_val_loss) else None},\n                'Hyperparameters': serializable_hparams,\n                'Final_Test_Metrics': final_results,\n                'Training_History': training_history # Includes per-epoch Acc and SSMD\n            }\n            with open(results_summary_path, 'w') as f:\n                # Use default=str to handle potential non-serializable numpy types if any slip through\n                json.dump(summary_data, f, indent=4, ensure_ascii=False, default=str)\n            logger.info(f\"Final evaluation summary saved to: {results_summary_path}\")\n        except Exception as e:\n            logger.error(f\"Error saving final results summary: {e}\", exc_info=True)\n\n        global_script_start_time = script_start_time if 'script_start_time' in globals() else start_train_time\n        total_script_time = time.time() - global_script_start_time # Approximate total time\n        logger.info(f\"=== SCRIPT EXECUTION FINISHED (Master Rank {rank}, Total Duration: {total_script_time // 3600:.0f}h {(total_script_time % 3600) // 60:.0f}m {total_script_time % 60:.2f}s) ===\")\n\n    xm.rendezvous(f'Rank {rank} finished execution')\n\n\n# ==============================================================\n# === MAIN EXECUTION BLOCK (using xmp.spawn) ===\n# ==============================================================\nif __name__ == \"__main__\":\n    logging.info(\"=\"*70); logging.info(\"=== STARTING Advanced H-RPE TPU Script (Epoch Metrics) ===\"); logging.info(\"=\"*70)\n    script_start_time = time.time() # Log start time before spawn\n\n    flags = {}\n    try:\n        # Using nprocs=1 as per original script, though nprocs=None is often better for TPU auto-detection\n        # If you have multiple TPU cores available and want to use them, change nprocs=None\n        xmp.spawn(_mp_fn, args=(flags,), nprocs=1, start_method='fork') # Kept nprocs=1\n    except Exception as main_exc:\n        logging.critical(f\"Exception during xmp.spawn: {main_exc}\", exc_info=True)\n        # Explicitly print traceback if logging fails or isn't fully configured yet\n        traceback.print_exc()\n\n\n    logging.info(\"=\"*70); logging.info(\"=== XMP Spawn Finished ===\"); logging.info(\"=\"*70)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}