{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5c3200",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-25T11:18:52.219882Z",
     "iopub.status.busy": "2025-04-25T11:18:52.219635Z",
     "iopub.status.idle": "2025-04-25T12:08:53.156466Z",
     "shell.execute_reply": "2025-04-25T12:08:53.155533Z"
    },
    "papermill": {
     "duration": 3000.949466,
     "end_time": "2025-04-25T12:08:53.157982",
     "exception": false,
     "start_time": "2025-04-25T11:18:52.208516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script Execution Start Time: 2025-04-25 11:18:56 UTC\n",
      "\n",
      "--- Configuration ---\n",
      "  amp_dtype: torch.float16\n",
      "  batch_size: 16\n",
      "  chord_data_dir: /kaggle/input/advance-h-rpe\n",
      "  chord_emb_dim: 64\n",
      "  chord_pad_token_id: 0\n",
      "  chord_vocab_size: 0\n",
      "  condition_proj_dim: 128\n",
      "  d_head: 64\n",
      "  d_inner: 2048\n",
      "  d_model: 512\n",
      "  dropout: 0.1\n",
      "  grad_clip_value: 1.0\n",
      "  learning_rate: 0.0003\n",
      "  max_seq_len: 512\n",
      "  melody_data_path: /kaggle/input/new-melody-model-new-approach-1/training_data.jsonl\n",
      "  melody_pad_token_id: 0\n",
      "  melody_vocab_path: /kaggle/input/new-melody-model-new-approach-1/event_vocab.json\n",
      "  melody_vocab_size: 0\n",
      "  mem_len: 256\n",
      "  midi_root_dir: LOCAL_PATH_IGNORE\n",
      "  n_head: 8\n",
      "  n_layer: 8\n",
      "  num_chord_features: 3\n",
      "  num_dataload_workers: 2\n",
      "  num_epochs: 100\n",
      "  output_dir: /kaggle/working/melody_model_output\n",
      "  rope_theta: 10000.0\n",
      "  seed: 42\n",
      "  test_split: 0.05\n",
      "  train_split: 0.9\n",
      "  val_split: 0.05\n",
      "  weight_decay: 0.01\n",
      "------------------------------\n",
      "\n",
      "Using device: cuda (Tesla P100-PCIE-16GB)\n",
      "CUDA Compute Capability: (6, 0)\n",
      "PyTorch Version: 2.5.1+cu124\n",
      "CUDA available: True, version: 12.4\n",
      "cuDNN enabled: True, version: 90300\n",
      "\n",
      "--- Loading Vocabularies ---\n",
      "\n",
      "--- Loading & Processing Dataset ---\n",
      "Loading dataset from: /kaggle/input/new-melody-model-new-approach-1/training_data.jsonl...\n",
      "Dataset loaded successfully: 1544 samples (skipped 0 invalid lines).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning IDs: 100%|██████████| 1544/1544 [00:00<00:00, 13112.14samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Configuration ---\n",
      "  Adjusted Melody Vocab Size: 306\n",
      "  Adjusted Chord Vocab Size: 44734\n",
      "  Melody Pad ID: 0\n",
      "  Chord Pad ID: 0\n",
      "  Max Sequence Length: 512\n",
      "  Batch Size: 16\n",
      "------------------------------\n",
      "\n",
      "--- Creating DataLoaders ---\n",
      "\n",
      "--- Initializing Training Components ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_19/960824275.py:1376: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=amp_enabled) # Uses default cuda device if available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training from Epoch 1 / 100 ---\n",
      "\n",
      "===== Epoch 1/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Summary | Time: 28.84s | Loss: 4.5163 | Acc: 0.1653 | Tokens: 660634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 4.4851 | Accuracy: 0.1567 | SSMD: 0.9879 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (inf -> 4.4851). Saving best model... **\n",
      "\n",
      "===== Epoch 2/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Summary | Time: 27.78s | Loss: 4.3825 | Acc: 0.1671 | Tokens: 659747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 4.2148 | Accuracy: 0.1630 | SSMD: 0.9722 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (4.4851 -> 4.2148). Saving best model... **\n",
      "\n",
      "===== Epoch 3/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Summary | Time: 27.78s | Loss: 3.9663 | Acc: 0.1800 | Tokens: 660330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.7437 | Accuracy: 0.1684 | SSMD: 0.8932 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (4.2148 -> 3.7437). Saving best model... **\n",
      "\n",
      "===== Epoch 4/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Summary | Time: 27.77s | Loss: 3.5225 | Acc: 0.1934 | Tokens: 660293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.3713 | Accuracy: 0.1874 | SSMD: 0.7452 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (3.7437 -> 3.3713). Saving best model... **\n",
      "\n",
      "===== Epoch 5/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Summary | Time: 27.78s | Loss: 3.2067 | Acc: 0.2275 | Tokens: 660023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.1368 | Accuracy: 0.2334 | SSMD: 0.7007 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (3.3713 -> 3.1368). Saving best model... **\n",
      "\n",
      "===== Epoch 6/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Summary | Time: 27.77s | Loss: 3.0198 | Acc: 0.2724 | Tokens: 660427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.9492 | Accuracy: 0.2855 | SSMD: 0.5944 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (3.1368 -> 2.9492). Saving best model... **\n",
      "\n",
      "===== Epoch 7/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train Summary | Time: 27.78s | Loss: 2.8349 | Acc: 0.3327 | Tokens: 660955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.8002 | Accuracy: 0.3356 | SSMD: 0.5345 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.9492 -> 2.8002). Saving best model... **\n",
      "\n",
      "===== Epoch 8/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train Summary | Time: 27.80s | Loss: 2.6885 | Acc: 0.3695 | Tokens: 660801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.6434 | Accuracy: 0.3806 | SSMD: 0.4763 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.8002 -> 2.6434). Saving best model... **\n",
      "\n",
      "===== Epoch 9/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train Summary | Time: 27.80s | Loss: 2.5682 | Acc: 0.3996 | Tokens: 660441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.5412 | Accuracy: 0.4016 | SSMD: 0.4540 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.6434 -> 2.5412). Saving best model... **\n",
      "\n",
      "===== Epoch 10/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train Summary | Time: 27.80s | Loss: 2.4820 | Acc: 0.4164 | Tokens: 659941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.4796 | Accuracy: 0.4123 | SSMD: 0.4355 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.5412 -> 2.4796). Saving best model... **\n",
      "\n",
      "===== Epoch 11/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train Summary | Time: 27.75s | Loss: 2.4177 | Acc: 0.4278 | Tokens: 660339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.4256 | Accuracy: 0.4225 | SSMD: 0.4319 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.4796 -> 2.4256). Saving best model... **\n",
      "\n",
      "===== Epoch 12/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train Summary | Time: 27.72s | Loss: 2.3587 | Acc: 0.4379 | Tokens: 660173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.3804 | Accuracy: 0.4315 | SSMD: 0.4219 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.4256 -> 2.3804). Saving best model... **\n",
      "\n",
      "===== Epoch 13/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Train Summary | Time: 27.78s | Loss: 2.3203 | Acc: 0.4440 | Tokens: 659841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.3377 | Accuracy: 0.4384 | SSMD: 0.4176 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.3804 -> 2.3377). Saving best model... **\n",
      "\n",
      "===== Epoch 14/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Train Summary | Time: 27.74s | Loss: 2.2809 | Acc: 0.4499 | Tokens: 660147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.3239 | Accuracy: 0.4404 | SSMD: 0.4143 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.3377 -> 2.3239). Saving best model... **\n",
      "\n",
      "===== Epoch 15/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Train Summary | Time: 27.72s | Loss: 2.2523 | Acc: 0.4538 | Tokens: 660071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2974 | Accuracy: 0.4460 | SSMD: 0.4122 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.3239 -> 2.2974). Saving best model... **\n",
      "\n",
      "===== Epoch 16/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Train Summary | Time: 27.73s | Loss: 2.2302 | Acc: 0.4565 | Tokens: 659896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2750 | Accuracy: 0.4460 | SSMD: 0.4035 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.2974 -> 2.2750). Saving best model... **\n",
      "\n",
      "===== Epoch 17/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Train Summary | Time: 27.72s | Loss: 2.2075 | Acc: 0.4598 | Tokens: 659998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2725 | Accuracy: 0.4462 | SSMD: 0.4050 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.2750 -> 2.2725). Saving best model... **\n",
      "\n",
      "===== Epoch 18/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Train Summary | Time: 27.74s | Loss: 2.1888 | Acc: 0.4632 | Tokens: 660712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2647 | Accuracy: 0.4484 | SSMD: 0.4129 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.2725 -> 2.2647). Saving best model... **\n",
      "\n",
      "===== Epoch 19/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Train Summary | Time: 27.79s | Loss: 2.1682 | Acc: 0.4658 | Tokens: 660272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2515 | Accuracy: 0.4498 | SSMD: 0.4076 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.2647 -> 2.2515). Saving best model... **\n",
      "\n",
      "===== Epoch 20/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Train Summary | Time: 27.79s | Loss: 2.1504 | Acc: 0.4679 | Tokens: 660262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2391 | Accuracy: 0.4517 | SSMD: 0.3926 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.2515 -> 2.2391). Saving best model... **\n",
      "\n",
      "===== Epoch 21/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Train Summary | Time: 27.78s | Loss: 2.1361 | Acc: 0.4705 | Tokens: 659797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2236 | Accuracy: 0.4502 | SSMD: 0.3859 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.2391 -> 2.2236). Saving best model... **\n",
      "\n",
      "===== Epoch 22/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Train Summary | Time: 27.81s | Loss: 2.1203 | Acc: 0.4725 | Tokens: 659947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2278 | Accuracy: 0.4511 | SSMD: 0.3814 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 23/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Train Summary | Time: 27.79s | Loss: 2.1094 | Acc: 0.4738 | Tokens: 660125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2206 | Accuracy: 0.4543 | SSMD: 0.3851 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.2236 -> 2.2206). Saving best model... **\n",
      "\n",
      "===== Epoch 24/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Train Summary | Time: 27.79s | Loss: 2.0948 | Acc: 0.4763 | Tokens: 659959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2178 | Accuracy: 0.4541 | SSMD: 0.3799 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.2206 -> 2.2178). Saving best model... **\n",
      "\n",
      "===== Epoch 25/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Train Summary | Time: 27.81s | Loss: 2.0814 | Acc: 0.4783 | Tokens: 660183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2302 | Accuracy: 0.4544 | SSMD: 0.3740 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 26/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Train Summary | Time: 27.80s | Loss: 2.0692 | Acc: 0.4805 | Tokens: 660218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2177 | Accuracy: 0.4547 | SSMD: 0.3820 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "** Validation Loss Improved (2.2178 -> 2.2177). Saving best model... **\n",
      "\n",
      "===== Epoch 27/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Train Summary | Time: 27.78s | Loss: 2.0526 | Acc: 0.4828 | Tokens: 659747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2217 | Accuracy: 0.4555 | SSMD: 0.3676 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 28/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Train Summary | Time: 27.81s | Loss: 2.0442 | Acc: 0.4838 | Tokens: 660388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2320 | Accuracy: 0.4569 | SSMD: 0.3588 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 29/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Train Summary | Time: 27.80s | Loss: 2.0311 | Acc: 0.4858 | Tokens: 659979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2233 | Accuracy: 0.4570 | SSMD: 0.3670 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 30/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Train Summary | Time: 27.80s | Loss: 2.0165 | Acc: 0.4885 | Tokens: 660433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2177 | Accuracy: 0.4547 | SSMD: 0.3749 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 31/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Train Summary | Time: 27.80s | Loss: 2.0039 | Acc: 0.4904 | Tokens: 660371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2353 | Accuracy: 0.4553 | SSMD: 0.3607 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 32/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Train Summary | Time: 27.79s | Loss: 1.9917 | Acc: 0.4920 | Tokens: 660796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2352 | Accuracy: 0.4560 | SSMD: 0.3546 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 33/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Train Summary | Time: 27.81s | Loss: 1.9797 | Acc: 0.4944 | Tokens: 660472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2323 | Accuracy: 0.4529 | SSMD: 0.3538 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 34/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Train Summary | Time: 27.79s | Loss: 1.9637 | Acc: 0.4971 | Tokens: 659747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2510 | Accuracy: 0.4547 | SSMD: 0.3552 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 35/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Train Summary | Time: 27.80s | Loss: 1.9513 | Acc: 0.4988 | Tokens: 660483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2474 | Accuracy: 0.4519 | SSMD: 0.3449 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 36/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Train Summary | Time: 27.82s | Loss: 1.9376 | Acc: 0.5013 | Tokens: 659873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2601 | Accuracy: 0.4505 | SSMD: 0.3278 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 37/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Train Summary | Time: 27.82s | Loss: 1.9205 | Acc: 0.5034 | Tokens: 660979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2526 | Accuracy: 0.4515 | SSMD: 0.3305 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 38/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Train Summary | Time: 27.79s | Loss: 1.9044 | Acc: 0.5072 | Tokens: 660493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2763 | Accuracy: 0.4535 | SSMD: 0.3250 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 39/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Train Summary | Time: 27.79s | Loss: 1.8897 | Acc: 0.5089 | Tokens: 660198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2775 | Accuracy: 0.4529 | SSMD: 0.3265 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 40/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Train Summary | Time: 27.82s | Loss: 1.8715 | Acc: 0.5126 | Tokens: 660712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.2845 | Accuracy: 0.4508 | SSMD: 0.3206 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 41/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Train Summary | Time: 27.82s | Loss: 1.8566 | Acc: 0.5149 | Tokens: 659895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.3105 | Accuracy: 0.4508 | SSMD: 0.3198 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 42/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Train Summary | Time: 27.81s | Loss: 1.8404 | Acc: 0.5178 | Tokens: 660227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.3269 | Accuracy: 0.4485 | SSMD: 0.3155 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 43/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Train Summary | Time: 27.82s | Loss: 1.8192 | Acc: 0.5218 | Tokens: 660195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.3386 | Accuracy: 0.4456 | SSMD: 0.3039 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 44/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Train Summary | Time: 27.78s | Loss: 1.8032 | Acc: 0.5244 | Tokens: 659928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.3425 | Accuracy: 0.4479 | SSMD: 0.3075 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 45/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Train Summary | Time: 27.80s | Loss: 1.7850 | Acc: 0.5278 | Tokens: 660075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.3559 | Accuracy: 0.4463 | SSMD: 0.2977 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 46/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Train Summary | Time: 27.80s | Loss: 1.7638 | Acc: 0.5312 | Tokens: 659970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.3715 | Accuracy: 0.4425 | SSMD: 0.2841 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 47/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Train Summary | Time: 27.78s | Loss: 1.7452 | Acc: 0.5352 | Tokens: 660303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.3715 | Accuracy: 0.4460 | SSMD: 0.2947 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 48/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Train Summary | Time: 27.79s | Loss: 1.7225 | Acc: 0.5395 | Tokens: 660089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.4169 | Accuracy: 0.4408 | SSMD: 0.2845 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 49/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Train Summary | Time: 27.83s | Loss: 1.7010 | Acc: 0.5434 | Tokens: 660605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.4286 | Accuracy: 0.4435 | SSMD: 0.2858 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 50/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Train Summary | Time: 27.81s | Loss: 1.6789 | Acc: 0.5478 | Tokens: 659970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.4404 | Accuracy: 0.4418 | SSMD: 0.2668 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 51/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Train Summary | Time: 27.81s | Loss: 1.6582 | Acc: 0.5514 | Tokens: 660168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.4544 | Accuracy: 0.4396 | SSMD: 0.2765 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 52/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Train Summary | Time: 27.80s | Loss: 1.6346 | Acc: 0.5561 | Tokens: 660057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.4910 | Accuracy: 0.4382 | SSMD: 0.2771 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 53/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Train Summary | Time: 27.80s | Loss: 1.6068 | Acc: 0.5619 | Tokens: 659824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.5208 | Accuracy: 0.4368 | SSMD: 0.2678 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 54/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Train Summary | Time: 27.80s | Loss: 1.5823 | Acc: 0.5672 | Tokens: 660937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.5325 | Accuracy: 0.4421 | SSMD: 0.2515 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 55/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Train Summary | Time: 27.80s | Loss: 1.5622 | Acc: 0.5712 | Tokens: 659925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.5755 | Accuracy: 0.4361 | SSMD: 0.2551 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 56/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Train Summary | Time: 27.79s | Loss: 1.5380 | Acc: 0.5756 | Tokens: 659747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.5880 | Accuracy: 0.4365 | SSMD: 0.2520 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 57/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Train Summary | Time: 27.80s | Loss: 1.5102 | Acc: 0.5817 | Tokens: 660069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.6466 | Accuracy: 0.4378 | SSMD: 0.2488 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 58/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Train Summary | Time: 27.83s | Loss: 1.4828 | Acc: 0.5871 | Tokens: 660795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.6474 | Accuracy: 0.4357 | SSMD: 0.2441 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 59/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Train Summary | Time: 27.82s | Loss: 1.4593 | Acc: 0.5927 | Tokens: 660284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.6629 | Accuracy: 0.4330 | SSMD: 0.2371 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 60/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Train Summary | Time: 27.82s | Loss: 1.4300 | Acc: 0.5993 | Tokens: 660401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.7209 | Accuracy: 0.4334 | SSMD: 0.2356 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 61/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Train Summary | Time: 27.83s | Loss: 1.4044 | Acc: 0.6050 | Tokens: 659957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.7703 | Accuracy: 0.4347 | SSMD: 0.2338 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 62/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Train Summary | Time: 27.81s | Loss: 1.3766 | Acc: 0.6110 | Tokens: 659747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.7583 | Accuracy: 0.4296 | SSMD: 0.2317 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 63/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Train Summary | Time: 27.80s | Loss: 1.3465 | Acc: 0.6172 | Tokens: 660269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.8220 | Accuracy: 0.4297 | SSMD: 0.2292 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 64/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Train Summary | Time: 27.82s | Loss: 1.3229 | Acc: 0.6230 | Tokens: 660627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.8691 | Accuracy: 0.4303 | SSMD: 0.2317 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 65/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Train Summary | Time: 27.80s | Loss: 1.2957 | Acc: 0.6295 | Tokens: 660101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.8734 | Accuracy: 0.4293 | SSMD: 0.2118 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 66/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Train Summary | Time: 27.78s | Loss: 1.2665 | Acc: 0.6361 | Tokens: 660231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.9259 | Accuracy: 0.4241 | SSMD: 0.2172 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 67/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Train Summary | Time: 27.79s | Loss: 1.2396 | Acc: 0.6426 | Tokens: 659747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.9465 | Accuracy: 0.4242 | SSMD: 0.2099 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 68/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Train Summary | Time: 27.80s | Loss: 1.2080 | Acc: 0.6499 | Tokens: 660654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.9982 | Accuracy: 0.4247 | SSMD: 0.2062 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 69/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 Train Summary | Time: 27.80s | Loss: 1.1811 | Acc: 0.6572 | Tokens: 660168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.0567 | Accuracy: 0.4261 | SSMD: 0.2036 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 70/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 Train Summary | Time: 27.81s | Loss: 1.1549 | Acc: 0.6633 | Tokens: 659989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.0629 | Accuracy: 0.4228 | SSMD: 0.2117 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 71/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Train Summary | Time: 27.81s | Loss: 1.1279 | Acc: 0.6693 | Tokens: 661122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.1179 | Accuracy: 0.4249 | SSMD: 0.2080 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 72/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Train Summary | Time: 27.81s | Loss: 1.1010 | Acc: 0.6766 | Tokens: 660485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.1399 | Accuracy: 0.4216 | SSMD: 0.2092 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 73/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 Train Summary | Time: 27.80s | Loss: 1.0707 | Acc: 0.6844 | Tokens: 660424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.2053 | Accuracy: 0.4248 | SSMD: 0.1989 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 74/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Train Summary | Time: 27.82s | Loss: 1.0491 | Acc: 0.6892 | Tokens: 660462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.2609 | Accuracy: 0.4246 | SSMD: 0.2021 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 75/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Train Summary | Time: 27.81s | Loss: 1.0257 | Acc: 0.6954 | Tokens: 659879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.2838 | Accuracy: 0.4209 | SSMD: 0.1943 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 76/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Train Summary | Time: 27.83s | Loss: 1.0031 | Acc: 0.7010 | Tokens: 659970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.2888 | Accuracy: 0.4204 | SSMD: 0.1961 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 77/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 Train Summary | Time: 27.81s | Loss: 0.9786 | Acc: 0.7076 | Tokens: 660965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.3429 | Accuracy: 0.4180 | SSMD: 0.1880 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 78/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Train Summary | Time: 27.86s | Loss: 0.9528 | Acc: 0.7148 | Tokens: 660549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.3670 | Accuracy: 0.4228 | SSMD: 0.1991 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 79/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Train Summary | Time: 27.81s | Loss: 0.9318 | Acc: 0.7200 | Tokens: 660764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.4157 | Accuracy: 0.4224 | SSMD: 0.1959 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 80/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Train Summary | Time: 27.80s | Loss: 0.9064 | Acc: 0.7267 | Tokens: 660494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.4745 | Accuracy: 0.4208 | SSMD: 0.1864 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 81/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 Train Summary | Time: 27.79s | Loss: 0.8845 | Acc: 0.7327 | Tokens: 660514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.5207 | Accuracy: 0.4211 | SSMD: 0.1853 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 82/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 Train Summary | Time: 27.81s | Loss: 0.8652 | Acc: 0.7375 | Tokens: 660055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.5580 | Accuracy: 0.4197 | SSMD: 0.1862 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 83/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 Train Summary | Time: 27.83s | Loss: 0.8429 | Acc: 0.7433 | Tokens: 659972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.5833 | Accuracy: 0.4230 | SSMD: 0.1895 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 84/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 Train Summary | Time: 27.81s | Loss: 0.8245 | Acc: 0.7488 | Tokens: 660164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.6372 | Accuracy: 0.4183 | SSMD: 0.1907 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 85/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 Train Summary | Time: 27.82s | Loss: 0.8057 | Acc: 0.7547 | Tokens: 660131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.6526 | Accuracy: 0.4200 | SSMD: 0.1834 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 86/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Train Summary | Time: 27.82s | Loss: 0.7868 | Acc: 0.7594 | Tokens: 659959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.6887 | Accuracy: 0.4180 | SSMD: 0.1890 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 87/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 Train Summary | Time: 27.82s | Loss: 0.7641 | Acc: 0.7657 | Tokens: 660662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.6992 | Accuracy: 0.4186 | SSMD: 0.1872 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 88/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 Train Summary | Time: 27.81s | Loss: 0.7496 | Acc: 0.7694 | Tokens: 659895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.7802 | Accuracy: 0.4214 | SSMD: 0.1933 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 89/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 Train Summary | Time: 27.81s | Loss: 0.7322 | Acc: 0.7739 | Tokens: 659913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.8153 | Accuracy: 0.4200 | SSMD: 0.1794 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 90/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 Train Summary | Time: 27.80s | Loss: 0.7164 | Acc: 0.7788 | Tokens: 660211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.8477 | Accuracy: 0.4176 | SSMD: 0.1807 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 91/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 Train Summary | Time: 27.79s | Loss: 0.7008 | Acc: 0.7831 | Tokens: 660266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.8861 | Accuracy: 0.4173 | SSMD: 0.1785 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 92/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 Train Summary | Time: 27.81s | Loss: 0.6818 | Acc: 0.7885 | Tokens: 659953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.9264 | Accuracy: 0.4228 | SSMD: 0.1859 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 93/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 Train Summary | Time: 27.79s | Loss: 0.6702 | Acc: 0.7919 | Tokens: 660291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.9356 | Accuracy: 0.4188 | SSMD: 0.1821 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 94/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 Train Summary | Time: 27.81s | Loss: 0.6530 | Acc: 0.7963 | Tokens: 660041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.9844 | Accuracy: 0.4227 | SSMD: 0.1828 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 95/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 Train Summary | Time: 27.82s | Loss: 0.6403 | Acc: 0.8005 | Tokens: 659747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 3.9887 | Accuracy: 0.4185 | SSMD: 0.1800 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 96/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 Train Summary | Time: 27.81s | Loss: 0.6280 | Acc: 0.8038 | Tokens: 660225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 4.0075 | Accuracy: 0.4174 | SSMD: 0.1809 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 97/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 Train Summary | Time: 27.86s | Loss: 0.6122 | Acc: 0.8077 | Tokens: 659747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 4.0663 | Accuracy: 0.4188 | SSMD: 0.1918 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 98/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 Train Summary | Time: 27.82s | Loss: 0.6026 | Acc: 0.8112 | Tokens: 660379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 4.0632 | Accuracy: 0.4190 | SSMD: 0.1858 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 99/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Train Summary | Time: 27.80s | Loss: 0.5875 | Acc: 0.8158 | Tokens: 660537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 4.1431 | Accuracy: 0.4198 | SSMD: 0.1772 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "===== Epoch 100/100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 Train Summary | Time: 27.82s | Loss: 0.5754 | Acc: 0.8192 | Tokens: 659747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 4.1587 | Accuracy: 0.4185 | SSMD: 0.1822 | GS: 0.9328 | Eval Seq Count: 77 | Eval Tokens: 36702\n",
      "\n",
      "--- Training Finished ---\n",
      "\n",
      "--- Final Evaluation on Test Set ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval Summary | Loss: 2.1751 | Accuracy: 0.4686 | SSMD: 0.3782 | GS: 0.9231 | Eval Seq Count: 77 | Eval Tokens: 36320\n",
      "\n",
      "--- Test Set Results (using BEST saved model) ---\n",
      " Loss:     2.1751\n",
      " Accuracy: 0.4686\n",
      " SSMD:     0.3782\n",
      " GS:       0.9231\n",
      " Eval Seqs:77\n",
      " Eval Toks:36320\n",
      "Output files potentially saved in: /kaggle/working/melody_model_output\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPLETE Training Script: Melody Transformer-XL with RoPE & Conditioning\n",
    "# Version: Corrected autocast API, Removed explicit model fp16 conversion,\n",
    "#          Added max_seq_len, Reduced batch_size\n",
    "# =============================================================================\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sys\n",
    "import traceback\n",
    "import gc # Garbage Collector\n",
    "from typing import Optional, List, Dict, Tuple, Any, Set\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm # Use standard tqdm\n",
    "import collections # Needed for defaultdict in collate_fn\n",
    "import logging\n",
    "\n",
    "# --- CUDA specific imports ---\n",
    "# Use torch.amp directly for autocast, keep GradScaler from cuda.amp\n",
    "from torch.cuda.amp import GradScaler\n",
    "import torch.amp # Use torch.amp for autocast\n",
    "\n",
    "# For timestamp and location context\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "# --- Define Vocabulary Constants (!! VERIFY THESE STRINGS !!) ---\n",
    "MELODY_PAD_TOKEN: str = \"<PAD>\"\n",
    "MELODY_UNK_TOKEN: str = \"<UNK>\"\n",
    "CHORD_VOCAB_FILENAME: str = \"chord_progression_vocab.json\"\n",
    "# CHORD_PAD_TOKEN: str = \"<PAD>\" # Example\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # --- Paths ---\n",
    "    midi_root_dir: str = \"LOCAL_PATH_IGNORE\"\n",
    "    chord_data_dir: str = \"/kaggle/input/advance-h-rpe\"\n",
    "    melody_data_path: str = \"/kaggle/input/new-melody-model-new-approach-1/training_data.jsonl\"\n",
    "    melody_vocab_path: str = \"/kaggle/input/new-melody-model-new-approach-1/event_vocab.json\"\n",
    "    output_dir: str = \"/kaggle/working/melody_model_output\"\n",
    "\n",
    "    # --- Vocab Sizes & Padding ---\n",
    "    melody_vocab_size: int = 0\n",
    "    chord_vocab_size: int = 0\n",
    "    melody_pad_token_id: int = 0\n",
    "    chord_pad_token_id: int = 0\n",
    "\n",
    "    # --- Model Architecture ---\n",
    "    n_layer: int = 8\n",
    "    d_model: int = 512\n",
    "    n_head: int = 8\n",
    "    d_head: int = 64\n",
    "    d_inner: int = 2048\n",
    "    dropout: float = 0.1\n",
    "    mem_len: int = 256 # Consider reducing if OOM persists (e.g., 128)\n",
    "    rope_theta: float = 10000.0\n",
    "    num_chord_features: int = 3\n",
    "    condition_proj_dim: int = 128\n",
    "    chord_emb_dim: Optional[int] = 64\n",
    "\n",
    "    # --- Training ---\n",
    "    batch_size: int = 16 # <<< REDUCED BATCH SIZE\n",
    "    num_epochs: int = 100\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 0.01\n",
    "    grad_clip_value: float = 1.0\n",
    "    seed: int = 42\n",
    "\n",
    "    # --- Data Loading ---\n",
    "    max_seq_len: Optional[int] = 512 # <<< REDUCED MAX SEQUENCE LENGTH FURTHER\n",
    "    train_split: float = 0.90\n",
    "    val_split: float = 0.05\n",
    "    test_split: float = 0.05\n",
    "    num_dataload_workers: int = 2\n",
    "\n",
    "    # --- Runtime ---\n",
    "    amp_dtype: torch.dtype = torch.float16 # Use float16 for Automatic Mixed Precision\n",
    "\n",
    "\n",
    "# === HELPER FUNCTIONS ===\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Sets random seeds for reproducibility across libraries.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    logging.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "# --- Metrics ---\n",
    "def calculate_accuracy(logits: torch.Tensor, targets: torch.Tensor, pad_token_id: int) -> Tuple[float, int]:\n",
    "    \"\"\"Calculates accuracy ignoring pad tokens.\"\"\"\n",
    "    mask = (targets != pad_token_id)\n",
    "    num_valid_tokens = mask.sum().item()\n",
    "    if num_valid_tokens == 0:\n",
    "        return 0.0, 0\n",
    "    predictions = logits.argmax(dim=-1)\n",
    "    correct_predictions = (predictions[mask] == targets[mask]).sum().item()\n",
    "    accuracy = correct_predictions / num_valid_tokens\n",
    "    return accuracy, num_valid_tokens\n",
    "\n",
    "def self_similarity_matrix_distance(pred_seq: List[int], target_seq: List[int], pad_id: int) -> float:\n",
    "    \"\"\"Calculates 1 - (ratio of common unique non-pad tokens to unique non-pad target tokens).\"\"\"\n",
    "    pred_clean = [tok for tok in pred_seq if tok != pad_id]\n",
    "    target_clean = [tok for tok in target_seq if tok != pad_id]\n",
    "    if not target_clean: return 1.0 if pred_clean else 0.0\n",
    "    pred_set = set(pred_clean)\n",
    "    target_set = set(target_clean)\n",
    "    if not target_set: return 1.0 if pred_set else 0.0\n",
    "    common_elements = len(pred_set.intersection(target_set))\n",
    "    unique_target_elements = len(target_set)\n",
    "    similarity_ratio = common_elements / unique_target_elements if unique_target_elements > 0 else 0.0\n",
    "    return 1.0 - similarity_ratio\n",
    "\n",
    "def grooving_similarity(pred_seq: List[int], target_seq: List[int], pad_id: int) -> float:\n",
    "    \"\"\"Calculates ratio of min length to max length after removing padding.\"\"\"\n",
    "    pred_clean = [tok for tok in pred_seq if tok != pad_id]\n",
    "    target_clean = [tok for tok in target_seq if tok != pad_id]\n",
    "    len_p, len_t = len(pred_clean), len(target_clean)\n",
    "    max_len = max(len_p, len_t)\n",
    "    min_len = min(len_p, len_t)\n",
    "    return min_len / max_len if max_len > 0 else 1.0\n",
    "\n",
    "# === RoPE Implementation ===\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    # x: [seq_len, bsz, n_head, d_head]\n",
    "    # cos, sin: [seq_len, 1, 1, d_head]\n",
    "    x_embed = (x * cos) + (rotate_half(x) * sin)\n",
    "    return x_embed\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=4096, base=10000.0, device=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=device, dtype=torch.float32) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.max_seq_len_cached = -1\n",
    "        self.register_buffer(\"cos_cached\", None, persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", None, persistent=False)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        if seq_len <= self.max_seq_len_cached and \\\n",
    "           self.cos_cached is not None and self.sin_cached is not None and \\\n",
    "           self.cos_cached.device == device and self.cos_cached.dtype == dtype:\n",
    "             return\n",
    "        self.max_seq_len_cached = max(seq_len, self.max_position_embeddings) # Ensure cache is at least max_pos_embeddings\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.cos_cached = emb.cos().to(dtype).detach()\n",
    "        self.sin_cached = emb.sin().to(dtype).detach()\n",
    "        logging.debug(f\"RoPE cache updated: seq_len={self.max_seq_len_cached}, device={device}, dtype={dtype}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int, start_pos: int = 0):\n",
    "        device = x.device\n",
    "        dtype = x.dtype\n",
    "        required_len = start_pos + seq_len\n",
    "        if required_len > self.max_seq_len_cached or self.cos_cached is None or self.cos_cached.device != device or self.cos_cached.dtype != dtype:\n",
    "            new_max_len = max(self.max_position_embeddings, required_len)\n",
    "            self._set_cos_sin_cache(seq_len=new_max_len, device=device, dtype=dtype)\n",
    "        # Slice directly using start_pos and seq_len\n",
    "        end_pos = start_pos + seq_len\n",
    "        cos = self.cos_cached[start_pos : end_pos]\n",
    "        sin = self.sin_cached[start_pos : end_pos]\n",
    "        return cos, sin\n",
    "\n",
    "# === Model Components ===\n",
    "class RelPartialLearnableMultiHeadAttn(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_head, dropout, config: TrainingConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_head\n",
    "        self.dropout = dropout\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head, bias=False)\n",
    "        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(\n",
    "            dim=self.d_head,\n",
    "            max_position_embeddings=config.mem_len + (config.max_seq_len if config.max_seq_len is not None else 2048),\n",
    "            base=config.rope_theta\n",
    "        )\n",
    "        self.scale = 1.0 / (d_head ** 0.5)\n",
    "\n",
    "    def forward(self, w: torch.Tensor, mems: Optional[torch.Tensor], attn_mask: Optional[torch.Tensor] = None, head_mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        qlen, bsz, _ = w.size()\n",
    "        mlen = mems.size(0) if mems is not None and mems.dim() == 3 and mems.shape[0] > 0 else 0\n",
    "        klen = mlen + qlen\n",
    "\n",
    "        if mlen > 0:\n",
    "            if mems.shape[1:] != w.shape[1:]:\n",
    "                logging.warning(f\"Layer {self.layer_idx}: Mem shape {mems.shape} != Input shape {w.shape}. Discarding memory.\")\n",
    "                cat = w; mlen = 0; klen = qlen\n",
    "            else:\n",
    "                cat = torch.cat([mems, w], dim=0)\n",
    "        else:\n",
    "            cat = w\n",
    "\n",
    "        w_heads = self.qkv_net(cat)\n",
    "        w_heads = w_heads.view(klen, bsz, self.n_head, 3 * self.d_head)\n",
    "        q_head_raw, k_head_raw, v_head = torch.chunk(w_heads, 3, dim=-1)\n",
    "\n",
    "        q_head = q_head_raw[-qlen:]\n",
    "        k_head = k_head_raw\n",
    "\n",
    "        cos_k, sin_k = self.rotary_emb(k_head, seq_len=klen, start_pos=0)\n",
    "        # Correctly slice cos/sin for queries based on memory length\n",
    "        cos_q = cos_k[mlen:klen]\n",
    "        sin_q = sin_k[mlen:klen]\n",
    "\n",
    "        q_head_rot = apply_rotary_pos_emb(q_head, cos_q.unsqueeze(1).unsqueeze(2), sin_q.unsqueeze(1).unsqueeze(2))\n",
    "        k_head_rot = apply_rotary_pos_emb(k_head, cos_k.unsqueeze(1).unsqueeze(2), sin_k.unsqueeze(1).unsqueeze(2))\n",
    "\n",
    "        q_head_ = q_head_rot.permute(1, 2, 0, 3)\n",
    "        k_head_ = k_head_rot.permute(1, 2, 0, 3)\n",
    "        v_head_ = v_head.permute(1, 2, 0, 3)\n",
    "\n",
    "        attn_score = torch.matmul(q_head_, k_head_.transpose(-2, -1))\n",
    "        attn_score = attn_score * self.scale\n",
    "\n",
    "        if attn_mask is not None:\n",
    "             if attn_mask.dim() == 2:\n",
    "                 mask_to_apply = attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "             elif attn_mask.dim() == 4:\n",
    "                mask_to_apply = attn_mask\n",
    "             else:\n",
    "                 logging.warning(f\"Layer {self.layer_idx}: Unexpected attention mask shape {attn_mask.shape}. Ignoring mask.\")\n",
    "                 mask_to_apply = None\n",
    "\n",
    "             if mask_to_apply is not None:\n",
    "                 mask_to_apply = mask_to_apply.to(device=attn_score.device, dtype=torch.bool)\n",
    "                 if mask_to_apply.shape[-2:] == attn_score.shape[-2:]:\n",
    "                     # Expand mask if necessary to match batch and head dimensions\n",
    "                     # Ensure mask broadcasting works correctly: (bsz, n_head, qlen, klen)\n",
    "                     if mask_to_apply.shape[0] != attn_score.shape[0] and mask_to_apply.shape[0] == 1:\n",
    "                         mask_to_apply = mask_to_apply.expand(attn_score.shape[0], -1, -1, -1)\n",
    "                     if mask_to_apply.shape[1] != attn_score.shape[1] and mask_to_apply.shape[1] == 1:\n",
    "                         mask_to_apply = mask_to_apply.expand(-1, attn_score.shape[1], -1, -1)\n",
    "\n",
    "                     # Check again after potential expansion\n",
    "                     if mask_to_apply.shape == attn_score.shape:\n",
    "                         attn_score = attn_score.masked_fill(mask_to_apply, torch.finfo(attn_score.dtype).min)\n",
    "                     else:\n",
    "                         logging.warning(f\"Layer {self.layer_idx}: Mask shape {mask_to_apply.shape} mismatch with score shape {attn_score.shape} after broadcasting attempts. Ignoring mask.\")\n",
    "                 else:\n",
    "                     logging.warning(f\"Layer {self.layer_idx}: Mask shape {mask_to_apply.shape[-2:]} incompatible with score shape {attn_score.shape[-2:]}. Ignoring mask.\")\n",
    "\n",
    "        attn_prob = F.softmax(attn_score.float(), dim=-1).to(attn_score.dtype)\n",
    "        attn_prob = self.drop(attn_prob)\n",
    "\n",
    "        if head_mask is not None:\n",
    "             attn_prob = attn_prob * head_mask.to(attn_prob.device)\n",
    "\n",
    "        attn_vec = torch.matmul(attn_prob, v_head_)\n",
    "        attn_vec = attn_vec.permute(2, 0, 1, 3).contiguous()\n",
    "        attn_vec = attn_vec.view(qlen, bsz, self.n_head * self.d_head)\n",
    "\n",
    "        attn_out = self.o_net(attn_vec)\n",
    "        attn_out = self.drop(attn_out)\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "class TransformerXLLayer(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_head, d_inner, dropout, config: TrainingConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.dec_attn = RelPartialLearnableMultiHeadAttn(\n",
    "            n_head, d_model, d_head, dropout, config=config, layer_idx=layer_idx\n",
    "        )\n",
    "        self.pos_ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_inner),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_inner, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, dec_inp: torch.Tensor, mems: Optional[torch.Tensor], attn_mask: Optional[torch.Tensor] = None, head_mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        x_norm1 = self.norm1(dec_inp)\n",
    "        attn_output = self.dec_attn(\n",
    "            w=x_norm1, mems=mems, attn_mask=attn_mask, head_mask=head_mask\n",
    "        )\n",
    "        h = dec_inp + self.dropout(attn_output)\n",
    "        h_norm2 = self.norm2(h)\n",
    "        ff_output = self.pos_ff(h_norm2)\n",
    "        output = h + self.dropout(ff_output)\n",
    "        return output\n",
    "\n",
    "# === Main Model ===\n",
    "class MelodyTransformerXL(nn.Module):\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # --- Input validations ---\n",
    "        if config.melody_vocab_size <= 0:\n",
    "            raise ValueError(f\"config.melody_vocab_size must be positive, got {config.melody_vocab_size}\")\n",
    "        if config.melody_pad_token_id < 0 or config.melody_pad_token_id >= config.melody_vocab_size:\n",
    "            raise ValueError(f\"config.melody_pad_token_id ({config.melody_pad_token_id}) is out of range for melody_vocab_size ({config.melody_vocab_size}).\")\n",
    "\n",
    "        self.use_chord_embedding = config.chord_emb_dim is not None and config.chord_emb_dim > 0\n",
    "        if self.use_chord_embedding:\n",
    "            if config.chord_vocab_size <= 0:\n",
    "                 raise ValueError(f\"config.chord_vocab_size ({config.chord_vocab_size}) must be positive if chord_emb_dim ({config.chord_emb_dim}) is enabled.\")\n",
    "            if config.chord_pad_token_id < 0 or config.chord_pad_token_id >= config.chord_vocab_size:\n",
    "                 raise ValueError(f\"Chord pad token ID ({config.chord_pad_token_id}) is out of bounds for final chord vocab size ({config.chord_vocab_size}).\")\n",
    "        if config.d_model % config.n_head != 0:\n",
    "            raise ValueError(f\"d_model ({config.d_model}) must be divisible by n_head ({config.n_head}).\")\n",
    "        config.d_head = config.d_model // config.n_head\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "        self.mem_len = config.mem_len\n",
    "        self.n_layer = config.n_layer\n",
    "\n",
    "        self.melody_emb = nn.Embedding(config.melody_vocab_size, config.d_model, padding_idx=config.melody_pad_token_id)\n",
    "\n",
    "        condition_proj_dim = max(1, config.condition_proj_dim)\n",
    "        self.chord_feature_processor = nn.Linear(config.num_chord_features, condition_proj_dim)\n",
    "\n",
    "        total_conditioning_dim = condition_proj_dim\n",
    "        if self.use_chord_embedding:\n",
    "            chord_emb_dim = max(1, config.chord_emb_dim)\n",
    "            self.chord_emb = nn.Embedding(config.chord_vocab_size, chord_emb_dim, padding_idx=config.chord_pad_token_id)\n",
    "            total_conditioning_dim += chord_emb_dim\n",
    "        else:\n",
    "            self.chord_emb = None\n",
    "\n",
    "        combined_input_dim = config.d_model + total_conditioning_dim\n",
    "        self.input_proj = nn.Linear(combined_input_dim, config.d_model)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerXLLayer(\n",
    "                n_head=self.n_head, d_model=self.d_model, d_head=self.d_head,\n",
    "                d_inner=config.d_inner, dropout=config.dropout, config=config, layer_idx=i\n",
    "            ) for i in range(config.n_layer)\n",
    "        ])\n",
    "\n",
    "        self.final_norm = nn.LayerNorm(config.d_model)\n",
    "        self.out_layer = nn.Linear(config.d_model, config.melody_vocab_size, bias=False)\n",
    "\n",
    "        if config.d_model == self.melody_emb.embedding_dim:\n",
    "           self.out_layer.weight = self.melody_emb.weight\n",
    "           logging.info(\"Tying input melody embedding weights with the final output layer.\")\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        logging.info(f\"MelodyTransformerXL initialized with {config.n_layer} layers, d_model={config.d_model}, n_head={config.n_head}, mem_len={config.mem_len}\")\n",
    "        logging.info(f\"Melody Vocab Size: {config.melody_vocab_size}, Chord Vocab Size: {config.chord_vocab_size}\")\n",
    "        logging.info(f\"Using Chord Embeddings: {self.use_chord_embedding}\")\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        try:\n",
    "            return next(self.parameters()).dtype\n",
    "        except StopIteration:\n",
    "            return torch.get_default_dtype()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initializes weights using standard practices.\"\"\"\n",
    "        scale = 1.0\n",
    "        if hasattr(self.config, 'n_layer') and self.config.n_layer > 0:\n",
    "             scale = 1 / math.sqrt(2.0 * self.config.n_layer)\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02 * scale)\n",
    "            if module.bias is not None: nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                with torch.no_grad(): module.weight[module.padding_idx].fill_(0)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            if module.bias is not None: module.bias.data.zero_()\n",
    "            if module.weight is not None: module.weight.data.fill_(1.0)\n",
    "\n",
    "    def _update_mems(self, hids: List[torch.Tensor], mems: List[Optional[torch.Tensor]], mlen: int) -> List[Optional[torch.Tensor]]:\n",
    "        \"\"\"Updates the memory states, detaching the results.\"\"\"\n",
    "        if mlen <= 0 or not hids:\n",
    "            return [None] * (self.n_layer + 1)\n",
    "\n",
    "        if mems is None or all(m is None or m.numel() == 0 for m in mems):\n",
    "             return [(h[-mlen:].detach() if h is not None and h.numel() > 0 else None) for h in hids]\n",
    "\n",
    "        if len(hids) != len(mems):\n",
    "            logging.error(f\"BUG: Mismatch between number of hidden states ({len(hids)}) and memory slots ({len(mems)}). Cannot update memory reliably.\")\n",
    "            return [None] * (self.n_layer + 1)\n",
    "\n",
    "        new_mems = []\n",
    "        with torch.no_grad():\n",
    "            for i, hid in enumerate(hids):\n",
    "                mem = mems[i]\n",
    "                if hid is None:\n",
    "                    new_mems.append(mem.detach() if mem is not None else None)\n",
    "                    logging.warning(f\"Hidden state for layer {i} is None during memory update.\")\n",
    "                    continue\n",
    "\n",
    "                if mem is not None and mem.dim() == 3 and mem.numel() > 0:\n",
    "                    if mem.shape[1:] == hid.shape[1:]:\n",
    "                        cat = torch.cat([mem, hid], dim=0)\n",
    "                    else:\n",
    "                        logging.warning(f\"Memory shape {mem.shape} incompatible with hid shape {hid.shape} at layer {i}. Resetting memory segment for this layer.\")\n",
    "                        cat = hid\n",
    "                else:\n",
    "                    cat = hid\n",
    "\n",
    "                new_mems.append(cat[-mlen:].detach())\n",
    "\n",
    "        return new_mems\n",
    "\n",
    "    def init_mems(self, bsz: int, device: torch.device, dtype: torch.dtype) -> List[Optional[torch.Tensor]]:\n",
    "        \"\"\"Initializes memory states (as empty tensors or Nones).\"\"\"\n",
    "        if self.mem_len > 0:\n",
    "            mems = [\n",
    "                torch.empty((0, bsz, self.config.d_model), dtype=dtype, device=device)\n",
    "                for _ in range(self.n_layer + 1)\n",
    "            ]\n",
    "        else:\n",
    "            mems = [None] * (self.n_layer + 1)\n",
    "        return mems\n",
    "\n",
    "    def _create_attn_mask(self, qlen, mlen, device):\n",
    "        \"\"\"Creates the causal attention mask for Transformer-XL (True values are masked).\"\"\"\n",
    "        klen = mlen + qlen\n",
    "        mask = torch.triu(torch.ones(qlen, klen, device=device, dtype=torch.bool), diagonal=1 + mlen)\n",
    "        return mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        event_ids: torch.Tensor,                     # (bsz, qlen) [Long]\n",
    "        conditioning_chord_ids: torch.Tensor,       # (bsz, qlen) [Long]\n",
    "        conditioning_root_pc: torch.Tensor,         # (bsz, qlen) [Float]\n",
    "        conditioning_quality_code: torch.Tensor,    # (bsz, qlen) [Float]\n",
    "        conditioning_function_code: torch.Tensor,   # (bsz, qlen) [Float]\n",
    "        mems: Optional[List[Optional[torch.Tensor]]] = None\n",
    "    ) -> Tuple[torch.Tensor, List[Optional[torch.Tensor]]]:\n",
    "\n",
    "        bsz, qlen = event_ids.size()\n",
    "        device = event_ids.device\n",
    "        target_dtype = self.dtype\n",
    "\n",
    "        # --- Memory Validation & Initialization ---\n",
    "        if self.mem_len > 0:\n",
    "            if mems is None:\n",
    "                 mems = self.init_mems(bsz, device, target_dtype)\n",
    "            elif len(mems) != self.n_layer + 1:\n",
    "                 logging.warning(f\"Incorrect memory list length ({len(mems)} vs {self.n_layer + 1}). Resetting memory.\")\n",
    "                 mems = self.init_mems(bsz, device, target_dtype)\n",
    "            else:\n",
    "                 for i, mem in enumerate(mems):\n",
    "                     if mem is not None and mem.numel() > 0:\n",
    "                         if mem.shape[1] != bsz or mem.device != device or mem.dtype != target_dtype:\n",
    "                             logging.warning(f\"Memory state at index {i} incompatible. Resetting memory.\")\n",
    "                             mems = self.init_mems(bsz, device, target_dtype)\n",
    "                             break\n",
    "        else:\n",
    "            mems = [None] * (self.n_layer + 1)\n",
    "\n",
    "        mlen = mems[0].size(0) if mems is not None and mems[0] is not None and mems[0].dim() == 3 else 0\n",
    "\n",
    "        # --- Input Embeddings & Conditioning ---\n",
    "        clamped_event_ids = event_ids.clamp(0, self.config.melody_vocab_size - 1)\n",
    "        melody_embedded = self.melody_emb(clamped_event_ids)\n",
    "\n",
    "        cond_features_raw = torch.stack([\n",
    "            conditioning_root_pc, conditioning_quality_code, conditioning_function_code\n",
    "        ], dim=-1).to(target_dtype)\n",
    "        cond_features_proj = F.relu(self.chord_feature_processor(cond_features_raw))\n",
    "\n",
    "        if self.use_chord_embedding:\n",
    "            clamped_chord_ids = conditioning_chord_ids.clamp(0, self.config.chord_vocab_size - 1)\n",
    "            chord_embedded = self.chord_emb(clamped_chord_ids)\n",
    "            cond_combined = torch.cat([cond_features_proj, chord_embedded], dim=-1)\n",
    "        else:\n",
    "            cond_combined = cond_features_proj\n",
    "\n",
    "        combined_input = torch.cat([melody_embedded, cond_combined], dim=-1)\n",
    "        core_input = self.input_proj(combined_input)\n",
    "        core_input = self.drop(core_input)\n",
    "\n",
    "        # --- Transpose for Transformer Layers ---\n",
    "        core_input = core_input.transpose(0, 1).contiguous() # (qlen, bsz, d_model)\n",
    "\n",
    "        # --- Attention Mask ---\n",
    "        attn_mask = self._create_attn_mask(qlen, mlen, device) if qlen > 0 else None\n",
    "\n",
    "        # --- Pass through Layers ---\n",
    "        hids_for_mem = [core_input]\n",
    "        layer_input = core_input\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer_mem = mems[i] if mems is not None else None\n",
    "            layer_output = layer(\n",
    "                dec_inp=layer_input,\n",
    "                mems=layer_mem,\n",
    "                attn_mask=attn_mask\n",
    "            )\n",
    "            hids_for_mem.append(layer_output)\n",
    "            layer_input = layer_output\n",
    "\n",
    "        # --- Update Memory ---\n",
    "        new_mems = self._update_mems(hids_for_mem, mems, self.mem_len)\n",
    "\n",
    "        # --- Final Output Processing ---\n",
    "        core_output = self.drop(layer_input)\n",
    "        final_output = self.final_norm(core_output)\n",
    "        logits = self.out_layer(final_output)\n",
    "\n",
    "        logits = logits.transpose(0, 1).contiguous() # (bsz, qlen, melody_vocab_size)\n",
    "\n",
    "        return logits, new_mems\n",
    "\n",
    "\n",
    "# === Dataset & Collation ===\n",
    "class MelodyDataset(Dataset):\n",
    "    def __init__(self, jsonl_path: str, melody_vocab: Dict[str, int], config: TrainingConfig):\n",
    "        self.samples: List[Dict[str, Any]] = []\n",
    "        self.melody_vocab = melody_vocab\n",
    "        self.melody_pad_id = config.melody_pad_token_id\n",
    "        self.melody_unk_id = melody_vocab.get(MELODY_UNK_TOKEN)\n",
    "        if self.melody_unk_id is None:\n",
    "            logging.warning(f\"Melody UNK token '{MELODY_UNK_TOKEN}' not found in vocab! Using 0 (check for conflict with PAD).\")\n",
    "            self.melody_unk_id = 0\n",
    "\n",
    "        self.max_len = config.max_seq_len\n",
    "        self.sequence_key = \"event_ids\"\n",
    "\n",
    "        print(f\"Loading dataset from: {jsonl_path}...\")\n",
    "        skipped_count = 0\n",
    "        required_keys = [self.sequence_key, \"conditioning_chord_ids\", \"conditioning_root_pc\",\n",
    "                         \"conditioning_quality_code\", \"conditioning_function_code\"]\n",
    "        try:\n",
    "            with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    try:\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            logging.debug(f\"Skipping empty line ~{i+1}\")\n",
    "                            skipped_count += 1; continue\n",
    "                        record = json.loads(line)\n",
    "                        missing_keys = [key for key in required_keys if key not in record]\n",
    "                        if missing_keys:\n",
    "                            logging.debug(f\"Skipping line ~{i+1}: Missing keys: {missing_keys}. Line content: {line[:100]}...\")\n",
    "                            skipped_count += 1; continue\n",
    "                        event_list = record.get(self.sequence_key)\n",
    "                        if not isinstance(event_list, list) or not event_list:\n",
    "                            logging.debug(f\"Skipping line ~{i+1}: Invalid or empty '{self.sequence_key}'. Got: {type(event_list)}\")\n",
    "                            skipped_count += 1; continue\n",
    "                        if any(not isinstance(item, int) for item in event_list):\n",
    "                             logging.warning(f\"Skipping line ~{i+1}: '{self.sequence_key}' contains non-integer values.\")\n",
    "                             skipped_count += 1; continue\n",
    "                        event_len = len(event_list)\n",
    "                        if event_len == 0:\n",
    "                            logging.debug(f\"Skipping line ~{i+1}: Zero length '{self.sequence_key}'.\")\n",
    "                            skipped_count += 1; continue\n",
    "                        valid_lengths = True\n",
    "                        for cond_key in required_keys[1:]:\n",
    "                             cond_data = record.get(cond_key)\n",
    "                             if not isinstance(cond_data, list) or len(cond_data) != event_len:\n",
    "                                  logging.debug(f\"Skipping line ~{i+1}: Length mismatch for key '{cond_key}'. Melody len={event_len}, '{cond_key}' len={len(cond_data) if isinstance(cond_data, list) else 'Not a list'}.\")\n",
    "                                  valid_lengths = False; break\n",
    "                        if not valid_lengths:\n",
    "                            skipped_count += 1; continue\n",
    "                        self.samples.append(record)\n",
    "                    except json.JSONDecodeError:\n",
    "                        logging.warning(f\"Skipping invalid JSON on line ~{i+1}: {line[:100]}...\")\n",
    "                        skipped_count += 1\n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Skipping record due to error on line ~{i+1}: {e} - Line content: {line[:100]}...\")\n",
    "                        skipped_count += 1\n",
    "            if not self.samples: raise ValueError(\"Dataset loaded but contains 0 valid samples after filtering.\")\n",
    "            print(f\"Dataset loaded successfully: {len(self.samples)} samples (skipped {skipped_count} invalid lines).\")\n",
    "        except FileNotFoundError: print(f\"FATAL: Dataset file not found at {jsonl_path}\"); raise\n",
    "        except Exception as e: print(f\"FATAL: Error loading/parsing dataset {jsonl_path}: {e}\"); raise\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Optional[Dict[str, torch.Tensor]]:\n",
    "        try:\n",
    "            record = self.samples[idx]\n",
    "            event_ids_from_file = record[self.sequence_key]\n",
    "            m_ids = torch.tensor(event_ids_from_file, dtype=torch.long)\n",
    "            c_ids = torch.tensor(record[\"conditioning_chord_ids\"], dtype=torch.long)\n",
    "            c_root = torch.tensor(record[\"conditioning_root_pc\"], dtype=torch.float)\n",
    "            c_qual = torch.tensor(record[\"conditioning_quality_code\"], dtype=torch.float)\n",
    "            c_func = torch.tensor(record[\"conditioning_function_code\"], dtype=torch.float)\n",
    "\n",
    "            current_len = m_ids.shape[0]\n",
    "            if self.max_len is not None and current_len > self.max_len:\n",
    "                m_ids = m_ids[:self.max_len]\n",
    "                c_ids = c_ids[:self.max_len]\n",
    "                c_root = c_root[:self.max_len]\n",
    "                c_qual = c_qual[:self.max_len]\n",
    "                c_func = c_func[:self.max_len]\n",
    "                logging.debug(f\"Truncated sample {idx} from {current_len} to {self.max_len}\")\n",
    "\n",
    "            return {\"event_ids\": m_ids, \"conditioning_chord_ids\": c_ids, \"conditioning_root_pc\": c_root,\n",
    "                    \"conditioning_quality_code\": c_qual, \"conditioning_function_code\": c_func}\n",
    "        except KeyError as e:\n",
    "             logging.error(f\"ERROR in __getitem__ for index {idx}: Missing key '{e}'. Available keys: {list(self.samples[idx].keys()) if idx < len(self.samples) else 'Index out of bounds'}\")\n",
    "             return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"ERROR in __getitem__ for index {idx}: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "def get_pad_value(key: str, melody_pad_id: int, chord_pad_id: int) -> float:\n",
    "    if key == \"event_ids\": return float(melody_pad_id)\n",
    "    elif key == \"conditioning_chord_ids\": return float(chord_pad_id) if chord_pad_id >= 0 else 0.0\n",
    "    else: return 0.0\n",
    "\n",
    "def melody_collate_fn(batch: List[Optional[Dict[str, torch.Tensor]]], melody_pad_id: int, chord_pad_id: int) -> Optional[Dict[str, torch.Tensor]]:\n",
    "    valid_batch = [item for item in batch if item is not None]\n",
    "    if not valid_batch:\n",
    "        logging.debug(\"Collate function received an empty or all-None batch.\")\n",
    "        return None\n",
    "\n",
    "    sequence_key = \"event_ids\"\n",
    "    required_keys = {\"event_ids\", \"conditioning_chord_ids\", \"conditioning_root_pc\", \"conditioning_quality_code\", \"conditioning_function_code\"}\n",
    "    first_item_keys = valid_batch[0].keys()\n",
    "    if not required_keys.issubset(first_item_keys):\n",
    "         logging.error(f\"Required keys missing in first valid batch item for collation. Expected: {required_keys}, Got: {first_item_keys}\")\n",
    "         return None\n",
    "\n",
    "    batch_data = collections.defaultdict(list)\n",
    "    actual_items_in_batch = []\n",
    "\n",
    "    for item_idx, item in enumerate(valid_batch):\n",
    "        if not all(k in item for k in required_keys):\n",
    "             logging.debug(f\"Skipping item index {item_idx} in collate_fn due to missing keys: {required_keys - set(item.keys())}\")\n",
    "             continue\n",
    "        seq_tensor = item[sequence_key]\n",
    "        if not isinstance(seq_tensor, torch.Tensor) or seq_tensor.numel() == 0:\n",
    "             logging.debug(f\"Skipping item index {item_idx} in collate_fn due to invalid sequence tensor.\")\n",
    "             continue\n",
    "        current_len = seq_tensor.shape[0]\n",
    "        mismatched = False\n",
    "        for cond_key in required_keys - {sequence_key}:\n",
    "            if item[cond_key].shape[0] != current_len:\n",
    "                logging.warning(f\"Skipping item index {item_idx} due to length mismatch: '{sequence_key}' ({current_len}) vs '{cond_key}' ({item[cond_key].shape[0]})\")\n",
    "                mismatched = True; break\n",
    "        if mismatched: continue\n",
    "        actual_items_in_batch.append(item)\n",
    "\n",
    "    if not actual_items_in_batch:\n",
    "        logging.warning(\"Collate function: No valid items found in the batch to process.\")\n",
    "        return None\n",
    "\n",
    "    for item in actual_items_in_batch:\n",
    "         for key, tensor in item.items():\n",
    "            if key in required_keys: batch_data[key].append(tensor)\n",
    "\n",
    "    padded_batch = {}\n",
    "    try:\n",
    "        for key in required_keys:\n",
    "             tensor_list = batch_data.get(key)\n",
    "             if not tensor_list:\n",
    "                 logging.error(f\"Collate Error: Missing data for required key '{key}' after filtering batch.\"); return None\n",
    "             pad_val = get_pad_value(key, melody_pad_id, chord_pad_id)\n",
    "             padded_sequences = pad_sequence(tensor_list, batch_first=True, padding_value=pad_val)\n",
    "             if key == sequence_key or key == \"conditioning_chord_ids\":\n",
    "                 padded_batch[key] = padded_sequences.long()\n",
    "             else:\n",
    "                 padded_batch[key] = padded_sequences.float()\n",
    "\n",
    "        final_bsz = len(actual_items_in_batch)\n",
    "        if sequence_key in padded_batch and padded_batch[sequence_key].shape[0] != final_bsz:\n",
    "            logging.error(f\"Collate Error: Batch size mismatch after padding. Processed {final_bsz} items, but tensor has shape {padded_batch[sequence_key].shape[0]}.\")\n",
    "            return None\n",
    "        if padded_batch[sequence_key].shape[0] == 0:\n",
    "             logging.warning(\"Collate function produced a zero-size batch after processing.\"); return None\n",
    "        return padded_batch\n",
    "    except Exception as e:\n",
    "        logging.error(f\"ERROR during padding/stacking in collate_fn: {e}\", exc_info=True); return None\n",
    "\n",
    "# === Training & Evaluation Functions ===\n",
    "\n",
    "def train_epoch(model: MelodyTransformerXL, dataloader: DataLoader, optimizer: torch.optim.Optimizer,\n",
    "                criterion: nn.Module, scaler: GradScaler, epoch: int, config: TrainingConfig, device: torch.device):\n",
    "    \"\"\"Trains the model for one epoch using segmental training (stateless).\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_tokens = 0.0, 0.0, 0.0\n",
    "    epoch_start_time = time.time()\n",
    "    amp_enabled = scaler.is_enabled()\n",
    "    model_dtype = model.dtype\n",
    "\n",
    "    try: num_batches = len(dataloader)\n",
    "    except TypeError: num_batches = -1\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}/{config.num_epochs} Train\", unit=\"batch\", leave=False, disable=(num_batches <= 0))\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # print(f\"Processing Train Batch {batch_idx+1}/{num_batches}\") # Debug print\n",
    "\n",
    "        if batch is None:\n",
    "             logging.warning(f\"Skipping None batch at index {batch_idx} in training.\")\n",
    "             continue\n",
    "\n",
    "        try:\n",
    "            event_ids = batch[\"event_ids\"].to(device, non_blocking=True)\n",
    "            cond_ids = batch[\"conditioning_chord_ids\"].to(device, non_blocking=True)\n",
    "            cond_root = batch[\"conditioning_root_pc\"].to(device=device, dtype=model_dtype, non_blocking=True)\n",
    "            cond_qual = batch[\"conditioning_quality_code\"].to(device=device, dtype=model_dtype, non_blocking=True)\n",
    "            cond_func = batch[\"conditioning_function_code\"].to(device=device, dtype=model_dtype, non_blocking=True)\n",
    "\n",
    "            targets = event_ids[:, 1:].contiguous()\n",
    "            model_input_ids = event_ids[:, :-1].contiguous()\n",
    "            cond_ids = cond_ids[:, :-1].contiguous()\n",
    "            cond_root = cond_root[:, :-1].contiguous()\n",
    "            cond_qual = cond_qual[:, :-1].contiguous()\n",
    "            cond_func = cond_func[:, :-1].contiguous()\n",
    "\n",
    "            qlen = model_input_ids.shape[1]\n",
    "            if qlen == 0:\n",
    "                logging.warning(f\"Skipping train batch {batch_idx}: sequence length is 0 after shifting.\")\n",
    "                continue\n",
    "\n",
    "            current_mems = model.init_mems(event_ids.size(0), device, model_dtype)\n",
    "\n",
    "            max_eid = model_input_ids.max().item() if qlen > 0 else -1\n",
    "            max_cid = cond_ids.max().item() if qlen > 0 and model.use_chord_embedding else -1\n",
    "            logging.debug(f\"[Train Batch {batch_idx}] Max event ID: {max_eid} (Vocab: {config.melody_vocab_size}), Max chord ID: {max_cid} (Vocab: {config.chord_vocab_size})\")\n",
    "            if max_eid >= config.melody_vocab_size:\n",
    "                logging.error(f\"!! Index Error Pre-Check Failed: Max Event ID {max_eid} >= Vocab Size {config.melody_vocab_size} in Train Batch {batch_idx}\")\n",
    "                continue\n",
    "            if model.use_chord_embedding and max_cid >= config.chord_vocab_size:\n",
    "                logging.error(f\"!! Index Error Pre-Check Failed: Max Chord ID {max_cid} >= Vocab Size {config.chord_vocab_size} in Train Batch {batch_idx}\")\n",
    "                continue\n",
    "\n",
    "        except KeyError as e:\n",
    "            logging.error(f\"KeyError preparing training batch {batch_idx}: {e}. Available keys: {list(batch.keys())}. Skipping batch.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error preparing training batch {batch_idx}: {e}. Skipping batch.\", exc_info=True)\n",
    "            continue\n",
    "\n",
    "        # --- Forward Pass & Loss ---\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        amp_dtype = config.amp_dtype if (amp_enabled and config.amp_dtype is not None) else None\n",
    "        device_type_str = device.type # 'cuda' or 'cpu'\n",
    "\n",
    "        try:\n",
    "            # *** CORRECTED autocast CALL ***\n",
    "            with torch.amp.autocast(device_type_str, dtype=amp_dtype, enabled=amp_enabled):\n",
    "                logits, _ = model(\n",
    "                    event_ids=model_input_ids,\n",
    "                    conditioning_chord_ids=cond_ids,\n",
    "                    conditioning_root_pc=cond_root,\n",
    "                    conditioning_quality_code=cond_qual,\n",
    "                    conditioning_function_code=cond_func,\n",
    "                    mems=current_mems\n",
    "                )\n",
    "                logits_flat = logits.view(-1, config.melody_vocab_size)\n",
    "                targets_flat = targets.view(-1)\n",
    "                targets_safe = targets_flat.clamp(0, config.melody_vocab_size - 1)\n",
    "                loss = criterion(logits_flat.float(), targets_safe) # Loss still in float32\n",
    "\n",
    "        except IndexError as e:\n",
    "             logging.error(f\"IndexError during model forward/loss in training (Batch {batch_idx}): {e}. \", exc_info=True)\n",
    "             print(f\"!!! IndexError in TRAIN Batch {batch_idx}: {e}\") # Explicit print\n",
    "             gc.collect(); torch.cuda.empty_cache(); continue\n",
    "        except RuntimeError as e: # Catch CUDA errors specifically\n",
    "             logging.error(f\"RuntimeError during model forward/loss in training (Batch {batch_idx}): {e}\", exc_info=True)\n",
    "             print(f\"!!! RuntimeError in TRAIN Batch {batch_idx}: {e}\") # Explicit print\n",
    "             if 'cuda' in str(e).lower(): gc.collect(); torch.cuda.empty_cache()\n",
    "             continue # Skip this batch\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Generic Error during model forward/loss in training (Batch {batch_idx}): {e}\", exc_info=True)\n",
    "             print(f\"!!! ERROR in TRAIN Batch {batch_idx}: {e}\") # Explicit print\n",
    "             if 'cuda' in str(e).lower(): gc.collect(); torch.cuda.empty_cache()\n",
    "             continue # Skip this batch\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            logging.warning(f\"Non-finite loss ({loss.item()}) at Train Batch {batch_idx}. Skipping backward/step.\")\n",
    "            print(f\"!!! Non-finite loss in TRAIN Batch {batch_idx}: {loss.item()}\") # Explicit print\n",
    "            optimizer.zero_grad(set_to_none=True); gc.collect(); torch.cuda.empty_cache(); continue\n",
    "\n",
    "        # --- Backward Pass & Optimization ---\n",
    "        try:\n",
    "            if amp_enabled:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip_value)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip_value)\n",
    "                optimizer.step()\n",
    "        except RuntimeError as e: # Specifically catch the scaler error\n",
    "             logging.error(f\"RuntimeError during backward/step in training (Batch {batch_idx}): {e}\", exc_info=False) # Reduce traceback noise\n",
    "             print(f\"!!! ERROR during backward/step in TRAIN Batch {batch_idx}: {e}\") # Explicit print\n",
    "             optimizer.zero_grad(set_to_none=True)\n",
    "             gc.collect(); torch.cuda.empty_cache()\n",
    "             continue # Skip metrics update for this batch\n",
    "        except Exception as e: # Catch other potential errors\n",
    "             logging.error(f\"Error during backward/step in training (Batch {batch_idx}): {e}\", exc_info=True)\n",
    "             print(f\"!!! ERROR during backward/step in TRAIN Batch {batch_idx}: {e}\") # Explicit print\n",
    "             optimizer.zero_grad(set_to_none=True)\n",
    "             gc.collect(); torch.cuda.empty_cache()\n",
    "             continue\n",
    "\n",
    "\n",
    "        # --- Metrics ---\n",
    "        with torch.no_grad():\n",
    "            # Check if logits were actually computed before calculating accuracy\n",
    "            if 'logits_flat' in locals() and logits_flat is not None:\n",
    "                acc, n_tokens = calculate_accuracy(logits_flat, targets_safe, config.melody_pad_token_id)\n",
    "                if n_tokens > 0:\n",
    "                    total_loss += loss.item() * n_tokens\n",
    "                    total_correct += acc * n_tokens\n",
    "                    total_tokens += n_tokens\n",
    "            else:\n",
    "                # This case should ideally not be reached if the batch wasn't skipped\n",
    "                logging.warning(f\"Logits were not computed for Train Batch {batch_idx}, skipping metric update.\")\n",
    "\n",
    "\n",
    "        # Update progress bar postfix\n",
    "        if total_tokens > 0:\n",
    "             avg_loss = total_loss / total_tokens\n",
    "             avg_acc = total_correct / total_tokens\n",
    "             progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\", acc=f\"{avg_acc:.4f}\", lr=f\"{optimizer.param_groups[0]['lr']:.1e}\", refresh=True)\n",
    "        else:\n",
    "             progress_bar.set_postfix(loss=\"N/A\", acc=\"N/A\", lr=f\"{optimizer.param_groups[0]['lr']:.1e}\", refresh=True)\n",
    "\n",
    "    progress_bar.close()\n",
    "    final_loss = total_loss / total_tokens if total_tokens > 0 else 0.0\n",
    "    final_acc = total_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "    train_summary = f\"Epoch {epoch} Train Summary | Time: {time.time() - epoch_start_time:.2f}s | Loss: {final_loss:.4f} | Acc: {final_acc:.4f} | Tokens: {int(total_tokens)}\"\n",
    "    logging.info(train_summary)\n",
    "    print(train_summary) # Explicit print for visibility\n",
    "    return final_loss, final_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_epoch(model: MelodyTransformerXL, dataloader: DataLoader, criterion: nn.Module,\n",
    "                   config: TrainingConfig, device: torch.device) -> Dict:\n",
    "    \"\"\"Evaluates the model on a given dataset (stateless).\"\"\"\n",
    "    model.eval()\n",
    "    total_eval_loss, total_tokens_eval, total_correct_eval = 0.0, 0.0, 0.0\n",
    "    all_preds_cpu: List[List[int]] = []\n",
    "    all_targets_cpu: List[List[int]] = []\n",
    "    amp_enabled = (config.amp_dtype == torch.float16 and device.type == 'cuda')\n",
    "    device_type_str = device.type # 'cuda' or 'cpu'\n",
    "    model_dtype = model.dtype\n",
    "\n",
    "    try: num_batches = len(dataloader)\n",
    "    except TypeError: num_batches = -1\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", unit=\"batch\", leave=False, disable=(num_batches <= 0))\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # print(f\"Processing Eval Batch {batch_idx+1}/{num_batches}\") # Debug print\n",
    "\n",
    "        if batch is None:\n",
    "             logging.warning(f\"Skipping None batch at index {batch_idx} in evaluation.\")\n",
    "             continue\n",
    "\n",
    "        try:\n",
    "            event_ids = batch[\"event_ids\"].to(device, non_blocking=True)\n",
    "            cond_ids = batch[\"conditioning_chord_ids\"].to(device, non_blocking=True)\n",
    "            cond_root = batch[\"conditioning_root_pc\"].to(device=device, dtype=model_dtype, non_blocking=True)\n",
    "            cond_qual = batch[\"conditioning_quality_code\"].to(device=device, dtype=model_dtype, non_blocking=True)\n",
    "            cond_func = batch[\"conditioning_function_code\"].to(device=device, dtype=model_dtype, non_blocking=True)\n",
    "\n",
    "            targets = event_ids[:, 1:].contiguous()\n",
    "            model_input_ids = event_ids[:, :-1].contiguous()\n",
    "\n",
    "            qlen = model_input_ids.shape[1]\n",
    "            if qlen == 0:\n",
    "                logging.debug(f\"Skipping eval batch {batch_idx}: sequence length is 0.\")\n",
    "                continue\n",
    "\n",
    "            cond_ids = cond_ids[:, :-1].contiguous()\n",
    "            cond_root = cond_root[:, :-1].contiguous()\n",
    "            cond_qual = cond_qual[:, :-1].contiguous()\n",
    "            cond_func = cond_func[:, :-1].contiguous()\n",
    "\n",
    "            # *** DEBUG LOGGING: Check max IDs just before model call ***\n",
    "            max_eid = model_input_ids.max().item() if qlen > 0 else -1\n",
    "            max_cid = cond_ids.max().item() if qlen > 0 and model.use_chord_embedding else -1\n",
    "            logging.debug(f\"[Eval Batch {batch_idx}] Max event ID: {max_eid} (Vocab: {config.melody_vocab_size}), Max chord ID: {max_cid} (Vocab: {config.chord_vocab_size})\")\n",
    "            if max_eid >= config.melody_vocab_size:\n",
    "                logging.error(f\"!! Index Error Pre-Check Failed: Max Event ID {max_eid} >= Vocab Size {config.melody_vocab_size} in Eval Batch {batch_idx}\")\n",
    "                continue\n",
    "            if model.use_chord_embedding and max_cid >= config.chord_vocab_size:\n",
    "                logging.error(f\"!! Index Error Pre-Check Failed: Max Chord ID {max_cid} >= Vocab Size {config.chord_vocab_size} in Eval Batch {batch_idx}\")\n",
    "                continue\n",
    "\n",
    "        except KeyError as e:\n",
    "            logging.error(f\"KeyError preparing evaluation batch {batch_idx}: {e}. Available keys: {list(batch.keys())}. Skipping batch.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error preparing evaluation batch {batch_idx}: {e}. Skipping batch.\", exc_info=True)\n",
    "            continue\n",
    "\n",
    "        # --- Forward Pass & Loss (Stateless) ---\n",
    "        current_mems = None\n",
    "        amp_dtype = config.amp_dtype if (amp_enabled and config.amp_dtype is not None) else None\n",
    "\n",
    "        try:\n",
    "            # *** CORRECTED autocast CALL ***\n",
    "            with torch.amp.autocast(device_type_str, dtype=amp_dtype, enabled=amp_enabled):\n",
    "                logits, _ = model(\n",
    "                    event_ids=model_input_ids,\n",
    "                    conditioning_chord_ids=cond_ids, conditioning_root_pc=cond_root,\n",
    "                    conditioning_quality_code=cond_qual, conditioning_function_code=cond_func,\n",
    "                    mems=current_mems\n",
    "                )\n",
    "                logits_flat = logits.view(-1, config.melody_vocab_size)\n",
    "                targets_flat = targets.view(-1)\n",
    "                targets_safe = targets_flat.clamp(0, config.melody_vocab_size - 1)\n",
    "                loss = criterion(logits_flat.float(), targets_safe) # Loss still in float32\n",
    "\n",
    "        except IndexError as e:\n",
    "             logging.error(f\"IndexError during model forward/loss in evaluation (Batch {batch_idx}): {e}. \", exc_info=True)\n",
    "             print(f\"!!! IndexError in EVAL Batch {batch_idx}: {e}\") # Explicit print\n",
    "             gc.collect(); torch.cuda.empty_cache(); continue\n",
    "        except RuntimeError as e: # Catch CUDA errors specifically\n",
    "             logging.error(f\"RuntimeError during model forward/loss in evaluation (Batch {batch_idx}): {e}\", exc_info=True)\n",
    "             print(f\"!!! RuntimeError in EVAL Batch {batch_idx}: {e}\") # Explicit print\n",
    "             if 'cuda' in str(e).lower(): gc.collect(); torch.cuda.empty_cache()\n",
    "             continue # Skip this batch\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Generic Error during model forward/loss in evaluation (Batch {batch_idx}): {e}\", exc_info=True)\n",
    "             print(f\"!!! ERROR in EVAL Batch {batch_idx}: {e}\") # Explicit print\n",
    "             if 'cuda' in str(e).lower(): gc.collect(); torch.cuda.empty_cache()\n",
    "             continue # Skip this batch\n",
    "\n",
    "        # --- Metrics ---\n",
    "        if torch.isfinite(loss):\n",
    "             acc, n_tokens = calculate_accuracy(logits_flat, targets_safe, config.melody_pad_token_id)\n",
    "             if n_tokens > 0:\n",
    "                  total_eval_loss += loss.item() * n_tokens\n",
    "                  total_correct_eval += acc * n_tokens\n",
    "                  total_tokens_eval += n_tokens\n",
    "                  if total_tokens_eval > 0:\n",
    "                     avg_loss = total_eval_loss / total_tokens_eval\n",
    "                     avg_acc_eval = total_correct_eval / total_tokens_eval\n",
    "                     progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\", acc=f\"{avg_acc_eval:.4f}\", refresh=True)\n",
    "        else:\n",
    "             logging.warning(f\"Non-finite loss ({loss.item()}) encountered in Eval Batch {batch_idx}.\")\n",
    "             print(f\"!!! Non-finite loss in EVAL Batch {batch_idx}: {loss.item()}\") # Explicit print\n",
    "\n",
    "        # Store predictions and targets\n",
    "        if 'logits' in locals() and logits is not None:\n",
    "             preds = logits.argmax(dim=-1)\n",
    "             all_preds_cpu.extend(preds.cpu().tolist())\n",
    "             all_targets_cpu.extend(targets.cpu().tolist())\n",
    "        else:\n",
    "             logging.warning(f\"Logits not generated for Eval Batch {batch_idx}, cannot store predictions.\")\n",
    "\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    # --- Calculate Final Metrics ---\n",
    "    logging.info(\" Aggregating & Calculating Final Eval Metrics on CPU...\")\n",
    "    final_loss = total_eval_loss / total_tokens_eval if total_tokens_eval > 0 else float('inf')\n",
    "    final_acc = total_correct_eval / total_tokens_eval if total_tokens_eval > 0 else 0.0\n",
    "    ssmd_sum, gs_sum, metric_count = 0.0, 0.0, 0\n",
    "\n",
    "    if not all_targets_cpu or total_tokens_eval == 0:\n",
    "        logging.warning(\"No sequences successfully processed for SSMD/GS metric calculation.\")\n",
    "        final_ssmd, final_gs = 1.0, 0.0\n",
    "    else:\n",
    "        pad_id = config.melody_pad_token_id\n",
    "        cpu_loop = tqdm(zip(all_preds_cpu, all_targets_cpu), total=len(all_targets_cpu), desc=\"Calculating SSMD/GS\", leave=False, disable=True)\n",
    "        for p_seq, t_seq in cpu_loop:\n",
    "            try:\n",
    "                p_seq_int = [int(p) for p in p_seq]\n",
    "                t_seq_int = [int(t) for t in t_seq]\n",
    "                ssmd = self_similarity_matrix_distance(p_seq_int, t_seq_int, pad_id)\n",
    "                gs = grooving_similarity(p_seq_int, t_seq_int, pad_id)\n",
    "                if not math.isnan(ssmd) and not math.isnan(gs):\n",
    "                    ssmd_sum += ssmd\n",
    "                    gs_sum += gs\n",
    "                    metric_count += 1\n",
    "                else:\n",
    "                    logging.warning(f\"NaN encountered during SSMD/GS calculation. Skipping pair.\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error calculating metrics for one sequence pair: {e}\")\n",
    "                continue\n",
    "\n",
    "        final_ssmd = ssmd_sum / metric_count if metric_count > 0 else 1.0\n",
    "        final_gs = gs_sum / metric_count if metric_count > 0 else 0.0\n",
    "\n",
    "    eval_summary = f\" Eval Summary | Loss: {final_loss:.4f} | Accuracy: {final_acc:.4f} | SSMD: {final_ssmd:.4f} | GS: {final_gs:.4f} | Eval Seq Count: {metric_count} | Eval Tokens: {int(total_tokens_eval)}\"\n",
    "    logging.info(eval_summary)\n",
    "    print(eval_summary) # Explicit print\n",
    "    return {\"loss\": final_loss, \"accuracy\": final_acc, \"ssmd\": final_ssmd, \"gs\": final_gs, \"count\": metric_count, \"tokens\": total_tokens_eval}\n",
    "\n",
    "\n",
    "# === Checkpoint Functions ===\n",
    "def save_checkpoint(state: Dict, filepath: str):\n",
    "    \"\"\"Saves model checkpoint atomically.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        tmp_filepath = filepath + \".tmp\"\n",
    "        torch.save(state, tmp_filepath)\n",
    "        try:\n",
    "            os.replace(tmp_filepath, filepath)\n",
    "        except OSError:\n",
    "             os.rename(tmp_filepath, filepath)\n",
    "        logging.info(f\" Checkpoint saved successfully to {filepath}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"ERROR saving checkpoint to {filepath}: {e}\", exc_info=True)\n",
    "        if os.path.exists(tmp_filepath):\n",
    "            try: os.remove(tmp_filepath)\n",
    "            except OSError as remove_err:\n",
    "                logging.error(f\"Error removing temporary checkpoint file {tmp_filepath}: {remove_err}\")\n",
    "\n",
    "def load_checkpoint(filepath: str, model: nn.Module, optimizer: Optional[torch.optim.Optimizer] = None, scaler: Optional[GradScaler] = None, device: torch.device = torch.device(\"cpu\")) -> Tuple[int, float]:\n",
    "    \"\"\"Loads checkpoint. Returns start_epoch and best_val_loss.\"\"\"\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    if os.path.exists(filepath):\n",
    "        try:\n",
    "            logging.info(f\"Attempting to load checkpoint from: {filepath}\")\n",
    "            checkpoint = torch.load(filepath, map_location=device, weights_only=False)\n",
    "            logging.info(f\"Successfully loaded checkpoint file.\")\n",
    "\n",
    "            # --- Load Model State Dict ---\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                state_dict = checkpoint['model_state_dict']\n",
    "                is_parallel_ckpt = all(key.startswith('module.') for key in state_dict)\n",
    "                current_is_parallel = isinstance(model, (nn.DataParallel, nn.parallel.DistributedDataParallel))\n",
    "\n",
    "                if is_parallel_ckpt and not current_is_parallel:\n",
    "                    logging.info(\"Removing 'module.' prefix from checkpoint state_dict keys.\")\n",
    "                    state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}\n",
    "                elif not is_parallel_ckpt and current_is_parallel:\n",
    "                     logging.info(\"Adding 'module.' prefix to checkpoint state_dict keys for DataParallel/DDP model.\")\n",
    "                     state_dict = {'module.' + k: v for k, v in state_dict.items()}\n",
    "\n",
    "                missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "                if unexpected_keys: logging.warning(f\" Checkpoint contained unexpected keys: {unexpected_keys}\")\n",
    "                if missing_keys: logging.warning(f\" Checkpoint was missing keys for the current model: {missing_keys}\")\n",
    "                logging.info(\" Model state loaded from checkpoint.\")\n",
    "            else:\n",
    "                logging.warning(\" Checkpoint does not contain 'model_state_dict'. Model weights not loaded.\")\n",
    "\n",
    "            # --- Load Optimizer State Dict ---\n",
    "            if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "                try:\n",
    "                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                    for state in optimizer.state.values():\n",
    "                        for k, v in state.items():\n",
    "                            if isinstance(v, torch.Tensor):\n",
    "                                state[k] = v.to(device)\n",
    "                    logging.info(\" Optimizer state loaded from checkpoint.\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\" Could not load optimizer state: {e}. Optimizer state will be re-initialized.\")\n",
    "                    optimizer.state = collections.defaultdict(dict)\n",
    "            elif optimizer:\n",
    "                logging.info(\" No optimizer state found in checkpoint. Optimizer will start from scratch.\")\n",
    "\n",
    "            # --- Load GradScaler State Dict ---\n",
    "            if scaler and scaler.is_enabled() and 'scaler_state_dict' in checkpoint and checkpoint['scaler_state_dict'] is not None:\n",
    "                 try:\n",
    "                     scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "                     logging.info(\" GradScaler state loaded from checkpoint.\")\n",
    "                 except Exception as e:\n",
    "                     logging.warning(f\" Could not load GradScaler state: {e}. Scaler state will be re-initialized.\")\n",
    "                     scaler._init_state()\n",
    "            elif scaler and scaler.is_enabled():\n",
    "                logging.info(\" No GradScaler state found or scaler is disabled. Scaler will start from scratch.\")\n",
    "\n",
    "            # --- Load Epoch and Best Loss ---\n",
    "            if 'epoch' in checkpoint:\n",
    "                 start_epoch = checkpoint['epoch'] + 1\n",
    "            else:\n",
    "                 start_epoch = 0\n",
    "                 logging.warning(\" Checkpoint missing 'epoch' information. Assuming start from epoch 0.\")\n",
    "\n",
    "            best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "            logging.info(f\" Checkpoint loaded. Resuming training from epoch {start_epoch + 1}. Best validation loss recorded in checkpoint: {best_val_loss:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"ERROR loading checkpoint '{filepath}': {e}. Starting from scratch.\", exc_info=True)\n",
    "            start_epoch = 0\n",
    "            best_val_loss = float('inf')\n",
    "            if optimizer: optimizer.state = collections.defaultdict(dict)\n",
    "            if scaler: scaler._init_state()\n",
    "    else:\n",
    "        logging.info(f\"No checkpoint found at '{filepath}'. Starting training from scratch (epoch 1).\")\n",
    "        start_epoch = 0\n",
    "\n",
    "    start_epoch = max(0, start_epoch)\n",
    "    return start_epoch, best_val_loss\n",
    "\n",
    "\n",
    "# === Main Execution Block ===\n",
    "if __name__ == '__main__':\n",
    "    # Setup logging - Use DEBUG for more detailed output during troubleshooting\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    main_start_time = time.time()\n",
    "    try:\n",
    "        tz = pytz.timezone('UTC')\n",
    "        now = datetime.datetime.now(tz)\n",
    "        current_time_str = now.strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "        print(f\"Script Execution Start Time: {current_time_str}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not set timezone context using pytz: {e}\")\n",
    "        now = datetime.datetime.now()\n",
    "        current_time_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"Script Execution Start Time (Local): {current_time_str}\")\n",
    "\n",
    "\n",
    "    # --- Configuration ---\n",
    "    config = TrainingConfig()\n",
    "    print(\"\\n--- Configuration ---\")\n",
    "    for key, value in sorted(config.__dict__.items()): print(f\"  {key}: {value}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- Setup ---\n",
    "    set_seed(config.seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "    print(f\"\\nUsing device: {device} ({gpu_name})\")\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"CUDA Compute Capability: {torch.cuda.get_device_capability(device)}\")\n",
    "        print(f\"PyTorch Version: {torch.__version__}\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}, version: {torch.version.cuda}\")\n",
    "        print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}, version: {torch.backends.cudnn.version()}\")\n",
    "        # os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True' # Uncomment if OOM persists\n",
    "        # logging.info(\"Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\")\n",
    "\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    latest_checkpoint_path = os.path.join(config.output_dir, \"latest_checkpoint.pth\")\n",
    "    best_model_path = os.path.join(config.output_dir, \"best_model.pth\")\n",
    "\n",
    "    # --- Load Vocabs & Determine Initial Config Sizes ---\n",
    "    print(\"\\n--- Loading Vocabularies ---\")\n",
    "    logging.info(f\"Loading melody vocab from: {config.melody_vocab_path}\")\n",
    "    logging.info(f\"Loading chord vocab from: {Path(config.chord_data_dir) / CHORD_VOCAB_FILENAME}\")\n",
    "    melody_vocab = None; chord_vocab = None\n",
    "    try:\n",
    "        melody_vocab_p = Path(config.melody_vocab_path)\n",
    "        if not melody_vocab_p.is_file(): raise FileNotFoundError(f\"Melody vocab file NOT FOUND: {melody_vocab_p}\")\n",
    "        with open(melody_vocab_p, 'r', encoding='utf-8') as f: melody_vocab = json.load(f)\n",
    "        config.melody_vocab_size = len(melody_vocab)\n",
    "        pad_id_lookup = melody_vocab.get(MELODY_PAD_TOKEN)\n",
    "        unk_id_lookup = melody_vocab.get(MELODY_UNK_TOKEN)\n",
    "        if pad_id_lookup is None: logging.warning(f\"Melody PAD token '{MELODY_PAD_TOKEN}' not found in vocab! Using default 0.\"); config.melody_pad_token_id = 0\n",
    "        else: config.melody_pad_token_id = int(pad_id_lookup)\n",
    "        if unk_id_lookup is None: logging.warning(f\"Melody UNK token '{MELODY_UNK_TOKEN}' not found in vocab! Using default ID {config.melody_pad_token_id + 1}.\")\n",
    "        logging.info(f\" Initial Melody Vocab Size (from file): {config.melody_vocab_size}, Pad ID: {config.melody_pad_token_id}\")\n",
    "\n",
    "        chord_vocab_path = Path(config.chord_data_dir) / CHORD_VOCAB_FILENAME\n",
    "        if not chord_vocab_path.is_file(): raise FileNotFoundError(f\"Chord vocab file NOT FOUND: {chord_vocab_path}\")\n",
    "        with open(chord_vocab_path, 'r', encoding='utf-8') as f: chord_vocab = json.load(f)\n",
    "        config.chord_vocab_size = len(chord_vocab)\n",
    "        pad_label = MELODY_PAD_TOKEN\n",
    "        pad_id_lookup = chord_vocab.get(pad_label)\n",
    "        if pad_id_lookup is None: logging.warning(f\"Chord PAD token '{pad_label}' not found in chord vocab! Using default 0.\"); config.chord_pad_token_id = 0\n",
    "        else: config.chord_pad_token_id = int(pad_id_lookup)\n",
    "        logging.info(f\" Initial Chord Vocab Size (from file): {config.chord_vocab_size}, Pad ID: {config.chord_pad_token_id}\")\n",
    "\n",
    "    except FileNotFoundError as e: logging.critical(f\"{e}\"); sys.exit(1)\n",
    "    except Exception as e: logging.critical(f\"Error loading vocabularies: {e}\", exc_info=True); sys.exit(1)\n",
    "\n",
    "    # --- Load Dataset & Finalize Vocab Sizes ---\n",
    "    print(\"\\n--- Loading & Processing Dataset ---\")\n",
    "    train_set, val_set, test_set = None, None, None; dataset = None\n",
    "    train_loader, val_loader, test_loader = None, None, None\n",
    "    train_size, val_size, test_size = 0, 0, 0\n",
    "    max_melody_id_found = -1\n",
    "    max_chord_id_found = -1\n",
    "    try:\n",
    "        if melody_vocab is None: raise RuntimeError(\"Melody vocabulary was not loaded.\")\n",
    "        melody_data_p = Path(config.melody_data_path)\n",
    "        if not melody_data_p.is_file(): raise FileNotFoundError(f\"Melody data file NOT FOUND: {melody_data_p}\")\n",
    "\n",
    "        logging.info(f\"Initializing dataset using key 'event_ids'...\")\n",
    "        dataset = MelodyDataset(config.melody_data_path, melody_vocab, config)\n",
    "        total_size = len(dataset)\n",
    "        if total_size == 0: raise ValueError(\"Dataset is empty after initialization.\")\n",
    "\n",
    "        logging.info(\"Checking maximum IDs in the loaded dataset...\")\n",
    "        max_melody_id_found = 0\n",
    "        max_chord_id_found = -1\n",
    "        chord_embeddings_enabled = (config.chord_emb_dim is not None and config.chord_emb_dim > 0)\n",
    "        logging.info(f\"Chord embeddings configured: {chord_embeddings_enabled}\")\n",
    "\n",
    "        for sample in tqdm(dataset.samples, desc=\"Scanning IDs\", unit=\"samples\"):\n",
    "            event_ids_list = sample.get(\"event_ids\")\n",
    "            if event_ids_list:\n",
    "                 try:\n",
    "                     current_max = max(event_ids_list) if event_ids_list else -1\n",
    "                     max_melody_id_found = max(max_melody_id_found, current_max)\n",
    "                 except (ValueError, TypeError) as e:\n",
    "                     logging.debug(f\"Could not find max in event_ids for a sample: {e}.\")\n",
    "                     continue\n",
    "            if chord_embeddings_enabled:\n",
    "                chord_ids_list = sample.get(\"conditioning_chord_ids\")\n",
    "                if chord_ids_list:\n",
    "                    try:\n",
    "                         current_max_chord = max(chord_ids_list) if chord_ids_list else -1\n",
    "                         max_chord_id_found = max(max_chord_id_found, current_max_chord)\n",
    "                    except (ValueError, TypeError) as e:\n",
    "                         logging.debug(f\"Could not find max in conditioning_chord_ids: {e}.\")\n",
    "                         continue\n",
    "\n",
    "        logging.info(f\"Max Melody ID found in data: {max_melody_id_found}\")\n",
    "        if chord_embeddings_enabled:\n",
    "            logging.info(f\"Max Chord ID found in data: {max_chord_id_found}\")\n",
    "\n",
    "        required_melody_vocab_size = max_melody_id_found + 1\n",
    "        if config.melody_vocab_size < required_melody_vocab_size:\n",
    "            logging.warning(f\"Melody vocab size from file ({config.melody_vocab_size}) is smaller than required ({required_melody_vocab_size}). Adjusting config.melody_vocab_size.\")\n",
    "            config.melody_vocab_size = required_melody_vocab_size\n",
    "        else:\n",
    "            logging.info(f\"Melody vocab size {config.melody_vocab_size} is sufficient.\")\n",
    "\n",
    "        if chord_embeddings_enabled:\n",
    "            required_chord_vocab_size = max_chord_id_found + 1\n",
    "            if config.chord_vocab_size < required_chord_vocab_size:\n",
    "                logging.warning(f\"Chord vocab size from file ({config.chord_vocab_size}) is smaller than required ({required_chord_vocab_size}). Adjusting config.chord_vocab_size.\")\n",
    "                config.chord_vocab_size = required_chord_vocab_size\n",
    "            else:\n",
    "                 logging.info(f\"Chord vocab size {config.chord_vocab_size} is sufficient.\")\n",
    "        elif config.chord_vocab_size == 0 and chord_vocab is not None:\n",
    "             config.chord_vocab_size = len(chord_vocab)\n",
    "             logging.info(f\"Chord embeddings not enabled. Set chord_vocab_size to {config.chord_vocab_size} from file.\")\n",
    "        else:\n",
    "             if config.chord_vocab_size <= 0: config.chord_vocab_size = 1\n",
    "             logging.info(f\"Chord embeddings not enabled. Chord vocab size set to {config.chord_vocab_size}.\")\n",
    "\n",
    "        if config.melody_pad_token_id < 0 or config.melody_pad_token_id >= config.melody_vocab_size:\n",
    "             raise ValueError(f\"Melody pad token ID {config.melody_pad_token_id} is out of bounds for final vocab size {config.melody_vocab_size}\")\n",
    "        if chord_embeddings_enabled:\n",
    "             if config.chord_pad_token_id < 0 or config.chord_pad_token_id >= config.chord_vocab_size:\n",
    "                 raise ValueError(f\"Chord pad token ID {config.chord_pad_token_id} is out of bounds for final chord vocab size {config.chord_vocab_size}\")\n",
    "\n",
    "        print(\"\\n--- Final Configuration ---\")\n",
    "        print(f\"  Adjusted Melody Vocab Size: {config.melody_vocab_size}\")\n",
    "        print(f\"  Adjusted Chord Vocab Size: {config.chord_vocab_size}\")\n",
    "        print(f\"  Melody Pad ID: {config.melody_pad_token_id}\")\n",
    "        print(f\"  Chord Pad ID: {config.chord_pad_token_id}\")\n",
    "        print(f\"  Max Sequence Length: {config.max_seq_len}\")\n",
    "        print(f\"  Batch Size: {config.batch_size}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        logging.info(\"Splitting dataset...\")\n",
    "        val_size = int(config.val_split * total_size)\n",
    "        test_size = int(config.test_split * total_size)\n",
    "        val_size = max(0, val_size)\n",
    "        test_size = max(0, test_size)\n",
    "\n",
    "        if total_size < 3 and (val_size > 0 or test_size > 0):\n",
    "            logging.warning(f\"Dataset size ({total_size}) too small for validation/test split. Using all data for training.\")\n",
    "            train_size = total_size; val_size = 0; test_size = 0\n",
    "        elif total_size - val_size - test_size <= 0:\n",
    "             logging.warning(f\"Train split size calculated as non-positive. Adjusting splits.\")\n",
    "             if total_size - val_size > 0:\n",
    "                 test_size = 0; train_size = total_size - val_size\n",
    "             else:\n",
    "                 val_size = 0; test_size = 0; train_size = total_size\n",
    "             logging.warning(f\"Adjusted split sizes: Train={train_size}, Val={val_size}, Test={test_size}\")\n",
    "        else:\n",
    "             train_size = total_size - val_size - test_size\n",
    "\n",
    "        logging.info(f\"Splitting into: Train={train_size}, Val={val_size}, Test={test_size}\")\n",
    "\n",
    "        if train_size + val_size + test_size != total_size:\n",
    "             train_size = total_size - val_size - test_size\n",
    "             logging.warning(f\"Corrected train_size due to rounding: {train_size}\")\n",
    "\n",
    "        if train_size <= 0 and config.num_epochs > 0:\n",
    "            logging.error(\"No training samples available after split, but num_epochs > 0. Exiting.\")\n",
    "            sys.exit(1)\n",
    "        if val_size == 0: logging.warning(\"No validation samples after split.\")\n",
    "        if test_size == 0: logging.warning(\"No test samples after split.\")\n",
    "\n",
    "        if total_size > 0 and (train_size >= 0 and val_size >= 0 and test_size >= 0):\n",
    "             train_set, val_set, test_set = random_split(\n",
    "                 dataset, [train_size, val_size, test_size],\n",
    "                 generator=torch.Generator().manual_seed(config.seed) )\n",
    "             logging.info(\"Dataset split successfully.\")\n",
    "        elif total_size == 0:\n",
    "             logging.warning(\"Dataset is empty, cannot split.\")\n",
    "             train_set, val_set, test_set = [], [], []\n",
    "        else:\n",
    "             logging.error(f\"Invalid split sizes calculated (Train: {train_size}, Val: {val_size}, Test: {test_size}). Exiting.\")\n",
    "             sys.exit(1)\n",
    "\n",
    "    except FileNotFoundError as e: logging.critical(f\"{e}\"); sys.exit(1)\n",
    "    except ValueError as e: logging.critical(f\"Error during dataset loading/checking/splitting: {e}\", exc_info=True); sys.exit(1)\n",
    "    except Exception as e: logging.critical(f\"Error creating or splitting dataset: {e}\", exc_info=True); sys.exit(1)\n",
    "\n",
    "    # --- Create DataLoaders ---\n",
    "    print(\"\\n--- Creating DataLoaders ---\")\n",
    "    pin_memory = device.type == 'cuda'\n",
    "    persistent_workers = config.num_dataload_workers > 0 and pin_memory\n",
    "    collate_wrapper = lambda batch: melody_collate_fn(batch, config.melody_pad_token_id, config.chord_pad_token_id)\n",
    "\n",
    "    loader_args = {'batch_size': config.batch_size,\n",
    "                   'collate_fn': collate_wrapper,\n",
    "                   'num_workers': config.num_dataload_workers,\n",
    "                   'pin_memory': pin_memory,\n",
    "                   'persistent_workers': persistent_workers if config.num_dataload_workers > 0 else False,\n",
    "                   'prefetch_factor': 2 if config.num_dataload_workers > 0 else None,\n",
    "                   'timeout': 120 if config.num_dataload_workers > 0 else 0\n",
    "                   }\n",
    "    if config.num_dataload_workers == 0:\n",
    "         loader_args.pop('prefetch_factor', None)\n",
    "         loader_args.pop('persistent_workers', None)\n",
    "         loader_args.pop('timeout', None)\n",
    "\n",
    "    try:\n",
    "        if train_set and len(train_set) > 0:\n",
    "            train_loader = DataLoader(train_set, shuffle=True, drop_last=True, **loader_args)\n",
    "            logging.info(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "        else:\n",
    "            logging.warning(\"Train set is empty or None. Train loader not created.\")\n",
    "\n",
    "        if val_set and len(val_set) > 0:\n",
    "             val_loader = DataLoader(val_set, shuffle=False, drop_last=False, **loader_args)\n",
    "             logging.info(f\"Validation loader created with {len(val_loader)} batches.\")\n",
    "        else:\n",
    "             logging.info(\"Validation set is empty or None. Validation loader not created.\")\n",
    "\n",
    "        if test_set and len(test_set) > 0:\n",
    "            test_loader = DataLoader(test_set, shuffle=False, drop_last=False, **loader_args)\n",
    "            logging.info(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "        else:\n",
    "            logging.info(\"Test set is empty or None. Test loader not created.\")\n",
    "\n",
    "        if not train_loader and config.num_epochs > 0:\n",
    "            logging.error(\"Training requested (num_epochs > 0) but no training data available/loader created.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Error creating DataLoaders: {e}\", exc_info=True)\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # --- Initialize Model, Optimizer, Loss, Scaler ---\n",
    "    print(\"\\n--- Initializing Training Components ---\")\n",
    "    try:\n",
    "        model = MelodyTransformerXL(config).to(device) # Keep model in default dtype (float32)\n",
    "\n",
    "        # <<< REMOVED EXPLICIT .to(dtype=torch.float16) >>>\n",
    "        # if config.amp_dtype == torch.float16 and device.type == 'cuda':\n",
    "        #     # model = model.to(dtype=torch.float16) # Don't do this when using autocast+GradScaler\n",
    "        #     logging.info(\"Model kept in float32, autocast will handle fp16.\")\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=config.melody_pad_token_id)\n",
    "        amp_enabled = (config.amp_dtype == torch.float16 and device.type == 'cuda')\n",
    "        scaler = GradScaler(enabled=amp_enabled) # Uses default cuda device if available\n",
    "        logging.info(f\"AMP Enabled: {scaler.is_enabled()}\")\n",
    "\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        logging.info(f\"Model Total Trainable Params: {num_params / 1e6:.2f} M\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        logging.critical(f\"Configuration error during model initialization: {e}\", exc_info=True); sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Error initializing model/optimizer/criterion: {e}\", exc_info=True); sys.exit(1)\n",
    "\n",
    "    # --- Load Checkpoint ---\n",
    "    start_epoch, best_val_loss = load_checkpoint(latest_checkpoint_path, model, optimizer, scaler, device)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    print(f\"\\n--- Starting Training from Epoch {start_epoch + 1} / {config.num_epochs} ---\")\n",
    "    if config.num_epochs <= start_epoch:\n",
    "         logging.info(f\"Target epochs ({config.num_epochs}) already reached by checkpoint (next epoch would be {start_epoch + 1}). Skipping training loop.\")\n",
    "    elif not train_loader:\n",
    "         logging.warning(\"No training data loader available. Skipping training loop.\")\n",
    "    else:\n",
    "        for epoch in range(start_epoch, config.num_epochs):\n",
    "            epoch_num = epoch + 1\n",
    "            print(f\"\\n===== Epoch {epoch_num}/{config.num_epochs} =====\")\n",
    "\n",
    "            # --- Train ---\n",
    "            try:\n",
    "                 train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, scaler, epoch_num, config, device)\n",
    "                 gc.collect()\n",
    "                 if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Critical Error during training epoch {epoch_num}: {e}\", exc_info=True)\n",
    "                 print(f\"Epoch {epoch_num} failed critically during training. Stopping.\")\n",
    "                 sys.exit(1)\n",
    "\n",
    "            # --- Validate ---\n",
    "            current_val_loss = float('inf')\n",
    "            val_acc = 0.0\n",
    "            if val_loader and len(val_loader) > 0:\n",
    "                try:\n",
    "                    val_metrics = evaluate_epoch(model, val_loader, criterion, config, device)\n",
    "                    if isinstance(val_metrics, dict) and \"loss\" in val_metrics and torch.isfinite(torch.tensor(val_metrics[\"loss\"])):\n",
    "                         current_val_loss = val_metrics[\"loss\"]\n",
    "                         val_acc = val_metrics.get(\"accuracy\", 0.0)\n",
    "                    else:\n",
    "                         logging.warning(f\"Epoch {epoch_num} Validation did not return valid results. Treating as high loss.\")\n",
    "                         current_val_loss = float('inf')\n",
    "                except Exception as e:\n",
    "                     logging.error(f\"Error during validation epoch {epoch_num}: {e}\", exc_info=True)\n",
    "                     print(f\"Validation for epoch {epoch_num} failed. Treating as high loss.\")\n",
    "                     current_val_loss = float('inf')\n",
    "                gc.collect()\n",
    "                if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "\n",
    "                # --- Save Best Model ---\n",
    "                if torch.isfinite(torch.tensor(current_val_loss)) and current_val_loss < best_val_loss:\n",
    "                    logging.info(f\"** Validation Loss Improved ({best_val_loss:.4f} -> {current_val_loss:.4f}). Saving best model... **\")\n",
    "                    print(f\"** Validation Loss Improved ({best_val_loss:.4f} -> {current_val_loss:.4f}). Saving best model... **\")\n",
    "                    best_val_loss = current_val_loss\n",
    "                    model_state_to_save = model.module.state_dict() if isinstance(model, (nn.DataParallel, nn.parallel.DistributedDataParallel)) else model.state_dict()\n",
    "                    save_checkpoint({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model_state_to_save,\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scaler_state_dict': scaler.state_dict() if scaler is not None and scaler.is_enabled() else None,\n",
    "                        'best_val_loss': best_val_loss,\n",
    "                        'config': config.__dict__\n",
    "                    }, best_model_path)\n",
    "                else:\n",
    "                    logging.info(f\"Validation loss ({current_val_loss:.4f}) did not improve from best ({best_val_loss:.4f}).\")\n",
    "            else:\n",
    "                logging.info(\"Skipping validation (no validation data/loader).\")\n",
    "\n",
    "            # --- Save Latest Checkpoint ---\n",
    "            logging.info(\"Saving latest checkpoint...\")\n",
    "            model_state_to_save = model.module.state_dict() if isinstance(model, (nn.DataParallel, nn.parallel.DistributedDataParallel)) else model.state_dict()\n",
    "            scaler_state = scaler.state_dict() if scaler is not None and scaler.is_enabled() else None\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model_state_to_save,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler_state,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'config': config.__dict__\n",
    "            }, latest_checkpoint_path)\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "\n",
    "    # --- Final Test Evaluation ---\n",
    "    print(\"\\n--- Final Evaluation on Test Set ---\")\n",
    "    if test_loader and len(test_loader) > 0:\n",
    "        logging.info(\"Loading best model weights for final evaluation...\")\n",
    "        try:\n",
    "             final_model = MelodyTransformerXL(config).to(device)\n",
    "             # No explicit dtype conversion needed here either if using autocast for eval\n",
    "             logging.info(\"Model for final evaluation initialized.\")\n",
    "        except Exception as e:\n",
    "             logging.critical(f\"Failed to initialize model for final evaluation: {e}\", exc_info=True)\n",
    "             sys.exit(1)\n",
    "\n",
    "        if os.path.exists(best_model_path):\n",
    "            try:\n",
    "                logging.info(f\"Loading best model state from: {best_model_path}\")\n",
    "                checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
    "                state_dict = checkpoint.get('model_state_dict')\n",
    "\n",
    "                if state_dict:\n",
    "                    is_parallel_ckpt = all(key.startswith('module.') for key in state_dict)\n",
    "                    is_current_parallel = isinstance(final_model, (nn.DataParallel, nn.parallel.DistributedDataParallel))\n",
    "                    if is_parallel_ckpt and not is_current_parallel:\n",
    "                        logging.info(\"Removing 'module.' prefix from best model state_dict for testing.\")\n",
    "                        state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}\n",
    "                    elif not is_parallel_ckpt and is_current_parallel:\n",
    "                         logging.info(\"Adding 'module.' prefix to best model state_dict for testing.\")\n",
    "                         state_dict = {'module.' + k: v for k, v in state_dict.items()}\n",
    "\n",
    "                    missing, unexpected = final_model.load_state_dict(state_dict, strict=False)\n",
    "                    if missing: logging.warning(f\"Final Test: Best model loaded with missing keys: {missing}\")\n",
    "                    if unexpected: logging.warning(f\"Final Test: Best model loaded with unexpected keys: {unexpected}\")\n",
    "                    loaded_epoch = checkpoint.get('epoch', -1) + 1\n",
    "                    loaded_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "                    logging.info(f\"Best model state (from epoch {loaded_epoch}, val_loss {loaded_val_loss:.4f}) loaded successfully for testing.\")\n",
    "\n",
    "                    try:\n",
    "                         test_metrics = evaluate_epoch(final_model, test_loader, criterion, config, device)\n",
    "                         print(\"\\n--- Test Set Results (using BEST saved model) ---\")\n",
    "                         if isinstance(test_metrics, dict):\n",
    "                             print(f\" Loss:     {test_metrics.get('loss', float('nan')):.4f}\")\n",
    "                             print(f\" Accuracy: {test_metrics.get('accuracy', float('nan')):.4f}\")\n",
    "                             print(f\" SSMD:     {test_metrics.get('ssmd', float('nan')):.4f}\")\n",
    "                             print(f\" GS:       {test_metrics.get('gs', float('nan')):.4f}\")\n",
    "                             print(f\" Eval Seqs:{test_metrics.get('count', 0)}\")\n",
    "                             print(f\" Eval Toks:{int(test_metrics.get('tokens', 0))}\")\n",
    "                         else: print(\"Test evaluation failed or produced no metrics.\")\n",
    "                    except Exception as e:\n",
    "                         logging.error(f\"Error during final test evaluation run: {e}\", exc_info=True)\n",
    "                         print(f\"!!! Final test evaluation run failed: {e}\")\n",
    "\n",
    "                else:\n",
    "                    logging.error(f\"Best model checkpoint '{best_model_path}' did not contain 'model_state_dict'. Cannot perform final test on best model.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Could not load or evaluate best model from {best_model_path}: {e}. Skipping final test.\", exc_info=True)\n",
    "                print(f\"!!! Failed to load or test best model: {e}\")\n",
    "        else:\n",
    "            logging.warning(f\"Best model checkpoint '{best_model_path}' not found. Skipping final test evaluation on the best model.\")\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "    else:\n",
    "        print(\"Skipping test evaluation (no test data loader or test split is empty).\")\n",
    "\n",
    "    main_end_time = time.time()\n",
    "    total_runtime = main_end_time - main_start_time\n",
    "    logging.info(f\"Script finished. Total Runtime: {total_runtime // 3600:.0f}h {(total_runtime % 3600) // 60:.0f}m {total_runtime % 60:.2f}s\")\n",
    "    print(f\"Output files potentially saved in: {config.output_dir}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d549f9af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:08:54.866247Z",
     "iopub.status.busy": "2025-04-25T12:08:54.865817Z",
     "iopub.status.idle": "2025-04-25T12:09:01.514184Z",
     "shell.execute_reply": "2025-04-25T12:09:01.513276Z"
    },
    "papermill": {
     "duration": 7.480618,
     "end_time": "2025-04-25T12:09:01.515714",
     "exception": false,
     "start_time": "2025-04-25T12:08:54.035096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pretty_midi\r\n",
      "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.26.4)\r\n",
      "Collecting mido>=1.1.16 (from pretty_midi)\r\n",
      "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido>=1.1.16->pretty_midi) (24.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7.0->pretty_midi) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7.0->pretty_midi) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7.0->pretty_midi) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7.0->pretty_midi) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7.0->pretty_midi) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7.0->pretty_midi) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7.0->pretty_midi) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7.0->pretty_midi) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.7.0->pretty_midi) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.7.0->pretty_midi) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.7.0->pretty_midi) (2024.2.0)\r\n",
      "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: pretty_midi\r\n",
      "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592287 sha256=b2f2a2a7a5f8cf5000fba60c4ff70ebdd307b2696e81c349991be7d05f664f5d\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/e6/95/ac/15ceaeb2823b04d8e638fd1495357adb8d26c00ccac9d7782e\r\n",
      "Successfully built pretty_midi\r\n",
      "Installing collected packages: mido, pretty_midi\r\n",
      "Successfully installed mido-1.3.3 pretty_midi-0.2.10\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9431b164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:09:03.088919Z",
     "iopub.status.busy": "2025-04-25T12:09:03.088595Z",
     "iopub.status.idle": "2025-04-25T12:09:10.220864Z",
     "shell.execute_reply": "2025-04-25T12:09:10.220051Z"
    },
    "papermill": {
     "duration": 7.95475,
     "end_time": "2025-04-25T12:09:10.222077",
     "exception": false,
     "start_time": "2025-04-25T12:09:02.267327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (Tesla P100-PCIE-16GB)\n",
      "\n",
      "--- Loading Vocabularies ---\n",
      "Melody Vocab Size: 306, Chord Vocab Size: 44733\n",
      "Melody Pad ID: 0, Chord Pad ID: 0, EOS ID: N/A\n",
      "\n",
      "--- Loading Model Checkpoint ---\n",
      "\n",
      "--- Final Configuration Used for Model ---\n",
      "  melody_vocab_size: 306\n",
      "  chord_vocab_size: 44734\n",
      "  melody_pad_token_id: 0\n",
      "  chord_pad_token_id: 0\n",
      "  n_layer: 8\n",
      "  d_model: 512\n",
      "  n_head: 8\n",
      "  d_head: 64\n",
      "  mem_len: 256\n",
      "  amp_dtype: torch.float16\n",
      "\n",
      "Model loaded successfully.\n",
      "Model parameter dtype: torch.float32\n",
      "\n",
      "--- Preparing Full Chord Progression ---\n",
      "Using chord progression of length: 512 tokens\n",
      "Using start melody token: 'NOTE_ON_60' (ID: 138)\n",
      "\n",
      "--- Starting Melody Generation Over Progression ---\n",
      "Progression length: 512\n",
      "Temperature: 0.75, Top-K: 40, Top-P: 0.9\n",
      "Using AMP for generation with dtype: torch.float16\n",
      "Starting generation for 511 steps over the provided progression (Total output length: 512)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Melody: 100%|██████████| 511/511 [00:06<00:00, 84.37token/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation Complete! Took 6.06 seconds.\n",
      "Total sequence length generated: 512\n",
      "\n",
      "--- Decoding Generated Sequence ---\n",
      "First 100 RAW decoded tokens (including start token):\n",
      "['NOTE_ON_60', 'NOTE_ON_69', 'NOTE_ON_81', 'TIME_SHIFT_2', 'NOTE_ON_67', 'TIME_SHIFT_8', 'NOTE_ON_64', 'NOTE_ON_76', 'TIME_SHIFT_2', 'NOTE_ON_64', 'NOTE_ON_64', 'TIME_SHIFT_2', 'NOTE_ON_71', 'NOTE_OFF_71', 'TIME_SHIFT_8', 'NOTE_ON_63', 'NOTE_ON_68', 'NOTE_ON_68', 'NOTE_OFF_73', 'TIME_SHIFT_4', 'NOTE_ON_43', 'NOTE_OFF_43', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128', 'TIME_SHIFT_128']\n",
      "\n",
      "--- Starting Post-processing (Limiting Polyphony to 6) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing finished. Skipped 5 NOTE_ON events.\n",
      "Original generated length: 512, Processed length: 507\n",
      "\n",
      "--- Saving Outputs ---\n",
      "Output base name: Melody_1\n",
      "Generated sequence data (JSON) saved to: /kaggle/working/generated_output/Melody_1.json\n",
      "\n",
      "--- Converting PROCESSED Sequence to MIDI ---\n",
      "Starting MIDI conversion for 507 events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "MIDI Conversion Summary:\n",
      "  MIDI file successfully written to: /kaggle/working/generated_output/Melody_1.mid\n",
      "  Notes created: 7 (Added: 7)\n",
      "  Total problematic events skipped/handled: 1\n",
      "    - Zero/Negative duration notes skipped: 1 (During seq: 1, At end: 0)\n",
      "------------------------------\n",
      "\n",
      "Script finished.\n",
      "Total Generation Script Runtime: 6.97 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPLETE Generation Script: Melody Transformer-XL with RoPE & Conditioning\n",
    "# Version: Fixed _init_weights, Improved Polyphony Filter, Progression Mode, Seq Naming\n",
    "#          *** USER ACTION REQUIRED: Provide full chord progression data ***\n",
    "# =============================================================================\n",
    "# Script generated around: Friday, April 25, 2025 at 3:47 PM IST (Bhopal, India time)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sys\n",
    "import traceback\n",
    "import gc # Garbage Collector\n",
    "from typing import Optional, List, Dict, Tuple, Any, Set\n",
    "from dataclasses import dataclass, field, fields # Added fields\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm # Use standard tqdm\n",
    "import collections # Needed for defaultdict in collate_fn\n",
    "import logging\n",
    "import re # Needed for sequential filename logic\n",
    "\n",
    "# --- Install and Import PrettyMIDI ---\n",
    "try:\n",
    "    import pretty_midi\n",
    "except ImportError:\n",
    "    print(\"Installing pretty_midi...\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pretty_midi\"])\n",
    "        import pretty_midi\n",
    "        print(\"pretty_midi installed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to install pretty_midi automatically: {e}\")\n",
    "        print(\"Please install it manually (e.g., 'pip install pretty_midi') and restart the script.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "# --- CUDA specific imports ---\n",
    "from torch.cuda.amp import GradScaler\n",
    "import torch.amp\n",
    "\n",
    "# For timestamp and location context\n",
    "try:\n",
    "    import pytz\n",
    "except ImportError:\n",
    "     print(\"Installing pytz...\")\n",
    "     try:\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\"])\n",
    "        import pytz\n",
    "        print(\"pytz installed successfully.\")\n",
    "     except Exception as e:\n",
    "        print(f\"Failed to install pytz automatically: {e}\")\n",
    "        print(\"Please install it manually (e.g., 'pip install pytz') and restart the script.\")\n",
    "        pytz = None # Continue without timezone awareness\n",
    "import datetime\n",
    "\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# !! NOTE: All necessary class definitions are included in this script.     !!\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# --- Define Vocabulary Constants ---\n",
    "MELODY_PAD_TOKEN: str = \"<PAD>\"\n",
    "MELODY_UNK_TOKEN: str = \"<UNK>\"\n",
    "CHORD_VOCAB_FILENAME: str = \"chord_progression_vocab.json\"\n",
    "CHORD_PAD_TOKEN: str = \"<PAD>\" # Assuming pad token for chords is also <PAD>\n",
    "\n",
    "# --- TrainingConfig Definition ---\n",
    "# Note: This config is loaded from the checkpoint, but defaults are here for reference\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # --- Paths ---\n",
    "    midi_root_dir: str = \"LOCAL_PATH_IGNORE\"\n",
    "    chord_data_dir: str = \"/kaggle/input/advance-h-rpe\" # Example Path\n",
    "    melody_data_path: str = \"/kaggle/input/new-melody-model-new-approach-1/training_data.jsonl\" # Example Path\n",
    "    melody_vocab_path: str = \"/kaggle/input/new-melody-model-new-approach-1/event_vocab.json\" # Example Path\n",
    "    output_dir: str = \"/kaggle/working/melody_model_output\" # Example Path\n",
    "\n",
    "    # --- Vocab Sizes & Padding ---\n",
    "    melody_vocab_size: int = 0\n",
    "    chord_vocab_size: int = 0\n",
    "    melody_pad_token_id: int = 0\n",
    "    chord_pad_token_id: int = 0\n",
    "\n",
    "    # --- Model Architecture ---\n",
    "    n_layer: int = 8\n",
    "    d_model: int = 512\n",
    "    n_head: int = 8\n",
    "    d_head: int = 64 # Will be calculated if d_model % n_head == 0\n",
    "    d_inner: int = 2048\n",
    "    dropout: float = 0.1\n",
    "    mem_len: int = 256\n",
    "    rope_theta: float = 10000.0\n",
    "    num_chord_features: int = 3\n",
    "    condition_proj_dim: int = 128\n",
    "    chord_emb_dim: Optional[int] = 64\n",
    "\n",
    "    # --- Training ---\n",
    "    batch_size: int = 16\n",
    "    num_epochs: int = 50\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 0.01\n",
    "    grad_clip_value: float = 1.0\n",
    "    seed: int = 42\n",
    "\n",
    "    # --- Data Loading ---\n",
    "    max_seq_len: Optional[int] = 512\n",
    "    train_split: float = 0.90\n",
    "    val_split: float = 0.05\n",
    "    test_split: float = 0.05\n",
    "    num_dataload_workers: int = 2\n",
    "\n",
    "    # --- Runtime ---\n",
    "    amp_dtype: Optional[torch.dtype] = torch.float16\n",
    "\n",
    "    # --- Make the class dict serializable ---\n",
    "    def as_dict(self):\n",
    "        d = {}\n",
    "        for f in fields(self):\n",
    "            value = getattr(self, f.name)\n",
    "            if isinstance(value, torch.dtype):\n",
    "                d[f.name] = str(value) # Convert torch.dtype to string\n",
    "            elif isinstance(value, Path):\n",
    "                d[f.name] = str(value) # Convert Path to string\n",
    "            else:\n",
    "                try:\n",
    "                    json.dumps({f.name: value})\n",
    "                    d[f.name] = value\n",
    "                except TypeError:\n",
    "                    logging.warning(f\"Could not serialize field '{f.name}' of type {type(value)}. Storing its string representation.\")\n",
    "                    d[f.name] = str(value) # Fallback to string representation\n",
    "        return d\n",
    "\n",
    "# --- Model Class Definitions ---\n",
    "\n",
    "# === RoPE Implementation ===\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    # x: [seq_len, bsz, n_head, d_head]\n",
    "    # cos, sin: [seq_len, 1, 1, d_head] or [seq_len, d_head] -> unsqueezed later\n",
    "    if cos.dim() == 2: # Handle case where cos/sin are [seq_len, d_head]\n",
    "         cos = cos.unsqueeze(1).unsqueeze(2)\n",
    "         sin = sin.unsqueeze(1).unsqueeze(2)\n",
    "    elif cos.dim() != 4 or cos.shape[1] != 1 or cos.shape[2] != 1:\n",
    "         raise ValueError(f\"Unexpected shape for RoPE cos/sin: {cos.shape}\")\n",
    "\n",
    "    x_embed = (x * cos) + (rotate_half(x) * sin)\n",
    "    return x_embed\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=4096, base=10000.0, device=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=device, dtype=torch.float32) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.max_seq_len_cached = -1\n",
    "        self.register_buffer(\"cos_cached\", None, persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", None, persistent=False)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        # Optimization: Only update cache if needed\n",
    "        if seq_len <= self.max_seq_len_cached and \\\n",
    "           self.cos_cached is not None and self.sin_cached is not None and \\\n",
    "           self.cos_cached.device == device and self.cos_cached.dtype == dtype:\n",
    "              return\n",
    "        # Increase cache size adaptively but ensure it covers max_position_embeddings\n",
    "        new_cache_len = max(seq_len, self.max_position_embeddings) # Use max_position_embeddings as a lower bound\n",
    "        self.max_seq_len_cached = new_cache_len # Update cached length *after* calculation\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.cos_cached = emb.cos().to(dtype).detach() # Ensure correct dtype\n",
    "        self.sin_cached = emb.sin().to(dtype).detach() # Ensure correct dtype\n",
    "        logging.debug(f\"RoPE cache updated: seq_len={self.max_seq_len_cached}, device={device}, dtype={dtype}\")\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int, start_pos: int = 0):\n",
    "        device = x.device\n",
    "        dtype = x.dtype\n",
    "        required_len = start_pos + seq_len # Total length needed from cache\n",
    "\n",
    "        # Check if cache needs update (size, device, or dtype mismatch)\n",
    "        if required_len > self.max_seq_len_cached or \\\n",
    "           self.cos_cached is None or self.sin_cached is None or \\\n",
    "           self.cos_cached.device != device or self.cos_cached.dtype != dtype:\n",
    "            # Determine new cache size - should be at least required_len and max_position_embeddings\n",
    "            new_max_len = max(self.max_position_embeddings, required_len)\n",
    "            self._set_cos_sin_cache(seq_len=new_max_len, device=device, dtype=dtype)\n",
    "\n",
    "        # Calculate end position for slicing\n",
    "        end_pos = start_pos + seq_len\n",
    "\n",
    "        # Slice the cache - ensure indices are within bounds\n",
    "        start_pos_clamped = max(0, min(start_pos, self.max_seq_len_cached - 1))\n",
    "        end_pos_clamped = max(0, min(end_pos, self.max_seq_len_cached))\n",
    "\n",
    "        # Handle potential empty slice after clamping\n",
    "        if start_pos_clamped >= end_pos_clamped:\n",
    "             logging.warning(f\"RoPE: start_pos {start_pos} >= end_pos {end_pos} (or became so after clamping). Returning empty tensors.\")\n",
    "             return torch.empty((0, self.dim), device=device, dtype=dtype), torch.empty((0, self.dim), device=device, dtype=dtype)\n",
    "\n",
    "        cos = self.cos_cached[start_pos_clamped : end_pos_clamped]\n",
    "        sin = self.sin_cached[start_pos_clamped : end_pos_clamped]\n",
    "\n",
    "        # Verify the sliced length matches the expected seq_len\n",
    "        if cos.shape[0] != seq_len:\n",
    "            logging.warning(f\"RoPE: Sliced length {cos.shape[0]} does not match expected seq_len {seq_len}. \"\n",
    "                            f\"start_pos={start_pos}, end_pos={end_pos}, \"\n",
    "                            f\"clamped=[{start_pos_clamped}:{end_pos_clamped}], cache_len={self.max_seq_len_cached}. \"\n",
    "                            f\"This might indicate an issue with position calculation.\")\n",
    "            # Attempt to adjust if possible, otherwise could lead to errors later\n",
    "            if cos.shape[0] < seq_len:\n",
    "                 logging.error(f\"RoPE: Sliced cache is too short ({cos.shape[0]} vs {seq_len}). Cannot proceed safely.\")\n",
    "                 raise IndexError(\"RoPE cache slicing resulted in tensor shorter than expected seq_len.\")\n",
    "            else: # Truncate if too long\n",
    "                 cos = cos[:seq_len]\n",
    "                 sin = sin[:seq_len]\n",
    "                 logging.warning(f\"RoPE: Truncated sliced cache to match seq_len {seq_len}.\")\n",
    "\n",
    "\n",
    "        return cos, sin\n",
    "\n",
    "\n",
    "# === Model Components ===\n",
    "class RelPartialLearnableMultiHeadAttn(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_head, dropout, config: TrainingConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_head\n",
    "        self.dropout = dropout\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head, bias=False)\n",
    "        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Calculate max positions needed based on mem_len and max_seq_len\n",
    "        # Use a reasonable upper bound if max_seq_len is None during generation setup\n",
    "        gen_max_seq_len = config.max_seq_len if config.max_seq_len is not None else 2048 # Default large if None\n",
    "        max_positions = config.mem_len + gen_max_seq_len\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(\n",
    "            dim=self.d_head,\n",
    "            max_position_embeddings=max_positions,\n",
    "            base=config.rope_theta\n",
    "        )\n",
    "        self.scale = 1.0 / (d_head ** 0.5)\n",
    "\n",
    "    def forward(self, w: torch.Tensor, mems: Optional[torch.Tensor], attn_mask: Optional[torch.Tensor] = None, head_mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        qlen, bsz, d_model_in = w.size() # w: (qlen, bsz, d_model)\n",
    "        if d_model_in != self.d_model:\n",
    "            raise ValueError(f\"Layer {self.layer_idx}: Input tensor d_model mismatch. Expected {self.d_model}, got {d_model_in}\")\n",
    "\n",
    "        mlen = mems.size(0) if mems is not None and mems.dim() == 3 and mems.shape[0] > 0 else 0\n",
    "        klen = mlen + qlen # Total length of key/value sequence (memory + current)\n",
    "\n",
    "        # Validate memory shape if provided\n",
    "        if mlen > 0:\n",
    "            if mems.shape[1] != bsz or mems.shape[2] != self.d_model:\n",
    "                logging.warning(f\"Layer {self.layer_idx}: Mem shape {mems.shape} incompatible with Input shape {w.shape}. Discarding memory.\")\n",
    "                cat = w # Discard memory\n",
    "                mlen = 0\n",
    "                klen = qlen\n",
    "            else:\n",
    "                cat = torch.cat([mems, w], dim=0) # Prepend memory: (klen, bsz, d_model)\n",
    "        else:\n",
    "            cat = w # No memory: (qlen, bsz, d_model)\n",
    "\n",
    "        # --- QKV Projection ---\n",
    "        # Project concatenated memory and input\n",
    "        w_heads = self.qkv_net(cat) # (klen, bsz, 3 * n_head * d_head)\n",
    "        w_heads = w_heads.view(klen, bsz, self.n_head, 3 * self.d_head)\n",
    "        q_head_raw, k_head_raw, v_head = torch.chunk(w_heads, 3, dim=-1) # Each: (klen, bsz, n_head, d_head)\n",
    "\n",
    "        # --- Separate Query, Key, Value ---\n",
    "        # Query comes only from the current input part\n",
    "        q_head = q_head_raw[-qlen:] # (qlen, bsz, n_head, d_head)\n",
    "        # Key and Value use the full concatenated sequence (memory + input)\n",
    "        k_head = k_head_raw # (klen, bsz, n_head, d_head)\n",
    "        # v_head already has the correct shape: (klen, bsz, n_head, d_head)\n",
    "\n",
    "        # --- Apply Rotary Positional Embeddings (RoPE) ---\n",
    "        # Calculate RoPE embeddings for the full key length (klen) starting from position 0\n",
    "        cos_k, sin_k = self.rotary_emb(k_head, seq_len=klen, start_pos=0) # cos/sin: [klen, d_head]\n",
    "\n",
    "        # Calculate RoPE embeddings for the query length (qlen) starting from the memory length (mlen)\n",
    "        cos_q, sin_q = self.rotary_emb(q_head, seq_len=qlen, start_pos=mlen) # cos/sin: [qlen, d_head]\n",
    "\n",
    "\n",
    "        # Check for shape mismatches before applying RoPE\n",
    "        if cos_q.shape[0] != qlen or sin_q.shape[0] != qlen:\n",
    "            logging.error(f\"Layer {self.layer_idx}: RoPE query cos/sin shape mismatch! Expected {qlen}, got {cos_q.shape[0]}. Mlen={mlen}, Klen={klen}. Skipping RoPE for query.\")\n",
    "            q_head_rot = q_head # Skip RoPE for query as a fallback\n",
    "        else:\n",
    "            q_head_rot = apply_rotary_pos_emb(q_head, cos_q, sin_q) # Apply RoPE to Query\n",
    "\n",
    "        if cos_k.shape[0] != klen or sin_k.shape[0] != klen:\n",
    "             logging.error(f\"Layer {self.layer_idx}: RoPE key cos/sin shape mismatch! Expected {klen}, got {cos_k.shape[0]}. Mlen={mlen}, Klen={klen}. Skipping RoPE for key.\")\n",
    "             k_head_rot = k_head # Skip RoPE for key as a fallback\n",
    "        else:\n",
    "            k_head_rot = apply_rotary_pos_emb(k_head, cos_k, sin_k) # Apply RoPE to Key\n",
    "\n",
    "\n",
    "        # --- Prepare for Attention Calculation ---\n",
    "        # Permute for batch matrix multiplication: (bsz, n_head, seq_len, d_head)\n",
    "        q_head_ = q_head_rot.permute(1, 2, 0, 3) # (bsz, n_head, qlen, d_head)\n",
    "        k_head_ = k_head_rot.permute(1, 2, 0, 3) # (bsz, n_head, klen, d_head)\n",
    "        v_head_ = v_head.permute(1, 2, 0, 3)     # (bsz, n_head, klen, d_head)\n",
    "\n",
    "        # --- Calculate Attention Scores ---\n",
    "        attn_score = torch.matmul(q_head_, k_head_.transpose(-2, -1)) # (bsz, n_head, qlen, klen)\n",
    "        attn_score = attn_score * self.scale # Scale scores\n",
    "\n",
    "        # --- Apply Attention Mask ---\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dim() == 2: mask_to_apply = attn_mask.unsqueeze(0).unsqueeze(0) # -> (1, 1, qlen, klen)\n",
    "            elif attn_mask.dim() == 4: mask_to_apply = attn_mask\n",
    "            else: logging.warning(f\"L{self.layer_idx}: Unexpected mask shape {attn_mask.shape}. Ignored.\"); mask_to_apply = None\n",
    "\n",
    "            if mask_to_apply is not None:\n",
    "                mask_to_apply = mask_to_apply.to(device=attn_score.device, dtype=torch.bool)\n",
    "                # Check compatibility before applying\n",
    "                if mask_to_apply.shape[-2:] == attn_score.shape[-2:]:\n",
    "                    if mask_to_apply.shape[0] != attn_score.shape[0] and mask_to_apply.shape[0] == 1: mask_to_apply = mask_to_apply.expand(attn_score.shape[0], -1, -1, -1)\n",
    "                    if mask_to_apply.shape[1] != attn_score.shape[1] and mask_to_apply.shape[1] == 1: mask_to_apply = mask_to_apply.expand(-1, attn_score.shape[1], -1, -1)\n",
    "                    if mask_to_apply.shape == attn_score.shape: attn_score = attn_score.masked_fill(mask_to_apply, torch.finfo(attn_score.dtype).min)\n",
    "                    else: logging.warning(f\"L{self.layer_idx}: Mask/score shape mismatch after expansion {mask_to_apply.shape} vs {attn_score.shape}. Mask Ignored.\")\n",
    "                else: logging.warning(f\"L{self.layer_idx}: Mask dims incompatible {mask_to_apply.shape[-2:]} vs {attn_score.shape[-2:]}. Mask Ignored.\")\n",
    "\n",
    "        # --- Calculate Attention Probabilities ---\n",
    "        attn_prob = F.softmax(attn_score.float(), dim=-1).to(attn_score.dtype)\n",
    "        attn_prob = self.drop(attn_prob)\n",
    "\n",
    "        # --- Apply Head Mask (Optional) ---\n",
    "        if head_mask is not None: attn_prob = attn_prob * head_mask.to(attn_prob.device)\n",
    "\n",
    "        # --- Calculate Attention Output ---\n",
    "        attn_vec = torch.matmul(attn_prob, v_head_).permute(2, 0, 1, 3).contiguous()\n",
    "        attn_vec = attn_vec.view(qlen, bsz, self.n_head * self.d_head)\n",
    "\n",
    "        # --- Final Output Projection ---\n",
    "        attn_out = self.o_net(attn_vec)\n",
    "        attn_out = self.drop(attn_out)\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "\n",
    "class TransformerXLLayer(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_head, d_inner, dropout, config: TrainingConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.dec_attn = RelPartialLearnableMultiHeadAttn(n_head, d_model, d_head, dropout, config=config, layer_idx=layer_idx)\n",
    "        self.pos_ff = nn.Sequential(nn.Linear(d_model, d_inner), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_inner, d_model), nn.Dropout(dropout))\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, dec_inp: torch.Tensor, mems: Optional[torch.Tensor], attn_mask: Optional[torch.Tensor] = None, head_mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        x_norm1 = self.norm1(dec_inp)\n",
    "        attn_output = self.dec_attn(w=x_norm1, mems=mems, attn_mask=attn_mask, head_mask=head_mask)\n",
    "        h = dec_inp + self.dropout(attn_output)\n",
    "        h_norm2 = self.norm2(h)\n",
    "        ff_output = self.pos_ff(h_norm2)\n",
    "        output = h + self.dropout(ff_output)\n",
    "        return output\n",
    "\n",
    "class MelodyTransformerXL(nn.Module):\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # --- Validate Config ---\n",
    "        if not (isinstance(config.melody_vocab_size, int) and config.melody_vocab_size > 0): raise ValueError(f\"Invalid melody_vocab_size: {config.melody_vocab_size}\")\n",
    "        if not (isinstance(config.melody_pad_token_id, int) and 0 <= config.melody_pad_token_id < config.melody_vocab_size): raise ValueError(f\"Invalid melody_pad_token_id: {config.melody_pad_token_id}\")\n",
    "        self.use_chord_embedding = isinstance(config.chord_emb_dim, int) and config.chord_emb_dim > 0\n",
    "        if self.use_chord_embedding:\n",
    "            if not (isinstance(config.chord_vocab_size, int) and config.chord_vocab_size > 0): raise ValueError(f\"Invalid chord_vocab_size: {config.chord_vocab_size}\")\n",
    "            if not (isinstance(config.chord_pad_token_id, int) and 0 <= config.chord_pad_token_id < config.chord_vocab_size): raise ValueError(f\"Invalid chord_pad_token_id: {config.chord_pad_token_id}\")\n",
    "        if not (isinstance(config.d_model, int) and config.d_model > 0): raise ValueError(f\"Invalid d_model: {config.d_model}\")\n",
    "        if not (isinstance(config.n_head, int) and config.n_head > 0): raise ValueError(f\"Invalid n_head: {config.n_head}\")\n",
    "        if config.d_model % config.n_head != 0: raise ValueError(f\"d_model ({config.d_model}) not divisible by n_head ({config.n_head}).\")\n",
    "        self.d_head = config.d_model // config.n_head\n",
    "        config.d_head = self.d_head\n",
    "        if not (isinstance(config.mem_len, int) and config.mem_len >= 0): raise ValueError(f\"Invalid mem_len: {config.mem_len}\")\n",
    "        if not (isinstance(config.n_layer, int) and config.n_layer > 0): raise ValueError(f\"Invalid n_layer: {config.n_layer}\")\n",
    "        if not (isinstance(config.num_chord_features, int) and config.num_chord_features > 0): raise ValueError(f\"Invalid num_chord_features: {config.num_chord_features}\")\n",
    "        if not (isinstance(config.condition_proj_dim, int) and config.condition_proj_dim >= 0): raise ValueError(f\"Invalid condition_proj_dim: {config.condition_proj_dim}\")\n",
    "\n",
    "        # --- Model Components ---\n",
    "        self.d_model = config.d_model; self.n_head = config.n_head; self.mem_len = config.mem_len; self.n_layer = config.n_layer\n",
    "        self.melody_emb = nn.Embedding(config.melody_vocab_size, config.d_model, padding_idx=config.melody_pad_token_id)\n",
    "\n",
    "        # --- Conditioning Processing ---\n",
    "        condition_proj_dim = max(1, config.condition_proj_dim) if config.condition_proj_dim > 0 else 0\n",
    "        total_conditioning_dim = 0\n",
    "        self.chord_feature_processor = None\n",
    "        if condition_proj_dim > 0 :\n",
    "            self.chord_feature_processor = nn.Linear(config.num_chord_features, condition_proj_dim)\n",
    "            total_conditioning_dim += condition_proj_dim\n",
    "        else:\n",
    "            logging.info(\"Raw chord features projection disabled (dim=0).\")\n",
    "\n",
    "        self.chord_emb = None\n",
    "        if self.use_chord_embedding:\n",
    "            chord_emb_dim = max(1, config.chord_emb_dim)\n",
    "            self.chord_emb = nn.Embedding(config.chord_vocab_size, chord_emb_dim, padding_idx=config.chord_pad_token_id)\n",
    "            total_conditioning_dim += chord_emb_dim\n",
    "\n",
    "        # --- Input Projection ---\n",
    "        self.input_proj = None\n",
    "        if total_conditioning_dim > 0:\n",
    "            combined_input_dim = config.d_model + total_conditioning_dim\n",
    "            self.input_proj = nn.Linear(combined_input_dim, config.d_model)\n",
    "            logging.info(f\"Input projection: Combined dim {combined_input_dim} -> {config.d_model}\")\n",
    "        else:\n",
    "            logging.info(\"No conditioning used, skipping input projection.\")\n",
    "\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "        # --- Transformer Layers ---\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerXLLayer(n_head=self.n_head, d_model=self.d_model, d_head=self.d_head, d_inner=config.d_inner, dropout=config.dropout, config=config, layer_idx=i)\n",
    "            for i in range(config.n_layer)\n",
    "        ])\n",
    "\n",
    "        # --- Output Layer ---\n",
    "        self.final_norm = nn.LayerNorm(config.d_model)\n",
    "        self.out_layer = nn.Linear(config.d_model, config.melody_vocab_size, bias=False)\n",
    "\n",
    "        # Weight Tying\n",
    "        if self.melody_emb.embedding_dim == self.out_layer.in_features:\n",
    "           self.out_layer.weight = self.melody_emb.weight\n",
    "           logging.info(\"Tying input melody embedding weights with the final output layer.\")\n",
    "        else:\n",
    "            logging.warning(f\"Output layer in_features ({self.out_layer.in_features}) != Melody embedding dim ({self.melody_emb.embedding_dim}). Weights not tied.\")\n",
    "\n",
    "        # Apply custom weight initialization AFTER defining all layers\n",
    "        self.apply(self._init_weights)\n",
    "        logging.info(f\"MelodyTransformerXL initialized: Layers={config.n_layer}, d_model={config.d_model}, n_head={config.n_head}, mem_len={config.mem_len}, ChordEmb={self.use_chord_embedding}, ChordFeat={self.chord_feature_processor is not None}\")\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        try: return next(self.parameters()).dtype\n",
    "        except StopIteration: return torch.get_default_dtype()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initializes weights of linear and embedding layers.\"\"\"\n",
    "        scale = 1.0\n",
    "        # Heuristic scale factor based on number of layers (from GPT-2)\n",
    "        if hasattr(self.config, 'n_layer') and self.config.n_layer > 0:\n",
    "            scale = 1 / math.sqrt(2.0 * self.config.n_layer)\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Initialize linear layers with small normal distribution\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02 * scale)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # Initialize embedding layers with small normal distribution\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            # Zero out padding token embedding correctly\n",
    "            if module.padding_idx is not None:\n",
    "                # <<< FIX: Use with torch.no_grad() context manager >>>\n",
    "                with torch.no_grad():\n",
    "                    module.weight[module.padding_idx].fill_(0)\n",
    "                # <<< END FIX >>>\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            # Initialize LayerNorm bias to 0 and weight to 1\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "            if module.weight is not None:\n",
    "                module.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "    def _update_mems(self, hids: List[Optional[torch.Tensor]], mems: List[Optional[torch.Tensor]], mlen: int) -> List[Optional[torch.Tensor]]:\n",
    "        if mlen <= 0 or not hids: return [None] * (self.n_layer + 1)\n",
    "        if mems is None or all(m is None or m.numel() == 0 for m in mems):\n",
    "            return [(h[-mlen:].detach() if h is not None and h.dim() > 1 and h.shape[0] > 0 else None) for h in hids]\n",
    "        if len(hids) != len(mems):\n",
    "            logging.error(f\"BUG: Mismatch hids({len(hids)}) vs mems({len(mems)}). Resetting memory.\")\n",
    "            return [(h[-mlen:].detach() if h is not None and h.dim() > 1 and h.shape[0] > 0 else None) for h in hids]\n",
    "        new_mems = []\n",
    "        with torch.no_grad():\n",
    "            for i, (hid, mem) in enumerate(zip(hids, mems)):\n",
    "                if hid is None: new_mems.append(mem.detach() if mem is not None else None); logging.warning(f\"Hid layer {i} None in mem update.\"); continue\n",
    "                if hid.dim() < 3 or hid.shape[0] == 0: logging.warning(f\"Hid layer {i} bad shape {hid.shape}.\"); new_mems.append(mem.detach() if mem is not None else None); continue\n",
    "                if mem is not None and mem.dim() == 3 and mem.numel() > 0:\n",
    "                    if mem.shape[1:] == hid.shape[1:]: cat = torch.cat([mem, hid], dim=0)\n",
    "                    else: logging.warning(f\"Mem/Hid shape mismatch L{i}: Mem={mem.shape}, Hid={hid.shape}. Resetting mem.\"); cat = hid\n",
    "                else: cat = hid\n",
    "                new_mems.append(cat[-mlen:].detach())\n",
    "        return new_mems\n",
    "\n",
    "    def init_mems(self, bsz: int, device: torch.device, dtype: torch.dtype) -> List[Optional[torch.Tensor]]:\n",
    "        return [None] * (self.n_layer + 1) # Initialize with Nones, let _update_mems handle creation\n",
    "\n",
    "    def _create_attn_mask(self, qlen: int, mlen: int, device: torch.device) -> Optional[torch.Tensor]:\n",
    "        if qlen <= 0: return None\n",
    "        klen = mlen + qlen\n",
    "        mask = torch.triu(torch.ones(qlen, klen, device=device, dtype=torch.bool), diagonal=1 + mlen)\n",
    "        return mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        event_ids: torch.Tensor,                  # (bsz, qlen) [Long]\n",
    "        conditioning_chord_ids: torch.Tensor,      # (bsz, qlen) [Long]\n",
    "        conditioning_root_pc: torch.Tensor,       # (bsz, qlen) [Float/Half]\n",
    "        conditioning_quality_code: torch.Tensor, # (bsz, qlen) [Float/Half]\n",
    "        conditioning_function_code: torch.Tensor, # (bsz, qlen) [Float/Half]\n",
    "        mems: Optional[List[Optional[torch.Tensor]]] = None # Memory\n",
    "    ) -> Tuple[torch.Tensor, List[Optional[torch.Tensor]]]:\n",
    "\n",
    "        bsz, qlen = event_ids.size(); device = event_ids.device; target_dtype = self.dtype\n",
    "        if qlen == 0: logging.warning(\"Forward qlen=0.\"); return torch.empty((bsz, 0, self.config.melody_vocab_size), device=device, dtype=target_dtype), mems if mems is not None else self.init_mems(bsz, device, target_dtype)\n",
    "        if self.mem_len > 0:\n",
    "            if mems is None: mems = self.init_mems(bsz, device, target_dtype)\n",
    "            elif not isinstance(mems, list) or len(mems) != self.n_layer + 1: logging.warning(f\"Incorrect mem list. Resetting.\"); mems = self.init_mems(bsz, device, target_dtype)\n",
    "            # Validate memory compatibility before use (more robust)\n",
    "            valid_mems = True\n",
    "            for i, mem in enumerate(mems):\n",
    "                if mem is not None and mem.numel() > 0:\n",
    "                    if mem.shape[1] != bsz or mem.shape[2] != self.d_model or mem.device != device or mem.dtype != target_dtype:\n",
    "                        logging.warning(f\"Memory state at index {i} incompatible (Shape:{mem.shape}, DType:{mem.dtype}, Device:{mem.device} vs Input: bsz={bsz}, d_model={self.d_model}, dtype={target_dtype}, device={device}). Resetting memory.\")\n",
    "                        mems = self.init_mems(bsz, device, target_dtype)\n",
    "                        valid_mems = False\n",
    "                        break\n",
    "            mlen = mems[0].size(0) if valid_mems and mems[0] is not None and mems[0].dim() == 3 else 0\n",
    "        else: mems = [None] * (self.n_layer + 1); mlen = 0\n",
    "\n",
    "        clamped_event_ids = event_ids.clamp(0, self.config.melody_vocab_size - 1); melody_embedded = self.melody_emb(clamped_event_ids)\n",
    "        all_conditioning_tensors = []\n",
    "        if self.chord_feature_processor is not None:\n",
    "            cond_features_raw = torch.stack([conditioning_root_pc, conditioning_quality_code, conditioning_function_code], dim=-1).to(target_dtype)\n",
    "            cond_features_proj = F.relu(self.chord_feature_processor(cond_features_raw)); all_conditioning_tensors.append(cond_features_proj)\n",
    "        if self.use_chord_embedding and self.chord_emb is not None:\n",
    "             clamped_chord_ids = conditioning_chord_ids.clamp(0, self.config.chord_vocab_size - 1)\n",
    "             chord_embedded = self.chord_emb(clamped_chord_ids); all_conditioning_tensors.append(chord_embedded)\n",
    "\n",
    "        if all_conditioning_tensors:\n",
    "            cond_combined = torch.cat(all_conditioning_tensors, dim=-1); combined_input_features = torch.cat([melody_embedded, cond_combined], dim=-1)\n",
    "            if self.input_proj is not None: core_input = self.input_proj(combined_input_features)\n",
    "            else: logging.error(\"Conditioning exists but input_proj is None.\"); core_input = melody_embedded\n",
    "        else: core_input = self.input_proj(melody_embedded) if self.input_proj is not None else melody_embedded\n",
    "\n",
    "        core_input = self.drop(core_input).transpose(0, 1).contiguous()\n",
    "        attn_mask = self._create_attn_mask(qlen, mlen, device)\n",
    "        hids_for_mem = [core_input]; layer_input = core_input\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer_mem = mems[i] if mems is not None else None\n",
    "            try: layer_output = layer(layer_input, mems=layer_mem, attn_mask=attn_mask); hids_for_mem.append(layer_output); layer_input = layer_output\n",
    "            except Exception as e: raise RuntimeError(f\"Failed during forward pass in layer {i}\") from e\n",
    "        new_mems = self._update_mems(hids_for_mem, mems, self.mem_len)\n",
    "        core_output = self.drop(layer_input); final_output = self.final_norm(core_output)\n",
    "        logits = self.out_layer(final_output).transpose(0, 1).contiguous()\n",
    "        return logits, new_mems\n",
    "# --- END Model Class Definitions ---\n",
    "\n",
    "\n",
    "# === MIDI Conversion Function ===\n",
    "def events_to_midi(event_sequence: List[str],\n",
    "                   output_midi_path: str = \"generated_melody.mid\",\n",
    "                   resolution: int = 480,\n",
    "                   default_velocity: int = 100,\n",
    "                   initial_tempo_qpm: float = 120.0):\n",
    "    \"\"\"\n",
    "    Converts a sequence of event tokens into a MIDI file using pretty_midi.\n",
    "    Handles potential errors during conversion and provides a summary.\n",
    "    \"\"\"\n",
    "    if not event_sequence:\n",
    "        print(\"Warning: Empty event sequence provided to events_to_midi. No MIDI file generated.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting MIDI conversion for {len(event_sequence)} events...\")\n",
    "    logging.info(f\"Starting MIDI conversion for {len(event_sequence)} events...\")\n",
    "\n",
    "    # Initialize counters outside the main try block\n",
    "    skipped_events = 0\n",
    "    note_count = 0\n",
    "    malformed_events = 0\n",
    "    unrecognized_events = 0\n",
    "    zero_duration_notes = 0\n",
    "    negative_time_shifts = 0\n",
    "    note_add_count = 0\n",
    "    note_skip_zero_dur_count = 0\n",
    "    notes_closed_at_end = 0\n",
    "    zero_dur_at_end = 0\n",
    "\n",
    "    try:\n",
    "        midi_data = pretty_midi.PrettyMIDI(resolution=resolution, initial_tempo=initial_tempo_qpm)\n",
    "        instrument = pretty_midi.Instrument(program=0, is_drum=False, name='Generated Melody') # Program 0 = Acoustic Grand Piano\n",
    "\n",
    "        current_time_seconds = 0.0\n",
    "        current_velocity = default_velocity\n",
    "        active_notes = {} # pitch -> (start_time_seconds, velocity) - Tracks notes currently playing\n",
    "\n",
    "        # Define expected special tokens\n",
    "        special_tokens = {MELODY_PAD_TOKEN, MELODY_UNK_TOKEN, \"<EOS>\", \"<START>\", \"<END>\"} # Use defined constants\n",
    "\n",
    "        time_shift_unit = 0.001 # Assume TIME_SHIFT value is in milliseconds\n",
    "\n",
    "        for i, event_str in enumerate(tqdm(event_sequence, desc=\"Converting Events\", leave=False, unit=\"event\")):\n",
    "            # Use 'continue' for skipping within the loop\n",
    "            if not isinstance(event_str, str) or not event_str or event_str in special_tokens:\n",
    "                skipped_events += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                parts = event_str.split('_')\n",
    "                if len(parts) < 2:\n",
    "                    logging.warning(f\"Event {i}: Malformed event '{event_str}'. Skipping.\")\n",
    "                    malformed_events += 1\n",
    "                    continue\n",
    "\n",
    "                event_type = \"_\".join(parts[:-1]).upper()\n",
    "                value_str = parts[-1]\n",
    "\n",
    "                try:\n",
    "                    value = int(value_str)\n",
    "                except ValueError:\n",
    "                    logging.warning(f\"Event {i}: Could not parse integer value '{value_str}' in event '{event_str}'. Skipping.\")\n",
    "                    malformed_events += 1\n",
    "                    continue\n",
    "\n",
    "                # --- Event Logic ---\n",
    "                if event_type == \"TIME_SHIFT\":\n",
    "                    is_neg = value < 0\n",
    "                    if is_neg:\n",
    "                        logging.warning(f\"Event {i}: Negative TIME_SHIFT ({value}) encountered. Clamping to 0.\")\n",
    "                        negative_time_shifts += 1\n",
    "                        value = 0\n",
    "                    time_delta_seconds = value * time_shift_unit\n",
    "                    current_time_seconds += time_delta_seconds\n",
    "\n",
    "                elif event_type == \"NOTE_ON\":\n",
    "                    pitch = value\n",
    "                    if not (0 <= pitch <= 127): logging.warning(f\"Event {i}: Invalid MIDI pitch {pitch} in NOTE_ON. Skipping.\"); malformed_events += 1; continue\n",
    "\n",
    "                    if pitch in active_notes: # End previous instance of same note if active\n",
    "                        prev_start_time, prev_vel = active_notes[pitch]\n",
    "                        if current_time_seconds > prev_start_time + 1e-6: # Ensure positive duration\n",
    "                            note = pretty_midi.Note(velocity=prev_vel, pitch=pitch, start=prev_start_time, end=current_time_seconds)\n",
    "                            instrument.notes.append(note); note_count += 1; note_add_count += 1\n",
    "                        else:\n",
    "                            zero_duration_notes += 1; note_skip_zero_dur_count += 1 # Count it as skipped due to zero duration\n",
    "                            # Optionally log this specific case:\n",
    "                            # logging.debug(f\"Event {i}: Re-triggering NOTE_ON_{pitch} at same time {current_time_seconds:.4f}s. Previous instance skipped.\")\n",
    "\n",
    "                    active_notes[pitch] = (current_time_seconds, current_velocity) # Start new note\n",
    "\n",
    "                elif event_type == \"NOTE_OFF\":\n",
    "                    pitch = value\n",
    "                    if not (0 <= pitch <= 127): logging.warning(f\"Event {i}: Invalid MIDI pitch {pitch} in NOTE_OFF. Skipping.\"); malformed_events += 1; continue\n",
    "                    if pitch in active_notes:\n",
    "                        start_time_sec, vel = active_notes.pop(pitch) # Remove from active notes\n",
    "                        end_time_sec = current_time_seconds\n",
    "                        if end_time_sec > start_time_sec + 1e-6: # Ensure positive duration\n",
    "                            note = pretty_midi.Note(velocity=vel, pitch=pitch, start=start_time_sec, end=end_time_sec)\n",
    "                            instrument.notes.append(note); note_count += 1; note_add_count += 1\n",
    "                        else:\n",
    "                            logging.warning(f\"Event {i}: NOTE_OFF_{pitch} at time {end_time_sec:.4f}s resulted in zero/negative duration (start time {start_time_sec:.4f}s). Skipping note.\")\n",
    "                            zero_duration_notes += 1\n",
    "                            note_skip_zero_dur_count += 1\n",
    "                    else:\n",
    "                         logging.debug(f\"Event {i}: NOTE_OFF_{pitch} received for inactive note. Ignoring.\") # Less severe than warning\n",
    "\n",
    "\n",
    "                elif event_type == \"SET_VELOCITY\":\n",
    "                    current_velocity = max(0, min(127, value)) # Clamp velocity\n",
    "\n",
    "                else:\n",
    "                    logging.warning(f\"Event {i}: Unrecognized event type '{event_type}' in '{event_str}'. Skipping.\")\n",
    "                    unrecognized_events += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unexpected error processing event '{event_str}' at index {i}: {e}\", exc_info=True)\n",
    "                skipped_events += 1 # Count as skipped due to unexpected error\n",
    "\n",
    "\n",
    "        # --- Final Cleanup ---\n",
    "        if active_notes:\n",
    "            logging.warning(f\"Found {len(active_notes)} notes still active at the end of the sequence. Closing them at final time {current_time_seconds:.4f}s.\")\n",
    "            for pitch, (start_time_sec, vel) in active_notes.items():\n",
    "                 if current_time_seconds > start_time_sec + 1e-6:\n",
    "                      note = pretty_midi.Note(velocity=vel, pitch=pitch, start=start_time_sec, end=current_time_seconds)\n",
    "                      instrument.notes.append(note); note_count += 1; notes_closed_at_end += 1\n",
    "                 else:\n",
    "                      zero_duration_notes += 1; zero_dur_at_end += 1\n",
    "            logging.info(f\"Notes closed at end: {notes_closed_at_end}, Zero duration at end: {zero_dur_at_end}\")\n",
    "\n",
    "        midi_data.instruments.append(instrument)\n",
    "\n",
    "        # --- Write MIDI File ---\n",
    "        midi_data.write(output_midi_path) # Attempt to write the file\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"MIDI Conversion Summary:\")\n",
    "        print(f\"  MIDI file successfully written to: {output_midi_path}\")\n",
    "        print(f\"  Notes created: {note_count} (Added: {note_add_count + notes_closed_at_end})\") # Use updated counters\n",
    "        total_problematic = skipped_events + malformed_events + unrecognized_events + zero_duration_notes + negative_time_shifts\n",
    "        print(f\"  Total problematic events skipped/handled: {total_problematic}\")\n",
    "        if skipped_events > 0: print(f\"    - Special/Empty/Error tokens skipped: {skipped_events}\")\n",
    "        if malformed_events > 0: print(f\"    - Malformed events skipped: {malformed_events}\")\n",
    "        if unrecognized_events > 0: print(f\"    - Unrecognized events skipped: {unrecognized_events}\")\n",
    "        if zero_duration_notes > 0: print(f\"    - Zero/Negative duration notes skipped: {zero_duration_notes} (During seq: {note_skip_zero_dur_count}, At end: {zero_dur_at_end})\")\n",
    "        if negative_time_shifts > 0: print(f\"    - Negative TIME_SHIFT values clamped: {negative_time_shifts}\")\n",
    "        print(\"-\" * 30)\n",
    "        logging.info(f\"MIDI file successfully written to {output_midi_path}\")\n",
    "        logging.info(f\"Converted {note_count} notes. Skipped/Handled {total_problematic} problematic events.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # This outer except block catches errors during setup or writing\n",
    "        print(f\"\\nFATAL ERROR during MIDI conversion or writing to {output_midi_path}: {e}\")\n",
    "        logging.error(f\"Error during MIDI conversion: {e}\", exc_info=True)\n",
    "        # Still print summary with potentially incomplete counts if error happened mid-loop\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"MIDI Conversion Summary (ERROR OCCURRED):\")\n",
    "        print(f\"  Attempted to write to: {output_midi_path}\")\n",
    "        print(f\"  Notes converted before error (approx): {note_count}\")\n",
    "        total_problematic = skipped_events + malformed_events + unrecognized_events + zero_duration_notes + negative_time_shifts\n",
    "        print(f\"  Total problematic events encountered before error: {total_problematic}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# === Generation Function (Over Progression) ===\n",
    "@torch.no_grad()\n",
    "def generate_melody_over_progression(\n",
    "    model: MelodyTransformerXL,\n",
    "    config: TrainingConfig,\n",
    "    device: torch.device,\n",
    "    start_event_id: int,              # Single start token ID\n",
    "    full_cond_ids: torch.Tensor,      # Full sequence (1, full_length) [Long]\n",
    "    full_cond_root: torch.Tensor,     # Full sequence (1, full_length) [Float/Half]\n",
    "    full_cond_qual: torch.Tensor,     # Full sequence (1, full_length) [Float/Half]\n",
    "    full_cond_func: torch.Tensor,     # Full sequence (1, full_length) [Float/Half]\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 0,\n",
    "    top_p: float = 0.0,\n",
    "    eos_token_id: Optional[int] = None,\n",
    "    pad_token_id: int = 0\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Generates a melody sequence autoregressively using the trained model,\n",
    "    conditioned step-by-step on the provided full chord progression.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    model_dtype = model.dtype # Get model's expected data type\n",
    "\n",
    "    # --- Input Validation ---\n",
    "    if full_cond_ids.shape[0] != 1: raise ValueError(\"Generation currently only supports batch size 1.\")\n",
    "    bsz, full_length = full_cond_ids.shape\n",
    "    if full_length == 0: raise ValueError(\"Conditioning sequence cannot be empty.\")\n",
    "    if not (full_cond_root.shape == (bsz, full_length) and full_cond_qual.shape == (bsz, full_length) and full_cond_func.shape == (bsz, full_length)):\n",
    "        raise ValueError(f\"Conditioning tensor lengths do not match. Chords:{full_cond_ids.shape}, Root:{full_cond_root.shape}, Qual:{full_cond_qual.shape}, Func:{full_cond_func.shape}\")\n",
    "\n",
    "    # Move conditioning tensors to the target device and ensure correct dtypes\n",
    "    full_cond_ids = full_cond_ids.to(device)\n",
    "    full_cond_root = full_cond_root.to(device=device, dtype=model_dtype)\n",
    "    full_cond_qual = full_cond_qual.to(device=device, dtype=model_dtype)\n",
    "    full_cond_func = full_cond_func.to(device=device, dtype=model_dtype)\n",
    "\n",
    "    # --- Initialize Memory and Output List ---\n",
    "    mems = model.init_mems(bsz=bsz, device=device, dtype=model_dtype)\n",
    "    generated_ids = [start_event_id] # Start with the initial melody token\n",
    "    current_event_ids = torch.tensor([[start_event_id]], dtype=torch.long, device=device) # Shape: (1, 1)\n",
    "\n",
    "    # --- Configure AMP ---\n",
    "    amp_gen_enabled = (config.amp_dtype == torch.float16 and device.type == 'cuda')\n",
    "    amp_gen_dtype = config.amp_dtype if amp_gen_enabled else None\n",
    "    device_type_str = device.type\n",
    "    if amp_gen_enabled: print(f\"Using AMP for generation with dtype: {amp_gen_dtype}\")\n",
    "\n",
    "    # --- Autoregressive Generation Loop ---\n",
    "    # Generate one token for each step of the provided conditioning sequence\n",
    "    # The length of generated melody will be equal to the length of conditioning\n",
    "    generation_steps = full_length - 1 # We generate one less token than the conditioning length\n",
    "    print(f\"Starting generation for {generation_steps} steps over the provided progression (Total output length: {full_length})...\")\n",
    "\n",
    "    for step in tqdm(range(generation_steps), desc=\"Generating Melody\", unit=\"token\"):\n",
    "        # Get conditioning for the *current* time step\n",
    "        # Note: We use conditioning from step 'step' to predict event 'step+1'\n",
    "        # Input shapes to model should be (bsz, qlen=1, dim)\n",
    "        current_cond_id   = full_cond_ids[:, step:step+1]\n",
    "        current_cond_root = full_cond_root[:, step:step+1]\n",
    "        current_cond_qual = full_cond_qual[:, step:step+1]\n",
    "        current_cond_func = full_cond_func[:, step:step+1]\n",
    "\n",
    "        try:\n",
    "            # --- Model Forward Pass (uses previous melody token + current conditioning) ---\n",
    "            with torch.amp.autocast(device_type=device_type_str, dtype=amp_gen_dtype, enabled=amp_gen_enabled):\n",
    "                 logits, new_mems = model(\n",
    "                     event_ids=current_event_ids, # Shape (1, 1) - the previous token\n",
    "                     conditioning_chord_ids=current_cond_id,\n",
    "                     conditioning_root_pc=current_cond_root,\n",
    "                     conditioning_quality_code=current_cond_qual,\n",
    "                     conditioning_function_code=current_cond_func,\n",
    "                     mems=mems # Pass the current memory state\n",
    "                 )\n",
    "            mems = new_mems\n",
    "\n",
    "            # --- Process Logits for Next Token ---\n",
    "            next_token_logits = logits[:, -1, :] # Shape: (1, vocab_size) - Logits for the token at this step\n",
    "\n",
    "            # --- Token Sampling ---\n",
    "            if temperature <= 0: # Greedy decoding\n",
    "                 next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            else: # Temperature + Top-K/Top-P sampling\n",
    "                 scaled_logits = next_token_logits / temperature\n",
    "                 if top_k > 0:\n",
    "                      v, _ = torch.topk(scaled_logits, min(top_k, scaled_logits.size(-1)))\n",
    "                      kth_value = v[:, [-1]]\n",
    "                      scaled_logits[scaled_logits < kth_value] = -float('Inf')\n",
    "                 if top_p > 0.0:\n",
    "                      sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True)\n",
    "                      cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                      sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                      sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                      sorted_indices_to_remove[..., 0] = 0 # Never remove the most likely token\n",
    "\n",
    "                      indices_to_remove = torch.zeros_like(scaled_logits, dtype=torch.bool).scatter_(\n",
    "                          dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "                      scaled_logits[indices_to_remove] = -float('Inf')\n",
    "                 # Sample from the filtered distribution\n",
    "                 probs = F.softmax(scaled_logits.float(), dim=-1)\n",
    "                 next_token_id = torch.multinomial(probs, num_samples=1) # Shape: (1, 1)\n",
    "\n",
    "            # --- Post-processing and Loop Control ---\n",
    "            next_token_item = next_token_id.item() # Get the generated token ID as an integer\n",
    "\n",
    "            # Check for End-of-Sequence token\n",
    "            if eos_token_id is not None and next_token_item == eos_token_id:\n",
    "                print(f\"\\nEOS token ({eos_token_id}) generated at step {step}. Stopping generation.\")\n",
    "                # Pad the rest of the sequence to match conditioning length if stopped early\n",
    "                padding_needed = generation_length - step\n",
    "                generated_ids.extend([pad_token_id] * padding_needed)\n",
    "                break\n",
    "\n",
    "            # Append generated token to the sequence\n",
    "            generated_ids.append(next_token_item)\n",
    "\n",
    "            # Update the input for the next iteration\n",
    "            current_event_ids = next_token_id # Shape: (1, 1)\n",
    "\n",
    "            # Optional: Periodic Garbage Collection / Cache Clearing\n",
    "            if step > 0 and step % 200 == 0:\n",
    "                gc.collect()\n",
    "                if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "\n",
    "        # --- Error Handling within the Loop ---\n",
    "        except RuntimeError as e:\n",
    "            print(f\"\\nRuntimeError during generation step {step}: {e}\")\n",
    "            logging.error(f\"RuntimeError during generation step {step}: {e}\", exc_info=True)\n",
    "            if 'cuda' in str(e).lower(): gc.collect(); torch.cuda.empty_cache(); print(\"CUDA cache cleared.\")\n",
    "            print(\"Stopping generation due to runtime error.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "             print(f\"\\nUnexpected error during generation step {step}: {e}\")\n",
    "             logging.error(f\"Unexpected error during generation step {step}: {e}\", exc_info=True)\n",
    "             traceback.print_exc(); print(\"Stopping generation due to unexpected error.\")\n",
    "             break\n",
    "\n",
    "    # Ensure final length matches conditioning length if generation finished normally\n",
    "    if len(generated_ids) < full_length:\n",
    "        padding_needed = full_length - len(generated_ids)\n",
    "        logging.warning(f\"Generation finished but sequence is short. Padding with {padding_needed} PAD tokens.\")\n",
    "        generated_ids.extend([pad_token_id] * padding_needed)\n",
    "\n",
    "    return generated_ids\n",
    "\n",
    "\n",
    "# === Helper Function to Load Real Priming Data ===\n",
    "# (Not used in \"generate over progression\" mode, but kept for flexibility)\n",
    "def load_real_prime_sequence(\n",
    "    data_path: str,\n",
    "    prime_len: int,\n",
    "    melody_vocab: Dict[str, int],\n",
    "    chord_vocab: Dict[str, int],\n",
    "    config: TrainingConfig\n",
    ") -> Optional[Tuple[List[int], List[int], List[float], List[float], List[float]]]:\n",
    "    \"\"\"\n",
    "    Attempts to load a real priming sequence from the start of the dataset file.\n",
    "\n",
    "    *** IMPORTANT USER ACTION REQUIRED: ***\n",
    "    1. VERIFY the `data_path` points to your actual training data file (JSON Lines format expected).\n",
    "    2. CHECK the keys used below match YOUR data structure.\n",
    "       Based on your sample data, the keys should be:\n",
    "       'event_ids', 'conditioning_chord_ids', 'conditioning_root_pc',\n",
    "       'conditioning_quality_code', 'conditioning_function_code'\n",
    "    3. UNCOMMENT the call to this function in the `__main__` block if you want to use priming.\n",
    "    \"\"\"\n",
    "    print(f\"(Attempting to load real priming sequence from: {data_path})\") # Indicate it's for priming\n",
    "    if not os.path.exists(data_path):\n",
    "        logging.error(f\"Priming data file not found: {data_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f): # Try first few lines\n",
    "                if line_num > 5: break # Stop after a few attempts\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "\n",
    "                try: data_item = json.loads(line)\n",
    "                except json.JSONDecodeError as json_e: logging.warning(f\"Skipping line {line_num+1} for priming (JSON decode): {json_e}\"); continue\n",
    "\n",
    "                # --- V V V --- KEYS BASED ON PROVIDED DATA --- V V V ---\n",
    "                event_key = 'event_ids'\n",
    "                chord_id_key = 'conditioning_chord_ids'\n",
    "                root_pc_key = 'conditioning_root_pc'\n",
    "                quality_key = 'conditioning_quality_code'\n",
    "                function_key = 'conditioning_function_code'\n",
    "                # --- ^ ^ ^ --- KEYS BASED ON PROVIDED DATA --- ^ ^ ^ ---\n",
    "\n",
    "                event_data = data_item.get(event_key)\n",
    "                chord_data = data_item.get(chord_id_key)\n",
    "                root_pcs = data_item.get(root_pc_key)\n",
    "                qualities = data_item.get(quality_key)\n",
    "                functions = data_item.get(function_key)\n",
    "\n",
    "                required_data = [event_data, chord_data, root_pcs, qualities, functions]\n",
    "                if not all(d is not None for d in required_data):\n",
    "                     missing_keys_str = [k for k,v in zip([event_key, chord_id_key, root_pc_key, quality_key, function_key], required_data) if v is None]\n",
    "                     logging.warning(f\"Missing required keys ({missing_keys_str}) in line {line_num+1} for priming. Check keys again. Trying next line.\"); continue\n",
    "\n",
    "                if not isinstance(event_data, list) or len(event_data) < prime_len:\n",
    "                     logging.warning(f\"Sequence in line {line_num+1} is too short (< {prime_len}) for priming. Trying next line.\"); continue\n",
    "\n",
    "                seq_len = len(event_data)\n",
    "                if not (isinstance(chord_data, list) and len(chord_data) == seq_len and \\\n",
    "                        isinstance(root_pcs, list) and len(root_pcs) == seq_len and \\\n",
    "                        isinstance(qualities, list) and len(qualities) == seq_len and \\\n",
    "                        isinstance(functions, list) and len(functions) == seq_len):\n",
    "                    logging.warning(f\"Inconsistent types or lengths in line {line_num+1} for priming. Trying next line.\"); continue\n",
    "\n",
    "                # --- Data Extraction & Conversion ---\n",
    "                prime_cond_root = root_pcs[:prime_len]\n",
    "                prime_cond_qual = qualities[:prime_len]\n",
    "                prime_cond_func = functions[:prime_len]\n",
    "\n",
    "                # Assume data is already IDs (integers) based on sample\n",
    "                if event_data and isinstance(event_data[0], int):\n",
    "                    prime_event_ids = event_data[:prime_len]\n",
    "                else:\n",
    "                    logging.error(f\"Melody data in file (key '{event_key}') is not a list of integers.\"); continue\n",
    "\n",
    "                if chord_data and isinstance(chord_data[0], int):\n",
    "                    prime_cond_ids = chord_data[:prime_len]\n",
    "                else:\n",
    "                    logging.error(f\"Chord data in file (key '{chord_id_key}') is not a list of integers.\"); continue\n",
    "\n",
    "                # Final safety checks\n",
    "                if max(prime_event_ids) >= config.melody_vocab_size: logging.error(f\"Prime melody IDs > vocab size!\"); continue\n",
    "                if config.use_chord_embedding and max(prime_cond_ids) >= config.chord_vocab_size: logging.error(f\"Prime chord IDs > vocab size!\"); continue\n",
    "\n",
    "                print(f\"Successfully loaded real priming sequence of length {prime_len} from line {line_num+1}.\")\n",
    "                return (prime_event_ids, prime_cond_ids, prime_cond_root, prime_cond_qual, prime_cond_func)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while loading priming data: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "    logging.error(f\"Could not find a suitable sequence for priming in the first few lines of {data_path}.\")\n",
    "    return None\n",
    "\n",
    "# === Post-processing Function ===\n",
    "def post_process_events(event_sequence: List[str], max_polyphony: int = 6, special_tokens: Set[str] = {MELODY_PAD_TOKEN, MELODY_UNK_TOKEN, \"<EOS>\", \"<START>\", \"<END>\"}) -> List[str]:\n",
    "    # (This function remains the same as previous version)\n",
    "    if max_polyphony <= 0: print(\"Warning: max_polyphony <= 0\"); return event_sequence\n",
    "    print(f\"\\n--- Starting Post-processing (Limiting Polyphony to {max_polyphony}) ---\")\n",
    "    processed_events = []; active_notes = set(); skipped_note_ons = 0\n",
    "    for i, event_str in enumerate(tqdm(event_sequence, desc=\"Post-processing Events\", leave=False, unit=\"event\")):\n",
    "        if not isinstance(event_str, str) or event_str in special_tokens: processed_events.append(event_str); continue\n",
    "        try:\n",
    "            parts = event_str.split('_'); event_type = \"_\".join(parts[:-1]).upper(); value = int(parts[-1])\n",
    "            if event_type == \"NOTE_ON\":\n",
    "                pitch = value\n",
    "                if len(active_notes) >= max_polyphony and pitch not in active_notes: skipped_note_ons += 1; continue\n",
    "                else: processed_events.append(event_str); active_notes.add(pitch)\n",
    "            elif event_type == \"NOTE_OFF\": processed_events.append(event_str); active_notes.discard(value)\n",
    "            else: processed_events.append(event_str)\n",
    "        except: processed_events.append(event_str) # Keep event if parsing fails\n",
    "    print(f\"Post-processing finished. Skipped {skipped_note_ons} NOTE_ON events.\"); return processed_events\n",
    "\n",
    "# === Main Generation Execution ===\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(module)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    generation_script_start_time = time.time()\n",
    "\n",
    "    # --- Configuration & Setup ---\n",
    "    # **********************************************************************\n",
    "    # ** USER: PLEASE UPDATE THESE PATHS TO MATCH YOUR ENVIRONMENT **\n",
    "    # **********************************************************************\n",
    "    CHECKPOINT_PATH = \"/kaggle/working/melody_model_output/best_model.pth\"\n",
    "    MELODY_VOCAB_PATH = \"/kaggle/input/new-melody-model-new-approach-1/event_vocab.json\"\n",
    "    CHORD_DATA_DIR = \"/kaggle/input/advance-h-rpe\"\n",
    "    # MELODY_DATA_PATH only needed if using load_real_prime_sequence below\n",
    "    MELODY_DATA_PATH = \"/kaggle/input/new-melody-model-new-approach-1/training_data.jsonl\"\n",
    "    OUTPUT_DIR = \"/kaggle/working/generated_output\"\n",
    "    # **********************************************************************\n",
    "\n",
    "    CHORD_VOCAB_FILENAME = \"chord_progression_vocab.json\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # --- Generation Parameters ---\n",
    "    TEMPERATURE = 0.75\n",
    "    TOP_K = 40\n",
    "    TOP_P = 0.9\n",
    "    MAX_POLYPHONY = 6           # Max simultaneous notes allowed by post-processing filter\n",
    "    START_MELODY_TOKEN = \"NOTE_ON_60\" # Default: Middle C (MIDI note 60)\n",
    "\n",
    "    # --- Device Setup ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device} ({torch.cuda.get_device_name(0) if device.type=='cuda' else 'CPU'})\")\n",
    "\n",
    "    # --- Load Vocabularies ---\n",
    "    print(\"\\n--- Loading Vocabularies ---\")\n",
    "    try:\n",
    "        with open(MELODY_VOCAB_PATH, 'r', encoding='utf-8') as f: melody_vocab = json.load(f); melody_inv_vocab = {v:k for k,v in melody_vocab.items()}\n",
    "        chord_vocab_path = Path(CHORD_DATA_DIR) / CHORD_VOCAB_FILENAME;\n",
    "        with open(chord_vocab_path, 'r', encoding='utf-8') as f: chord_vocab = json.load(f); chord_inv_vocab = {v:k for k,v in chord_vocab.items()} # Useful for debugging chord IDs\n",
    "        MELODY_PAD_ID = melody_vocab.get(MELODY_PAD_TOKEN, 0); CHORD_PAD_ID = chord_vocab.get(\"<PAD>\", 0)\n",
    "        MELODY_EOS_TOKEN = \"<EOS>\"; MELODY_EOS_ID = melody_vocab.get(MELODY_EOS_TOKEN, -1)\n",
    "        SPECIAL_TOKENS_SET = {MELODY_PAD_TOKEN, MELODY_UNK_TOKEN, MELODY_EOS_TOKEN, \"<START>\", \"<END>\"}\n",
    "        print(f\"Melody Vocab Size: {len(melody_vocab)}, Chord Vocab Size: {len(chord_vocab)}\")\n",
    "        print(f\"Melody Pad ID: {MELODY_PAD_ID}, Chord Pad ID: {CHORD_PAD_ID}, EOS ID: {MELODY_EOS_ID if MELODY_EOS_ID!=-1 else 'N/A'}\")\n",
    "    except Exception as e: print(f\"FATAL: Vocab loading error: {e}\"); traceback.print_exc(); sys.exit(1)\n",
    "\n",
    "    # --- Load Model and Configuration ---\n",
    "    print(f\"\\n--- Loading Model Checkpoint ---\")\n",
    "    try:\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n",
    "        loaded_config_dict = checkpoint.get('config'); assert isinstance(loaded_config_dict, dict), \"Config missing/invalid in checkpoint\"\n",
    "        config = TrainingConfig() # Create default config\n",
    "        valid_keys = {f.name for f in fields(config)}\n",
    "        for k, v in loaded_config_dict.items():\n",
    "            if k in valid_keys:\n",
    "                if k=='amp_dtype' and isinstance(v,str): config.amp_dtype={'float16':torch.float16,'bfloat16':torch.bfloat16}.get(v.split('.')[-1],None)\n",
    "                else: setattr(config, k, v)\n",
    "        config.melody_vocab_size=loaded_config_dict.get('melody_vocab_size', len(melody_vocab))\n",
    "        config.chord_vocab_size=loaded_config_dict.get('chord_vocab_size', len(chord_vocab))\n",
    "        config.melody_pad_token_id=loaded_config_dict.get('melody_pad_token_id', MELODY_PAD_ID)\n",
    "        config.chord_pad_token_id=loaded_config_dict.get('chord_pad_token_id', CHORD_PAD_ID)\n",
    "        if config.melody_pad_token_id != MELODY_PAD_ID or config.chord_pad_token_id != CHORD_PAD_ID: logging.warning(\"Pad ID mismatch! Checkpoint vs Vocab file. Using Checkpoint IDs.\")\n",
    "        MELODY_PAD_ID=config.melody_pad_token_id; CHORD_PAD_ID=config.chord_pad_token_id\n",
    "        print(\"\\n--- Final Configuration Used for Model ---\"); [print(f\"  {k}: {getattr(config, k)}\") for k in ['melody_vocab_size','chord_vocab_size','melody_pad_token_id','chord_pad_token_id','n_layer','d_model','n_head','d_head','mem_len','amp_dtype']]\n",
    "\n",
    "        model = MelodyTransformerXL(config).to(device) # Instantiate model AFTER setting config\n",
    "        state_dict = checkpoint['model_state_dict']; assert state_dict, \"model_state_dict missing\"\n",
    "        is_parallel_ckpt = all(key.startswith('module.') for key in state_dict)\n",
    "        if is_parallel_ckpt:\n",
    "            logging.info(\"Detected DataParallel/DDP checkpoint. Removing 'module.' prefix.\")\n",
    "            state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False) # Use loaded state_dict\n",
    "        if missing_keys: logging.warning(f\"Loaded model state dict was missing keys: {missing_keys}\")\n",
    "        if unexpected_keys: logging.warning(f\"Loaded model state dict had unexpected keys: {unexpected_keys}\")\n",
    "        print(f\"\\nModel loaded successfully.\")\n",
    "        model.eval(); model_dtype = model.dtype; print(f\"Model parameter dtype: {model_dtype}\")\n",
    "    except Exception as e: print(f\"FATAL: Model loading error: {e}\"); traceback.print_exc(); sys.exit(1)\n",
    "\n",
    "    # --- Prepare FULL Chord Progression & Conditioning ---\n",
    "    print(f\"\\n--- Preparing Full Chord Progression ---\")\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # !! USER ACTION REQUIRED: Load or Define Your Chord Progression Here     !!\n",
    "    # !! Provide the *full* sequences for chord IDs and features below.       !!\n",
    "    # !! The length of these lists determines the length of the generated     !!\n",
    "    # !! melody. Ensure they are properly aligned temporally.                 !!\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    # *** REPLACE THIS EXAMPLE WITH YOUR ACTUAL PROGRESSION DATA ***\n",
    "    # This example creates a 512-token long progression: Cmaj7 | Fmaj7 | Dmin7 | G7, repeated.\n",
    "    # You'll need to look up the correct IDs from your chord_vocab.json for your desired chords.\n",
    "    # Also, provide the correct normalized root_pc (0-11 -> 0.0-11.0/12.0), quality, and function codes.\n",
    "    # The `tokens_per_chord` determines how many melody events are generated for each chord. Adjust this based on tempo and desired rhythm density.\n",
    "\n",
    "    # --- START OF EXAMPLE DATA (TO BE REPLACED BY USER) ---\n",
    "    try:\n",
    "        # Example Chord IDs (Use your actual IDs from chord_vocab.json)\n",
    "        example_chord_map = {\n",
    "            \"Cmaj7\": {\"id\": chord_vocab.get(\"C:maj7\", 0), \"root\": 0/12, \"qual\": 1.0, \"func\": 1.0},\n",
    "            \"Fmaj7\": {\"id\": chord_vocab.get(\"F:maj7\", 0), \"root\": 5/12, \"qual\": 1.0, \"func\": 4.0},\n",
    "            \"Dm7\":   {\"id\": chord_vocab.get(\"D:min7\", 0), \"root\": 2/12, \"qual\": 0.0, \"func\": 2.0},\n",
    "            \"G7\":    {\"id\": chord_vocab.get(\"G:7\", 0),    \"root\": 7/12, \"qual\": 8.0, \"func\": 5.0}, # Check if 8 is correct for dominant 7th in your system\n",
    "            \"PAD\":   {\"id\": CHORD_PAD_ID,                 \"root\": 0.0, \"qual\": 9.0, \"func\": 10.0} # Ensure pad features align with training\n",
    "        }\n",
    "\n",
    "        # Define the progression using the names above\n",
    "        progression_sequence = [\"Cmaj7\", \"Fmaj7\", \"Dm7\", \"G7\"] * 8 # 32 chords total\n",
    "\n",
    "        tokens_per_chord = 16 # How many event tokens correspond to one chord change? ADJUST AS NEEDED!\n",
    "        prog_len = len(progression_sequence) * tokens_per_chord # Total tokens for conditioning\n",
    "\n",
    "        full_chord_ids_list = []\n",
    "        full_root_pc_list = []\n",
    "        full_quality_code_list = []\n",
    "        full_function_code_list = []\n",
    "\n",
    "        valid_progression = True\n",
    "        for chord_name in progression_sequence:\n",
    "            if chord_name in example_chord_map:\n",
    "                chord_info = example_chord_map[chord_name]\n",
    "                full_chord_ids_list.extend([chord_info[\"id\"]] * tokens_per_chord)\n",
    "                full_root_pc_list.extend([chord_info[\"root\"]] * tokens_per_chord)\n",
    "                full_quality_code_list.extend([chord_info[\"qual\"]] * tokens_per_chord)\n",
    "                full_function_code_list.extend([chord_info[\"func\"]] * tokens_per_chord)\n",
    "            else:\n",
    "                logging.error(f\"Chord '{chord_name}' not found in example_chord_map. Please define it or check spelling. Using PAD.\")\n",
    "                pad_info = example_chord_map[\"PAD\"]\n",
    "                full_chord_ids_list.extend([pad_info[\"id\"]] * tokens_per_chord)\n",
    "                full_root_pc_list.extend([pad_info[\"root\"]] * tokens_per_chord)\n",
    "                full_quality_code_list.extend([pad_info[\"qual\"]] * tokens_per_chord)\n",
    "                full_function_code_list.extend([pad_info[\"func\"]] * tokens_per_chord)\n",
    "                # If a chord is missing, you might want to stop: valid_progression = False; break\n",
    "\n",
    "        if not valid_progression:\n",
    "            print(\"FATAL: Invalid chord progression defined. Check logs.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Error defining or processing the chord progression: {e}\")\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "    # --- END OF EXAMPLE/USER DATA SECTION ---\n",
    "\n",
    "\n",
    "    # --- Check Data Length ---\n",
    "    if prog_len == 0:\n",
    "        print(\"FATAL: Chord progression data is empty. Please define or load it.\")\n",
    "        sys.exit(1)\n",
    "    print(f\"Using chord progression of length: {prog_len} tokens\")\n",
    "\n",
    "\n",
    "    # --- Define Start Melody Token ---\n",
    "    start_token_name = \"NOTE_ON_60\" # Default: Middle C\n",
    "    start_event_id = melody_vocab.get(start_token_name)\n",
    "    if start_event_id is None:\n",
    "        start_event_id = melody_vocab.get(\"<START>\", melody_vocab.get(\"NOTE_ON_60\", 138)) # Try <START> or fallback\n",
    "        logging.warning(f\"Start token '{start_token_name}' not found, using fallback ID {start_event_id} ('{melody_inv_vocab.get(start_event_id, 'UNK')}')\")\n",
    "    print(f\"Using start melody token: '{melody_inv_vocab.get(start_event_id, 'UNK')}' (ID: {start_event_id})\")\n",
    "\n",
    "    # --- Convert Progression to Tensors ---\n",
    "    try:\n",
    "        full_input_cond_ids = torch.tensor([full_chord_ids_list], dtype=torch.long, device=device)\n",
    "        full_input_cond_root = torch.tensor([full_root_pc_list], dtype=model_dtype, device=device)\n",
    "        full_input_cond_qual = torch.tensor([full_quality_code_list], dtype=model_dtype, device=device)\n",
    "        full_input_cond_func = torch.tensor([full_function_code_list], dtype=model_dtype, device=device)\n",
    "    except Exception as e: print(f\"FATAL: Tensor conversion error: {e}\"); traceback.print_exc(); sys.exit(1)\n",
    "\n",
    "\n",
    "    # --- Run Generation ---\n",
    "    print(f\"\\n--- Starting Melody Generation Over Progression ---\")\n",
    "    print(f\"Progression length: {prog_len}\")\n",
    "    print(f\"Temperature: {TEMPERATURE}, Top-K: {TOP_K}, Top-P: {TOP_P}\")\n",
    "    start_gen_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Call the specific generation function\n",
    "        generated_sequence_ids = generate_melody_over_progression(\n",
    "            model=model,\n",
    "            config=config,\n",
    "            device=device,\n",
    "            start_event_id=start_event_id,\n",
    "            full_cond_ids=full_input_cond_ids,\n",
    "            full_cond_root=full_input_cond_root,\n",
    "            full_cond_qual=full_input_cond_qual,\n",
    "            full_cond_func=full_input_cond_func,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_k=TOP_K,\n",
    "            top_p=TOP_P,\n",
    "            eos_token_id=MELODY_EOS_ID if MELODY_EOS_ID != -1 else None,\n",
    "            pad_token_id=MELODY_PAD_ID\n",
    "        )\n",
    "    except Exception as e: print(f\"FATAL: Generation error: {e}\"); traceback.print_exc(); sys.exit(1)\n",
    "\n",
    "    gen_duration = time.time() - start_gen_time\n",
    "    print(f\"\\nGeneration Complete! Took {gen_duration:.2f} seconds.\")\n",
    "    print(f\"Total sequence length generated: {len(generated_sequence_ids)}\") # Should match prog_len\n",
    "\n",
    "    # --- Decode Sequence ---\n",
    "    print(\"\\n--- Decoding Generated Sequence ---\")\n",
    "    try:\n",
    "        decoded_sequence_raw = [melody_inv_vocab.get(idx, f\"<UNK_ID_{idx}>\") for idx in generated_sequence_ids]\n",
    "        print(f\"First 100 RAW decoded tokens (including start token):\")\n",
    "        print(decoded_sequence_raw[:100])\n",
    "    except Exception as e: print(f\"Decoding error: {e}\"); decoded_sequence_raw = []\n",
    "\n",
    "    # Apply Post-processing\n",
    "    if decoded_sequence_raw:\n",
    "        try:\n",
    "            processed_sequence = post_process_events(\n",
    "                decoded_sequence_raw,\n",
    "                max_polyphony=MAX_POLYPHONY,\n",
    "                special_tokens=SPECIAL_TOKENS_SET\n",
    "            )\n",
    "            print(f\"Original generated length: {len(decoded_sequence_raw)}, Processed length: {len(processed_sequence)}\")\n",
    "        except Exception as e: print(f\"Post-processing error: {e}. Using raw sequence.\"); traceback.print_exc(); processed_sequence = decoded_sequence_raw\n",
    "    else: processed_sequence = []\n",
    "\n",
    "    # --- Save Outputs ---\n",
    "    print(\"\\n--- Saving Outputs ---\")\n",
    "    # Determine next available Melody_N filename\n",
    "    next_run_number = 1\n",
    "    try:\n",
    "        if os.path.exists(OUTPUT_DIR):\n",
    "            filename_pattern = re.compile(r\"^Melody_(\\d+)\\.(json|mid)$\", re.IGNORECASE)\n",
    "            max_num = 0\n",
    "            for filename in os.listdir(OUTPUT_DIR):\n",
    "                match = filename_pattern.match(filename)\n",
    "                if match: max_num = max(max_num, int(match.group(1)))\n",
    "            next_run_number = max_num + 1\n",
    "        else: logging.info(f\"Output directory {OUTPUT_DIR} not found. Creating and starting count at 1.\")\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True) # Ensure directory exists just before saving\n",
    "    except Exception as e: logging.error(f\"Error finding next run number: {e}. Defaulting to 1.\"); next_run_number = 1\n",
    "\n",
    "    gen_filename_base = f\"Melody_{next_run_number}\"\n",
    "    output_json_path = os.path.join(OUTPUT_DIR, f\"{gen_filename_base}.json\")\n",
    "    output_midi_path = os.path.join(OUTPUT_DIR, f\"{gen_filename_base}.mid\")\n",
    "    print(f\"Output base name: {gen_filename_base}\")\n",
    "\n",
    "    try:\n",
    "        config_dict_serializable = config.as_dict()\n",
    "        output_data = {\n",
    "            \"generation_params\": {\n",
    "                \"generation_mode\": \"Over Progression\",\n",
    "                \"output_filename_base\": gen_filename_base,\n",
    "                \"checkpoint_path\": str(CHECKPOINT_PATH),\n",
    "                \"progression_length\": prog_len,\n",
    "                \"start_event_id\": start_event_id,\n",
    "                \"temperature\": TEMPERATURE,\"top_k\": TOP_K,\"top_p\": TOP_P,\n",
    "                \"max_polyphony_filter\": MAX_POLYPHONY,\n",
    "                \"eos_token_id\": MELODY_EOS_ID,\n",
    "                \"device\": str(device),\n",
    "                \"generation_duration_sec\": gen_duration,\n",
    "                \"generation_timestamp\": datetime.datetime.now().isoformat(),\n",
    "            },\n",
    "            \"config_used\": config_dict_serializable,\n",
    "            # Optionally save conditioning IDs (can be large)\n",
    "            # \"conditioning_ids\": full_chord_ids_list,\n",
    "            \"generated_ids_full_raw\": generated_sequence_ids,\n",
    "            \"generated_tokens_full_raw\": decoded_sequence_raw,\n",
    "            \"generated_tokens_full_processed\": processed_sequence,\n",
    "        }\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f: json.dump(output_data, f, indent=2)\n",
    "        print(f\"Generated sequence data (JSON) saved to: {output_json_path}\")\n",
    "    except Exception as e: print(f\"\\nERROR saving generated sequence JSON: {e}\"); traceback.print_exc()\n",
    "\n",
    "    # --- Convert to MIDI ---\n",
    "    if processed_sequence:\n",
    "        print(\"\\n--- Converting PROCESSED Sequence to MIDI ---\")\n",
    "        try: events_to_midi(processed_sequence, output_midi_path)\n",
    "        except Exception as e: print(f\"\\nERROR during MIDI conversion: {e}\"); traceback.print_exc()\n",
    "    else: print(\"\\nSkipping MIDI conversion due to issues in sequence decoding or processing.\")\n",
    "\n",
    "    # --- Script Finish ---\n",
    "    print(\"\\nScript finished.\")\n",
    "    total_script_runtime = time.time() - generation_script_start_time\n",
    "    print(f\"Total Generation Script Runtime: {total_script_runtime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d9475",
   "metadata": {
    "papermill": {
     "duration": 0.806567,
     "end_time": "2025-04-25T12:09:11.806104",
     "exception": false,
     "start_time": "2025-04-25T12:09:10.999537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7224624,
     "sourceId": 11519501,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7244767,
     "sourceId": 11553450,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3027.204042,
   "end_time": "2025-04-25T12:09:15.351408",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-25T11:18:48.147366",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
